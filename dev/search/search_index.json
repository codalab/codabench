{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#documentation","title":"Documentation","text":"<p>Welcome to the Codabench wiki!</p> <p>Codabench is a platform allowing you to flexibly specify a benchmark. First you define tasks, e.g. datasets and metrics of success, then you specify the API for submissions of code (algorithms), add some documentation pages, and \"CLICK!\" your benchmark is created, ready to accept submissions of new algorithms. Participant results get appended to an ever-growing leaderboard.</p> <p>You may also create inverted benchmarks in which the role of datasets and algorithms are swapped. You specify reference algorithms and your participants submit datasets.</p> <p>Here are some links to get you started:</p> <p>Getting Started with Codabench Basic Installation Guide Compute Worker Setup Administrative Procedures </p> <p>Use the top bar or the search functionality to navigate the wiki!</p>"},{"location":"#useful-links","title":"Useful links","text":"<p>Governance Document Privacy and Terms of Use About</p>"},{"location":"Project_CodaBench_FAQ/","title":"FAQ","text":""},{"location":"Project_CodaBench_FAQ/#general-questions","title":"General questions","text":""},{"location":"Project_CodaBench_FAQ/#what-is-codabench-for","title":"What is Codabench for?","text":"<p>Codabench benchmarks are aimed at researchers, scientists and other professionals who want to track algorithm performance via benchmarks or have participants participate in a competition to find the best solution to a problem. We run a free public instance at https://www.codabench.org/ and the raw code is on Github.</p>"},{"location":"Project_CodaBench_FAQ/#can-codalab-competitions-be-privately-hosted","title":"Can CodaLab competitions be privately hosted?","text":"<p>Yes, you can host your own CodaLab instance on a private or hosted server (e.g. Azure, GCP or AWS). For more information, see how to deploy Codabench on your server and local installation guide. However, most benchmark organizers do NOT need to run their own instance. If you run a computationally demanding competition, you can hook up your own compute workers in the backend very easily. </p>"},{"location":"Project_CodaBench_FAQ/#how-to-change-my-username","title":"How to change my username?","text":"<p>You cannot change your username BUT you can change your display name which will then be displayed instead of your username. To change your display name, follow these instructions:</p> <pre><code>1. Login to Codabench\n2. Click your username in the top right corner\n3. Click `Edit Profile` in the list\n4. Set a display name you want to use\n5. Click `Submit` button to save changes\n</code></pre>"},{"location":"Project_CodaBench_FAQ/#how-to-make-a-task-public-or-use-public-tasks-from-other-users","title":"How to make a task public or use public tasks from other users?","text":"<p>Follow the detailed instruction here to know how you can make your task public and use other public tasks in your competitions.</p>"},{"location":"Project_CodaBench_FAQ/#how-to-delete-my-account","title":"How to delete my account?","text":"<p>Click on your account name on the top right of the website, then on <code>account</code></p>"},{"location":"Project_CodaBench_FAQ/#technical-questions","title":"Technical questions","text":""},{"location":"Project_CodaBench_FAQ/#server-setup-issues","title":"Server Setup Issues","text":"<p>Many technical FAQ are already located in the deploy your own server instructions.</p> <p>Questions already answered there:</p> <ul> <li>Getting <code>Invalid HTTP method</code> in django logs.</li> <li>I am missing some static resources (css/js) on front end.</li> <li>CORS error when uploading bundle.</li> <li>Logos don't upload from minio.</li> <li>Compute worker execution with insufficient privileges</li> <li>Securing Codabench and Minio</li> </ul>"},{"location":"Project_CodaBench_FAQ/#a-library-is-missing-in-the-docker-environment-what-do-to","title":"A library is missing in the docker environment. What do to?","text":""},{"location":"Project_CodaBench_FAQ/#how-does-codabench-use-dockers","title":"How does Codabench use dockers?","text":"<p>When you submit code to the Codabench platform, your code is executed inside a docker container. This environment can be exactly reproduced on your local machine by downloading the corresponding docker image.</p>"},{"location":"Project_CodaBench_FAQ/#for-participants","title":"For participants","text":"<ul> <li>If you are a competition participant, contact the competition organizers to ask them if they can add the missing library or program. They can either accept or refuse the request.</li> </ul>"},{"location":"Project_CodaBench_FAQ/#for-organizers","title":"For organizers","text":"<ul> <li>If you are a competition organizer, you can select a different competition docker image. If the default docker image (<code>codalab/codalab-legacy:py37</code>) does not suits your needs, you can either:<ul> <li>Select another image from DockerHub</li> <li>Create a new image from scratch</li> <li>Edit the default image and push it to your own DockerHub account</li> </ul> </li> </ul> <p>More information here.</p>"},{"location":"Project_CodaBench_FAQ/#emails-are-not-showing-up-in-my-inbox-for-registration","title":"Emails are not showing up in my inbox for registration","text":"<p>When deploying a local instance, the email server is not configured by default, so you won't receive the confirmation email during signup. In <code>.env</code> towards the bottom you will find: </p>.env<pre><code># Uncomment to enable email settings\n#EMAIL_BACKEND=django.core.mail.backends.smtp.EmailBackend\n#EMAIL_HOST=smtp.sendgrid.net\n#EMAIL_HOST_USER=user\n#EMAIL_HOST_PASSWORD=pass\n#EMAIL_PORT=587\n#EMAIL_USE_TLS=True\n</code></pre><p></p> <p>Uncomment and fill in SMPT server credentials. A good suggestion if you've never done this is to use sendgrid.</p>"},{"location":"Project_CodaBench_FAQ/#robots-and-automated-submissions","title":"Robots and automated submissions?","text":"<p>What about robot policy, reckless, or malicious behavior? Codabench does not forbid the use of robots (bots) to access the website, provided that it is not done with malicious intentions to disturb the normal use and jam the system. A user who abuses their rights by knowingly, maliciously, or recklessly jamming the system, causing the system to crash, causing loss of data, or gaining access to unauthorized data, will be banned from accessing all Codabench services.</p>"},{"location":"contact-us/","title":"Contact Us","text":"<ul> <li>The preferred way is via posting a GitHub issue.</li> <li>If you wish to get in touch with the community, you can use the Google Groups.</li> <li>In case of emergency</li> </ul> <p>Send us an email </p>"},{"location":"Contribute/","title":"Index","text":"<ul> <li>Use Codabench by either participating in a competition or hosting a new competition.</li> <li>Find a bug? Got a feature request? Submit a GitHub issue.</li> <li>P1 issues are the most important ones.</li> <li>Submit pull requests on GitHub to implement new features or fix bugs (see the contributing section).</li> <li>Improve this documentation.</li> <li>Let others know about Codabench!</li> </ul>"},{"location":"Contribute/contributing/","title":"Contributing","text":""},{"location":"Contribute/contributing/#being-a-codabench-user","title":"Being a Codabench user","text":"<ul> <li>Create a user account on Codabench</li> <li>Register on Codabench to this existing competition IRIS-tuto  and make a submission (you can find the necessary files here): <code>sample_result_submission</code> and <code>sample_code_submission</code>. See this page for more information.</li> <li>Create your own private competition (you can find the necessary files here ). See this page for more information.</li> </ul>"},{"location":"Contribute/contributing/#setting-up-a-local-instance-of-codabench","title":"Setting up a local instance of Codabench","text":"<ul> <li>Follow the tutorial in codabench wiki. According to your hosting OS, you might have to tune your environment file a bit. Try without enabling the SSL protocol (doing so, you don't need a domain name for the server). Try using the embedded Minio storage solution instead of a private cloud storage.</li> <li>If needed, you can also look into How to deploy Codabench on your server</li> </ul>"},{"location":"Contribute/contributing/#using-your-local-instance","title":"Using your local instance","text":"<ul> <li>Create your own competition and play with it. You can look at the output logs of each different docker container.</li> <li>Setting you as an admin of your platform and visit the Django Admin menu.</li> </ul>"},{"location":"Contribute/contributing/#setting-up-an-autonomous-compute-worker-on-a-machine","title":"Setting up an autonomous Compute Worker on a machine","text":"<ul> <li>Configure and launch a compute worker docker container.</li> <li>Create a private Queue on your new own competition on the production server codabench.org</li> <li>Assign your own compute-worker to this private queue instead of the default queue.</li> </ul>"},{"location":"Developers_and_Administrators/Administrator-procedures/","title":"Administrative Procedures","text":""},{"location":"Developers_and_Administrators/Administrator-procedures/#maintenance-mode","title":"Maintenance Mode","text":"<p>You can turn on maintenance mode by creating a <code>maintenance.on</code> file in the <code>maintenance_mode</code> folder. This will change the front page of the website, showing a customizable page (the <code>maintenance.html</code> file in the same folder).</p> <p>Simply remove the <code>maintenance_mode/maintenance.on</code> file to end maintenance mode.</p> <p>During testing, if you want to update or restart some services (e.g : Django), you should follow the following steps:</p> <pre><code>docker compose stop django\ndocker compose rm django ## remove old django container\ndocker compose create django ## create new django container with the changes from your development\ndocker compose start django\n</code></pre> <p>This procedure helps you update changes of your development on Django without having to restart every Codabench container.</p>"},{"location":"Developers_and_Administrators/Administrator-procedures/#give-superuser-privileges-to-a-user","title":"Give superuser privileges to a user","text":"<p>With superuser privileges, the user can edit any benchmark and can access the Django admin interface.</p> <pre><code>docker compose exec django ./manage.py shell_plus\nu = User.objects.get(username='&lt;USERNAME&gt;') ## can also use email\nu.is_staff = True\nu.is_superuser = True\nu.save()\n</code></pre>"},{"location":"Developers_and_Administrators/Administrator-procedures/#migration","title":"Migration","text":"<pre><code>docker compose exec django ./manage.py makemigrations\ndocker compose exec django ./manage.py migrate\n</code></pre>"},{"location":"Developers_and_Administrators/Administrator-procedures/#collect-static-files","title":"Collect static files","text":"<pre><code>docker compose exec django ./manage.py collectstatic --noinput\n</code></pre>"},{"location":"Developers_and_Administrators/Administrator-procedures/#delete-postgresdb-and-minio","title":"Delete POSTGRESDB and MINIO :","text":"<p>Warning</p> <p>This will delete all your data !</p> <pre><code>## Begin in codabench root directory\ncd codabench\n</code></pre>"},{"location":"Developers_and_Administrators/Administrator-procedures/#purge-data","title":"Purge data","text":"<pre><code>sudo rm -r var/postgres/*\nsudo rm -r var/minio/*\n</code></pre>"},{"location":"Developers_and_Administrators/Administrator-procedures/#see-data-we-are-going-to-purge","title":"See data we are going to purge","text":"<pre><code>ls var\n</code></pre>"},{"location":"Developers_and_Administrators/Administrator-procedures/#restart-services-and-recreate-database-tables","title":"Restart services and recreate database tables","text":"<pre><code>docker compose down\ndocker compose up -d\ndocker compose exec django ./manage.py migrate\n</code></pre>"},{"location":"Developers_and_Administrators/Administrator-procedures/#feature-competitions-in-home-page","title":"Feature competitions in home page","text":"<p>There are two ways of setting a competition as featured: 1. Use Django admin (see below) -&gt; click the competition -&gt; scroll down to is featured filed -&gt; Check/Uncheck it 2. Use competition ID in the django bash to feature or unfeature a competition </p><pre><code>docker compose exec django ./manage.py shell_plus\ncomp = Competition.objects.get(id=&lt;ID&gt;)  ## replace &lt;ID&gt; with competition id\ncomp.is_featured = True  ## set to False if you want to unfeature a competition\ncomp.save()\n</code></pre><p></p>"},{"location":"Developers_and_Administrators/Administrator-procedures/#shell-based-admin-features","title":"Shell Based Admin Features","text":"<p>If you're running your own Codabench instance, there are different ways to interact with the application. Inside the <code>django</code> container (<code>docker compose exec django bash</code>) you can use <code>python manage.py help</code> to display all available commands and a brief description. By far the most useful are <code>createsuperuser</code> and <code>shell/shell_plus</code>. </p>"},{"location":"Developers_and_Administrators/Administrator-procedures/#django-admin-interface","title":"Django Admin interface","text":"<p>Once you log in an account with superuser privileges, you have access to the \"Django Admin\" interface:</p> <p></p> <p>From this interface, you can change a user's quota, change their staff and superuser status, change the featured competitions displayed on the homepage, manage user accounts and more.</p>"},{"location":"Developers_and_Administrators/Administrator-procedures/#edit-announcement-and-news","title":"Edit announcement and news","text":"<p>In the Django admin interface, click on <code>Announcements</code> or <code>New posts</code>:</p> <p></p> <p>For announcement, only the first announcement is read by the front page. For news, all objects are read as separate news. You can create and edit objects using the interface. Write the announcement and news using HTML to format the text, add links, and more:</p> <p></p>"},{"location":"Developers_and_Administrators/Administrator-procedures/#delete-a-user","title":"Delete a user","text":"<p>Go to <code>Users</code>:</p> <p></p> <p>Select it, select the <code>Delete selected users</code> action and click on <code>Go</code>:</p> <p></p>"},{"location":"Developers_and_Administrators/Administrator-procedures/#rabbitmq-management","title":"RabbitMQ Management","text":"<p>The RabbitMQ management tool allows you to see the status of various queues, virtual hosts, and jobs. By default, you can access it at: <code>http://&lt;your_codalab_instance&gt;:15672/</code>. The username/password is your RabbitMQ <code>.env</code> settings for username and password. The port is hard-set in <code>docker-compose.yml</code> to 15672, but you can always change this if needed. For more information, see: https://www.rabbitmq.com/management.html</p>"},{"location":"Developers_and_Administrators/Administrator-procedures/#flower-management","title":"Flower Management","text":"<p>Flower is a web based tool for monitoring and administrating Celery clusters. By default, you can access the Flower web portal at <code>http://&lt;your_codalab_instance&gt;:5555/</code>. The username/password is your Flower <code>.env</code> settings for username and password. 5555 is the default port, and cannot be changed without editing the <code>docker-compose.yml</code> file.</p> <p>For more information on flower, please visit: https://flower.readthedocs.io/en/latest/</p>"},{"location":"Developers_and_Administrators/Administrator-procedures/#storage-analytics","title":"Storage analytics","text":""},{"location":"Developers_and_Administrators/Administrator-procedures/#the-interface","title":"The interface","text":"<p>The storage analytics page is accessible at codabench-url/analytics/ under the storage tab.</p> <p></p> <p>From this interface, you will have access to various analytics data:</p> <ul> <li>A Storage usage history chart</li> <li>A Competitions focused storage evolution, distribution and table</li> <li>A Users focused storage evolution, distribution and table</li> </ul> <p>All of those data can be filtered by date range and resolution, and exported as CSVs.</p> <p>The data displayed in those charts are only calculated from a background analytics task that takes place every Sunday at 02:00 UTC time (value editable in the <code>src/settings/base.py</code>).</p>"},{"location":"Developers_and_Administrators/Administrator-procedures/#the-background-task","title":"The background task","text":"<p>The analytics task is a celery tasked named <code>analytics.tasks.create_storage_analytics_snapshot</code>. What it does:</p> <ul> <li>It scans the database looking for file sizes that are not set or flagged in error</li> <li>Actually measures their size and saves it in the database</li> <li>Aggregate the storage space used by day and by Competition/User (for example every day for the last year for each competition) by looking at the database file size fields</li> <li>For data related to the Platform Administration it measures as well the database backup folder directly from the storage instance.</li> <li>Everything is saved as multiple snapshot in time in each Category table (i.g.: <code>UserStorageDataPoint</code>)</li> <li>This tasks also runs a database &lt;-&gt; storage inconsistency check and saves the results in a log file located in the <code>var/logs/</code> folder</li> </ul> <p>To manually start the task, you can do the following:</p> <ul> <li>Start codabench <code>docker compose up -d</code></li> <li>Bash into the django container and start a python console: <pre><code>docker compose exec django ./manage.py shell_plus\n</code></pre></li> <li>Manually start the task: <pre><code>from analytics.tasks import create_storage_analytics_snapshot\neager_results = create_storage_analytics_snapshot.apply_async()\n</code></pre></li> <li>If you check the logs (<code>docker compose logs -f</code>) of the app you should see \"Task create_storage_analytics_snapshot started\" coming from the site_worker container</li> <li>If you have to restart the task, don't worry, it will only compute the size of the files that hasn't been computed yet.</li> <li>Once the task is over you should be able to see the results on the web page</li> </ul>"},{"location":"Developers_and_Administrators/Administrator-procedures/#homepage-counters","title":"Homepage counters","text":"<p>There is also a daily background task counting users, competitions and submissions, in order to display it on the homepage.</p> <p>You can manually run it:</p> <pre><code>docker compose exec django ./manage.py shell_plus\n</code></pre> <pre><code>from analytics.tasks import update_home_page_counters\neager_results = update_home_page_counters.apply_async()\n</code></pre>"},{"location":"Developers_and_Administrators/Administrator-procedures/#user-quota-management","title":"User Quota management","text":""},{"location":"Developers_and_Administrators/Administrator-procedures/#increase-user-quota","title":"Increase user quota","text":""},{"location":"Developers_and_Administrators/Administrator-procedures/#using-the-django-shell","title":"Using the Django Shell","text":"<pre><code>docker-compose exec django ./manage.py shell_plus\n</code></pre> <pre><code>u = User.objects.get(username='&lt;USERNAME&gt;') ## can also use email\nu.quota = u.quota * 3 # We multiply the quota by 3 in this example\nu.save()\n</code></pre>"},{"location":"Developers_and_Administrators/Administrator-procedures/#using-the-django-admin-interface","title":"Using the Django Admin Interface","text":"<ul> <li>Go to the Django admin page</li> <li>Click user table</li> <li>Select the user for whom you want to increase/decrease quota</li> <li>Update the quota field with new quota (in GB e.g. 15) </li> </ul>"},{"location":"Developers_and_Administrators/Administrator-procedures/#codabench-statistics","title":"Codabench Statistics","text":"<p>You can create two types of codabench statistics: </p> <ul> <li>Overall platform statistics for a specified year</li> <li>Overall published competitions statistics</li> </ul> <p>Follow the steps below to create the statistics</p>"},{"location":"Developers_and_Administrators/Administrator-procedures/#start-codabench","title":"Start codabench","text":"<pre><code>docker compose up -d\n</code></pre>"},{"location":"Developers_and_Administrators/Administrator-procedures/#bash-into-the-django-container-and-start-a-python-console","title":"Bash into the django container and start a python console:","text":"<pre><code>docker compose exec django ./manage.py shell_plus\n</code></pre>"},{"location":"Developers_and_Administrators/Administrator-procedures/#for-overall-platform-statistics","title":"For overall platform statistics","text":"<pre><code>from competitions.statistics import create_codabench_statistics\ncreate_codabench_statistics(year=2024)\n</code></pre> <p>Note</p> <ul> <li>If <code>year</code> is not specified, the current year is used by default</li> <li>A CSV file named <code>codabench_statistics_2024.csv</code> is generated in <code>statistics</code> folder (for year=2024)</li> </ul>"},{"location":"Developers_and_Administrators/Administrator-procedures/#for-overall-published-competitions-statistics","title":"For overall published competitions statistics","text":"<pre><code>from competitions.statistics import create_codabench_statistics_published_comps\ncreate_codabench_statistics_published_comps()\n</code></pre> <p>Note</p> <p>A csv file named <code>codabench_statistics_published_comps.csv</code> is generated in <code>statistics folder</code></p>"},{"location":"Developers_and_Administrators/Automating-with-Selenium/","title":"Automation","text":""},{"location":"Developers_and_Administrators/Automating-with-Selenium/#what-and-why","title":"What and Why","text":"<p>It's useful to test various parts of the system with lots of data or many intricate actions. The selenium tests do this generally and are used as a guide for this tutorial. One problem is that Selenium needs to launch an instance of a browser to control. Our tests do this inside a docker container and uses a test database that doesn't persist as it cleans up after itself. We need to be able to control a live codabench session that is running. To do that we install a driver locally which is normally only inside the selenium docker container during tests. It is specific to your browser so keep that in mind.</p>"},{"location":"Developers_and_Administrators/Automating-with-Selenium/#virtualenv","title":"Virtualenv","text":"<p>You'll need a python virtual env as you don't want to be inside Django or you won't be able to launch a browser. </p>"},{"location":"Developers_and_Administrators/Automating-with-Selenium/#virtualenv_1","title":"Virtualenv","text":"<p>I used 3.8. </p><pre><code>python3 -m venv codabench\nsource ./codabench/bin/activate\n</code></pre><p></p>"},{"location":"Developers_and_Administrators/Automating-with-Selenium/#pyenv","title":"Pyenv","text":"<pre><code>pyenv install 3.8\npyenv virtualenv 3.8 codabench\npyenv activate codabench\n</code></pre>"},{"location":"Developers_and_Administrators/Automating-with-Selenium/#requirements","title":"Requirements","text":"<p>We have a couple extra things like <code>webdriver-manager</code> for getting a driver programmatically and <code>selenium</code> needs to be upgraded to use modern client interface. </p><pre><code>pip install -r requitements.txt\npip install -r requitements.dev.txt\npip install webdriver-manager\npip install --upgrade selenium\n</code></pre><p></p>"},{"location":"Developers_and_Administrators/Automating-with-Selenium/#automate-competition-creation","title":"Automate competition creation","text":"<p>Main Selenium Docs Install Getting Started </p> <pre><code>import os, time\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.chrome.service import Service\nfrom webdriver_manager.chrome import ChromeDriverManager\n\n# Use `ChromeDriverManager` to ensure the `chromedriver` is installed and in PATH\nservice = Service(ChromeDriverManager().install())\ndriver = webdriver.Chrome(service=service)\n\n# ... now use `driver` to control the local Chrome instance\ndriver.get(\"http://localhost/accounts/login\")\n\n# Use CSS selectors to find the input fields and button\nusername_input = driver.find_element(By.CSS_SELECTOR, 'input[name=\"username\"]')\npassword_input = driver.find_element(By.CSS_SELECTOR, 'input[name=\"password\"]')\nsubmit_button = driver.find_element(By.CSS_SELECTOR, '.submit.button')\n\n# Type the credentials into the fields\nusername_input.send_keys('bbearce')\npassword_input.send_keys('testtest')\n\n# Click the submit button\nsubmit_button.click()\n\ncomp_path = \"/home/bbearce/Documents/codabench/src/tests/functional/test_files/competition_v2_multi_task.zip\"\ndef upload_competition(competition_zip_path):\n    driver.get(\"http://localhost/competitions/upload\")\n    file_input = driver.find_element(By.CSS_SELECTOR, 'input[ref=\"file_input\"]')\n    file_input.send_keys(os.path.join(competition_zip_path))\n\n\nfor i in range(30):\n    upload_competition(comp_path)\n    time.sleep(5) # tune for your system\n</code></pre>"},{"location":"Developers_and_Administrators/Codabench-Architecture/","title":"Codabench Docker Architecture","text":"<p>Source: </p><pre><code>graph TD\nA((User))\nB[Caddy]\nC{{Django}}\nD[Postgres]\nF{{Rabbit}}\nG[Minio]\nH[Create Buckets]\nI{{Builder}}\nK[Flower]\nL[Compute Worker]\nM[Site Worker]\nA --&gt; B\nB --&gt; C\nC --&gt; D\nC --&gt; F\nF --&gt; L\nF --&gt; M\nC --&gt;|Storage|G\nG --&gt;|Initialization Helper| H\nF --&gt; K\nC --&gt;|JS Asset Helper| I</code></pre><p></p> <p>Codabench consists of many docker containers that are connected and organized through docker compose. Below is an overview of each container and their function:</p>"},{"location":"Developers_and_Administrators/Codabench-Architecture/#django","title":"Django","text":"<p>The main container that runs and contains the python code (A Django project). This is the container that is mainly used for utility functions like creating admins, creating backups, and manually making changes through the python django shell. It has a gunicorn webserver that serves internally on port 8000.</p>"},{"location":"Developers_and_Administrators/Codabench-Architecture/#caddy","title":"Caddy","text":"<p>The HTTP/HTTPs web server. It acts as a reverse proxy for the Django container. This container controls SSL/HTTPs functionality and other web server configuration options. Serves on port 80/443.</p>"},{"location":"Developers_and_Administrators/Codabench-Architecture/#postgres-labeled-db-in-docker-compose","title":"Postgres (Labeled DB in docker-compose)","text":"<p>The default database container. It will contain the default Postgres database used by codabench. (Name/user/pass,etc determined by .env.) If you need to make manual DB changes, this is the container to look into.</p> <p>Here is the DB schema </p>"},{"location":"Developers_and_Administrators/Codabench-Architecture/#compute-worker","title":"Compute Worker","text":"<p>The container(s) (can be external) that runs submissions for the codabench instance on their associated queues. The default workers are tied to the default queue. Commands to re-build the compute worker Docker image can be found here.</p>"},{"location":"Developers_and_Administrators/Codabench-Architecture/#site-worker","title":"Site Worker","text":"<p>The container that runs various tasks for the Django container such as unpacking and processing a competition bundle.</p>"},{"location":"Developers_and_Administrators/Codabench-Architecture/#minio","title":"Minio","text":"<p>The default storage solution container. Runs a MinIO instance that serves on the port defined by settings in your .env.</p>"},{"location":"Developers_and_Administrators/Codabench-Architecture/#create-buckets","title":"Create Buckets","text":"<p>A helper container for the Minio container that initially creates the buckets defined by settings in your .env if they don't already exist. Usually this container exits after that.</p>"},{"location":"Developers_and_Administrators/Codabench-Architecture/#builder","title":"Builder","text":"<p>A container to build RiotJS tags into a single unified tag that can then be mounted. Uses NPM.</p>"},{"location":"Developers_and_Administrators/Codabench-Architecture/#rabbit","title":"Rabbit","text":"<p>Task/Message management container. Organizes queues for Celery Tasks and compute workers. Any queues that get created are accessible through rabbit's own command line interface.</p>"},{"location":"Developers_and_Administrators/Codabench-Architecture/#flower","title":"Flower","text":"<p>Administrative utility container for Celery tasks and queues.</p>"},{"location":"Developers_and_Administrators/Codabench-Architecture/#competition-docker-image","title":"Competition docker image","text":"<p>The services of Codabench run inside Docker environments. This should not be confused with the Docker environment used to run the submissions, which may vary for each benchmark. The docker running submissions is discussed here</p>"},{"location":"Developers_and_Administrators/Codabench-Installation/","title":"Codabench Basic Installation Guide","text":"<p>Compared to Codalab, installing Codabench should be relatively easy since you no longer have to worry about special ways to set up SSL or storage. We include default solutions that should handle that for most basic uses.</p>"},{"location":"Developers_and_Administrators/Codabench-Installation/#pre-requisites","title":"Pre-requisites","text":""},{"location":"Developers_and_Administrators/Codabench-Installation/#install-docker-and-docker-compose","title":"Install Docker and Docker Compose","text":"<ul> <li>Docker</li> <li>Docker Compose</li> </ul>"},{"location":"Developers_and_Administrators/Codabench-Installation/#clone-repository","title":"Clone Repository","text":"<p>Download the Codabench repository:</p> <pre><code>git clone https://github.com/codalab/codabench\n</code></pre>"},{"location":"Developers_and_Administrators/Codabench-Installation/#edit-the-settings-env","title":"Edit the settings (.env)","text":"<p>The <code>.env</code> file contains the settings of your instance. On a fresh installation, you will need to use the following command to get your <code>.env</code> file: </p> <pre><code>cd codabench\ncp .env_sample .env\n</code></pre> <p>Then edit the necessary settings inside. The most important are the database, storage, and Caddy/SSL settings. For a quick local setup, you should not need to edit this file. For a public server deployment, you will have to modify some settings.</p> <p>It is important to change the default passwords if you intend for the instance to be public</p> <p>If you are using <code>AWS_S3_ENDPOINT_URL=http://minio:9000/</code> in your <code>.env</code>, edit your <code>/etc/hosts</code> file by adding this line <code>127.0.0.1 minio</code></p>"},{"location":"Developers_and_Administrators/Codabench-Installation/#for-macos","title":"For MacOS","text":"<p>In <code>.env</code>, replace: </p><pre><code>AWS_S3_ENDPOINT_URL=http://minio:9000/\n</code></pre><p></p> <p>by</p> <pre><code>AWS_S3_ENDPOINT_URL=http://docker.for.mac.localhost:9000/\n</code></pre> <p>If needed, some troubleshooting of this step is provided at the end of this page or in this page</p>"},{"location":"Developers_and_Administrators/Codabench-Installation/#start-the-service","title":"Start the service","text":"<p>To deploy the platform, run:</p> <pre><code>docker compose up -d\n</code></pre>"},{"location":"Developers_and_Administrators/Codabench-Installation/#run-the-following-commands","title":"Run the following commands","text":"<p>Create the required tables in the database:</p> <pre><code>docker compose exec django ./manage.py migrate\n</code></pre> <p>Generate the required static resource files:</p> <pre><code>docker compose exec django ./manage.py collectstatic --noinput\n</code></pre> <p>You should be able to verify it is running correctly by looking at the logs with <code>docker compose logs -f</code> and by visiting <code>localhost:80</code> (Depending on your configuration).</p>"},{"location":"Developers_and_Administrators/Codabench-Installation/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"Developers_and_Administrators/Codabench-Installation/#testing","title":"Testing","text":"<p>To run automated tests for your local instance, get inside the Django container with <code>docker compose exec django bash</code> then run <code>py.test</code> to start the automated tests.</p>"},{"location":"Developers_and_Administrators/Codabench-Installation/#ssl","title":"SSL","text":"<p>To enable SSL:</p> <ul> <li>If you already have a DNS for your server that is appropriate, in the <code>.env</code> simply set <code>DOMAIN_NAME</code> to your DNS. Remove any port designation like <code>:80</code>. This will have Caddy serve both HTTP and HTTPS.</li> </ul> <p>For a public instance, HTTPS is strongly recomended</p>"},{"location":"Developers_and_Administrators/Codabench-Installation/#validate-user-account-on-local-instance","title":"Validate user account on local instance","text":"<p>When deploying a local instance, the email server is not configured by default, so you won't receive the confirmation email during signup.</p> <p>To manually confirm your account:</p> <ol> <li>Find the confirmation link in the Django logs using <code>docker compose logs -f django</code></li> <li>Replace <code>example.com</code> by <code>localhost</code>on the URL and open it in your browser.</li> </ol> <p>Another way is to go inside the Django containers and use commands like in administrative procedures.</p>"},{"location":"Developers_and_Administrators/Codabench-Installation/#troubleshooting-storage-endpoint-url","title":"Troubleshooting storage endpoint URL","text":"<p>You may have to manually change the endpoint URL to have your local instance working. This may be an OS related issue. Here is a possible fix:</p> <ol> <li><code>docker compose logs -f minio</code></li> <li>Grab the first one of these IP addresses: <pre><code>minio_1           | Browser Access:\nminio_1           |    http://172.27.0.5:9000  http://127.0.0.1:9000\n</code></pre></li> <li>Set <code>AWS_S3_ENDPOINT_URL=http://172.27.0.5:9000</code>in your <code>.env</code> file.</li> </ol> <p>If static files are not loaded correctly, adding <code>DEBUG=True</code> to the <code>.env</code> file can help.</p>"},{"location":"Developers_and_Administrators/Codabench-Installation/#for-apple-cpu-m1-m2-chips","title":"For Apple CPU (M1, M2 chips)","text":"<p>In <code>docker-compose.yml</code>, replace in the <code>compute_worker</code> service:</p> docker-compose.yml<pre><code>command: bash -c \"watchmedo auto-restart -p '*.py' --recursive -- celery -A compute_worker worker -l info -Q compute-worker -n compute-worker@%n\"\n</code></pre> <p>by</p> docker-compose.yml<pre><code>command: bash -c \"celery -A compute_worker worker -l info -Q compute-worker -n compute-worker@%n\"\n</code></pre>"},{"location":"Developers_and_Administrators/Codabench-Installation/#storage","title":"Storage","text":"<p>By default, Codabench uses a built-in MinIO container. Some users may want a different solution, such as S3 or Azure. The configuration will vary slightly for each different type of storage.</p> <p>For all possible supported storage solutions, see: https://django-storages.readthedocs.io/en/latest/</p>"},{"location":"Developers_and_Administrators/Codabench-Installation/#remote-compute-workers","title":"Remote Compute Workers","text":"<p>To set up remote compute workers, you can follow the steps described in our Compute Worker Management page.</p>"},{"location":"Developers_and_Administrators/Codabench-Installation/#troubleshooting","title":"Troubleshooting","text":"<p>Read the following guide for troubleshooting: How to deploy Codabench.</p> <p>Also, adding <code>DEBUG=True</code> to the <code>.env</code> file can help with troubleshooting the deployment.</p> <p>Open a Github issue to find help with your installation</p>"},{"location":"Developers_and_Administrators/Codabench-Installation/#online-deployement","title":"Online Deployement","text":"<p>For information about online deployment of Codabench, go to the following page</p>"},{"location":"Developers_and_Administrators/Creating-and-Restoring-from-Backup/","title":"Backups - Automating Creation and Restoring","text":"<p>Codabench has a custom command that uploads a database backup, and copies it to the storage you are using under <code>/backups</code>. We'll see how to execute and automate that command, and how to restore from one of these backups in the event of a failure.</p>"},{"location":"Developers_and_Administrators/Creating-and-Restoring-from-Backup/#creating-backups","title":"Creating Backups","text":""},{"location":"Developers_and_Administrators/Creating-and-Restoring-from-Backup/#create","title":"Create","text":"<pre><code>DB_NAME=\nDB_USERNAME=\nDB_PASSWORD=\nDUMP_NAME=\ndocker exec codabench-db-1 bash -c \"PGPASSWORD=$DB_PASSWORD pg_dump -Fc -U $DB_USERNAME $DB_NAME &gt; /app/backups/$DUMP_NAME.dump\"\n</code></pre>"},{"location":"Developers_and_Administrators/Creating-and-Restoring-from-Backup/#upload","title":"Upload","text":"<p>There's a custom command on codabench that we use to upload database backups to storage. It should be accessible from inside the Django container (<code>docker compose exec django bash</code>) with <code>python manage.py upload_backup &lt;backup_path&gt;</code>. It takes an argument <code>backup_path</code> which is the path relative to your backup folder, <code>codabench/backups</code> and storage bucket, <code>/backups</code>. For instance if I pass it as <code>2022/$DUMP_NAME.dump</code>, the backup should happen in <code>codabench/backups/2022/$DUMP_NAME.dump</code> and will be uploaded to <code>/backups/2022/$DUMP_NAME.dump</code> in your storage bucket.</p>"},{"location":"Developers_and_Administrators/Creating-and-Restoring-from-Backup/#scheduling-automatic-backups","title":"Scheduling Automatic Backups","text":"<p>To schedule automatic backups, we're going to schedule a daily cronjob. To start, open the cron editor in a shell with <code>crontab -e</code>.</p> <p>Add a new entry like so, with the correct path to <code>pg_dump.py</code>:</p> <pre><code>@daily /home/ubuntu/codabench/bin/pg_dump.py\n</code></pre> <p>You should confirm this backup process works by setting some known cronjob time a few minutes in the future and see the dump in storage.</p> <p>Once done, save and quit the crontab editor, and verify your changes held by listing out cronjobs with <code>crontab -l</code>. You should see your new crontab entry.</p>"},{"location":"Developers_and_Administrators/Creating-and-Restoring-from-Backup/#restoring-from-backup","title":"Restoring From Backup","text":"<p>Re-install Codabench according to the documentation here: Codabench Installation.</p> <p>Once Codabench is re-installed and working, we're ready to restore our database backup. Upload the database backup to the webserver. It should go under the <code>codabench</code> install folder in the <code>/backups</code> directory. For example your path might look like: <code>/home/users/ubuntu/codabench/backups</code></p> <p>Once the backup is located in the <code>/backups</code> folder, you'll want to get into the postgres container (<code>docker compose exec db bash</code>). Make sure you know your <code>DB_NAME</code>, <code>DB_USERNAME</code>, and <code>DB_PASSWORD</code> variables from your .env.</p> <p>You can restore two ways. The first would be manually dropping the db, re-creating it, then using pg_restore to restore the data: </p>Inside the database container<pre><code>dropdb $DB_NAME -U $DB_USERNAME\ncreatedb $DB_NAME -U $DB_USERNAME\npg_restore -U $DB_USERNAME -d $DB_NAME -1 /app/backups/&lt;filename&gt;.dump\n</code></pre><p></p> <p>Or, you can let <code>pg_restore</code> do that for you with a couple of flags/arguments: </p>Inside the database container<pre><code>pg_restore --verbose --clean --no-acl --no-owner -h $DB_HOST -U $DB_USERNAME -d $DB_NAME /app/backups/&lt;filename&gt;.dump\n</code></pre><p></p> <p>The arguments <code>--no-acl</code> &amp; <code>--no-owner</code> may be useful if you're restoring as a non-root user. The owner argument is used for: <code>Do  not  output  commands to set ownership of objects to match the original database.</code></p> <p>The ACL argument is for: <code>Prevent dumping of access privileges (grant/revoke commands).</code></p> <p>After running <code>pg_restore</code> successfully without errors, you should find everything has been restored.</p>"},{"location":"Developers_and_Administrators/How-to-deploy-Codabench-on-your-server/","title":"How to Deploy a Server","text":""},{"location":"Developers_and_Administrators/How-to-deploy-Codabench-on-your-server/#overview","title":"Overview","text":"<p>This document focuses on how to deploy the current project to the local machine or server you are on.</p>"},{"location":"Developers_and_Administrators/How-to-deploy-Codabench-on-your-server/#preliminary-steps","title":"Preliminary steps","text":"<p>As for the minimal local installation, you first need to:</p> <ol> <li> <p>Install docker and docker-compose (see instructions)</p> </li> <li> <p>Clone Codabench repository:</p> </li> </ol> <pre><code>git clone https://github.com/codalab/codabench\n</code></pre>"},{"location":"Developers_and_Administrators/How-to-deploy-Codabench-on-your-server/#modify-env-file-configuration","title":"Modify .env file configuration","text":"<p>Then you need to modify the <code>.env</code> file with the relevant settings. This step is critical to have a working and secure deployment.</p> <ul> <li>Go to the folder where codabench is located (<code>cd codabench</code>) <pre><code>cp .env_sample .env\n</code></pre></li> </ul> <p>Then edit the variables inside the <code>.env</code> file.</p>"},{"location":"Developers_and_Administrators/How-to-deploy-Codabench-on-your-server/#submissions-endpoint","title":"Submissions endpoint","text":"<p>For an online deployment, you'll need to fill in the IP address or domain name in some environment variables.</p>"},{"location":"Developers_and_Administrators/How-to-deploy-Codabench-on-your-server/#using-an-ip-address","title":"Using an IP address","text":"<p>b) For an online deployment using IP address:</p> <p>Note</p> <p>To get the IP address of the machine. You can use one of the following commands:</p> <ul> <li><code>ifconfig -a</code></li> <li><code>ip addr</code></li> <li><code>ip a</code></li> <li><code>hostname -I | awk '{print $1}'</code></li> <li><code>nmcli -p device show</code></li> </ul> <p>Replace the value of IP address in the following environment variables according to your infrastructure configuration:</p> .env<pre><code>SUBMISSIONS_API_URL=https://&lt;IP ADDRESS&gt;/api\nDOMAIN_NAME=&lt;IP ADDRESS&gt;:80\nAWS_S3_ENDPOINT_URL=http://&lt;IP ADDRESS&gt;/\n</code></pre>"},{"location":"Developers_and_Administrators/How-to-deploy-Codabench-on-your-server/#using-a-domain-name-dns","title":"Using a domain name (DNS)","text":".env<pre><code>SUBMISSIONS_API_URL=https://yourdomain.com/api\nDOMAIN_NAME=yourdomain.com\nAWS_S3_ENDPOINT_URL=https://yourdomain.com\n</code></pre> <p>If you are deploying on an azure machine, then AWS_S3_ENDPOINT_URL needs to be set to an IP address that is accessible on the external network</p>"},{"location":"Developers_and_Administrators/How-to-deploy-Codabench-on-your-server/#change-default-usernames-and-passwords","title":"Change default usernames and passwords","text":"<p>Set up new usernames and passwords:</p> <pre><code>DB_USERNAME=postgres\nDB_PASSWORD=postgres\n[...]\nRABBITMQ_DEFAULT_USER=rabbit-username\nRABBITMQ_DEFAULT_PASS=rabbit-password-you-should-change\n[...]\nFLOWER_BASIC_AUTH=root:password-you-should-change\n[...]\n#EMAIL_HOST_USER=user\n#EMAIL_HOST_PASSWORD=pass\n[...]\nMINIO_ACCESS_KEY=testkey\nMINIO_SECRET_KEY=testsecret\n# or\nAWS_ACCESS_KEY_ID=testkey\nAWS_SECRET_ACCESS_KEY=testsecret\n</code></pre> <p>It is very important to set up an SSL certificate for Public deployement</p>"},{"location":"Developers_and_Administrators/How-to-deploy-Codabench-on-your-server/#open-access-permissions-for-following-port-number","title":"Open Access Permissions for following port number","text":"<p>If you are deploying on a Linux server, which usually has a firewall, you need to open access permissions to the following port numbers</p> <ul> <li><code>5672</code>: rabbit mq port</li> <li><code>8000</code>: django port</li> <li><code>9000</code>: minio port</li> </ul>"},{"location":"Developers_and_Administrators/How-to-deploy-Codabench-on-your-server/#modify-django-related-configuration","title":"Modify django-related configuration","text":"<ul> <li>Go to the folder where codabench is located</li> <li>Go to the settings directory and modify <code>base.py</code> file<ul> <li><code>cd src/settings/</code></li> <li><code>nano base.py</code></li> </ul> </li> <li>Change the value of <code>DEBUG</code> to <code>True</code><ul> <li><code>DEBUG = os.environ.get(\"DEBUG\", True)</code></li> </ul> </li> </ul> <p>If DEBUG is not set to true, then you will not be able to load to the static resource file</p> <ul> <li>Comment out the following code <pre><code># =============================================================================\n# Debug\n# =============================================================================\n#if DEBUG:\n#    INSTALLED_APPS += ('debug_toolbar',)\n#    MIDDLEWARE = ('debug_toolbar.middleware.DebugToolbarMiddleware',\n#                  'querycount.middleware.QueryCountMiddleware',\n#                  ) + MIDDLEWARE  # we want Debug Middleware at the top\n#    # tricks to have debug toolbar when developing with docker\n#\n#    INTERNAL_IPS = ['127.0.0.1']\n#\n#    import socket\n#\n#    try:\n#        INTERNAL_IPS.append(socket.gethostbyname(socket.gethostname())[:-1])\n#    except socket.gaierror:\n#        pass\n#\n#    QUERYCOUNT = {\n#        'IGNORE_REQUEST_PATTERNS': [\n#            r'^/admin/',\n#            r'^/static/',\n#        ]\n#    }\n#\n#    DEBUG_TOOLBAR_CONFIG = {\n#        \"SHOW_TOOLBAR_CALLBACK\": lambda request: True\n#    }\n</code></pre></li> </ul>"},{"location":"Developers_and_Administrators/How-to-deploy-Codabench-on-your-server/#start-service","title":"Start service","text":"<ul> <li>Execute command <code>docker compose up -d</code></li> <li>Check if the service is started properly <code>docker compose ps</code></li> </ul> <pre><code>codabench_compute_worker_1   \"bash -c 'watchmedo \u2026\"   running      \ncodabench_caddy_1            \"/bin/parent caddy -\u2026\"   running      0.0.0.0:80-&gt;80/tcp, :::80-&gt;80/tcp, 0.0.0.0:443-&gt;443/tcp, :::443-&gt;443/tcp, 2015/tcp\ncodabench_site_worker_1      \"bash -c 'watchmedo \u2026\"   running      \ncodabench_django_1           \"bash -c 'cd /app/sr\u2026\"   running      0.0.0.0:8000-&gt;8000/tcp, :::8000-&gt;8000/tcp\ncodabench_flower_1           \"flower\"                 restarting   \ncodabench_rabbit_1           \"docker-entrypoint.s\u2026\"   running      4369/tcp, 5671/tcp, 0.0.0.0:5672-&gt;5672/tcp, :::5672-&gt;5672/tcp, 15671/tcp, 25672/tcp, 0.0.0.0:15672-&gt;15672/tcp, :::15672-&gt;15672/tcp\ncodabench_minio_1            \"/usr/bin/docker-ent\u2026\"   running      0.0.0.0:9000-&gt;9000/tcp, :::9000-&gt;9000/tcp\ncodabench_db_1               \"docker-entrypoint.s\u2026\"   running      0.0.0.0:5432-&gt;5432/tcp, :::5432-&gt;5432/tcp\ncodabench_builder_1          \"docker-entrypoint.s\u2026\"   running      \ncodabench_redis_1            \"docker-entrypoint.s\u2026\"   running      0.0.0.0:6379-&gt;6379/tcp, :::6379-&gt;6379/tcp\n</code></pre> <ul> <li>Create the required tables in the database: <code>docker compose exec django ./manage.py migrate</code></li> <li>Generate the required static resource files: <code>docker compose exec django ./manage.py collectstatic --noinput</code></li> </ul> <p>Tip</p> <p>You can generate mock data with <code>docker compose exec django ./manage.py generate_data</code> if you want to test the website. However, it is not recomended to do that on an installation that you intend to use for Production</p>"},{"location":"Developers_and_Administrators/How-to-deploy-Codabench-on-your-server/#set-public-bucket-policy-to-readwrite","title":"Set public bucket policy to read/write","text":"<p>This can easily be done via the minio web console (local URL: minio:9000) </p>"},{"location":"Developers_and_Administrators/How-to-deploy-Codabench-on-your-server/#checkout-the-log-of-the-specified-container","title":"Checkout the log of the specified container","text":"<p>The following commands can help you debug</p> <ul> <li><code>docker compose logs -f django</code> : checkout django container logs in the docker-compose service</li> <li><code>docker compose logs -f site_worker</code> :  checkout site-worker container logs in the docker-compose service</li> <li><code>docker compose logs -f compute_worker</code> : checkout compute-worker container logs in the docker-compose service</li> <li><code>docker compose logs -f minio</code> : checkout minio container logs in the docker-compose service</li> </ul> <p>You can also use <code>docker compose logs -f</code> to get the logs of all the containers.</p>"},{"location":"Developers_and_Administrators/How-to-deploy-Codabench-on-your-server/#stop-service","title":"Stop service","text":"<ul> <li>Execute command <code>docker compose down --volumes</code></li> </ul>"},{"location":"Developers_and_Administrators/How-to-deploy-Codabench-on-your-server/#disabling-docker-containers-on-production","title":"Disabling docker containers on production","text":"<p>To override settings on your production server, create a <code>docker-compose.override.yml</code> in the <code>codabench</code> root directory. If on your production server, you are using remote MinIO or another cloud storage provider then you don't need minio container. If you have already buckets available for your s3 storage, you don't need createbuckets container. Therefore, you should disable minio and createbuckets containers. You may also want to disable the compute worker that is contained in the main server compute, to keep only remote compute workers.</p> <p>Add this to your <code>docker-compose.override.yml</code>:</p> docker-compose.override.yml<pre><code>version: '3.4'\nservices:\n  compute_worker:\n    command: \"/bin/true\"\n  minio:\n    restart: \"no\"\n    command: \"/bin/true\"\n  createbuckets:\n    entrypoint: \"/bin/true\"\n    restart: \"no\"\n    depends_on:\n      minio:\n        condition: service_started\n</code></pre> <p>Warning</p> <p>This will force the following container from exiting on start:</p> <ul> <li>Compute Worker</li> <li>MinIO</li> <li>CreateBuckets</li> </ul> <p>If you need one of these then remove the corresponding lines from the file before launching</p>"},{"location":"Developers_and_Administrators/How-to-deploy-Codabench-on-your-server/#link-compute-workers-to-default-queue","title":"Link compute workers to default queue","text":"<p>The default queue of the platform runs all jobs, except when a custom queue is specified by the competition or benchmark. By default, the compute worker of the default queue is a docker container run by the main VM. If your server is used by many users and receives several submissions per day, it is recommended to use separate compute workers and to link them to the default queue.</p> <p>To set up a compute worker, follow this guide</p> <p>In the <code>.env</code> file of the compute worker, the <code>BROKER_URL</code> should reflect settings of the <code>.env</code> file of the platform:</p> .env<pre><code>BROKER_URL=pyamqp://&lt;RABBITMQ_DEFAULT_USER&gt;:&lt;RABBITMQ_DEFAULT_PASS&gt;@&lt;DOMAIN_NAME&gt;:&lt;RABBITMQ_PORT&gt;/\nHOST_DIRECTORY=/codabench\nBROKER_USE_SSL=True\n</code></pre>"},{"location":"Developers_and_Administrators/How-to-deploy-Codabench-on-your-server/#personalize-main-banner","title":"Personalize Main Banner","text":"<p>The main banner on the Codabench home page shows 3 organization logos</p> <ul> <li>LISN</li> <li>Universit\u00e9 Paris-Saclay</li> <li>CNRS </li> </ul> <p>You can update these by:</p> <ol> <li>Replacing the logos in <code>src/static/img/</code> folder</li> <li>Updating the code in <code>src/templates/pages/home.html</code> to point to the right websites of your organizations</li> </ol>"},{"location":"Developers_and_Administrators/How-to-deploy-Codabench-on-your-server/#frequently-asked-questions-faqs","title":"Frequently asked questions (FAQs)","text":""},{"location":"Developers_and_Administrators/How-to-deploy-Codabench-on-your-server/#invalid-http-method","title":"Invalid HTTP method","text":"<p>Exception detail (by using <code>docker logs -f codabench_django_1</code>)</p> <pre><code>Traceback (most recent call last):\n  File \"/usr/local/lib/python3.8/site-packages/uvicorn/protocols/http/httptools_impl.py\", line 165, in data_received\n    self.parser.feed_data(data)\n  File \"httptools/parser/parser.pyx\", line 193, in httptools.parser.parser.HttpParser.feed_data\nhttptools.parser.errors.HttpParserInvalidMethodError: invalid HTTP method\n[2021-02-09 06:58:58 +0000] [14] [WARNING] Invalid HTTP request received.\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.8/site-packages/uvicorn/protocols/http/httptools_impl.py\", line 165, in data_received\n    self.parser.feed_data(data)\n  File \"httptools/parser/parser.pyx\", line 193, in httptools.parser.parser.HttpParser.feed_data\nhttptools.parser.errors.HttpParserInvalidMethodError: invalid HTTP method\n</code></pre> <p></p> <p>Solution</p> <ul> <li> <p>First, modify the <code>.env</code> file and set <code>DJANGO_SETTINGS_MODULE=settings.develop</code></p> </li> <li> <p>Then, restart services by using following docker-compose command</p> </li> </ul> <pre><code>docker compose down --volumes\ndocker compose up -d\n</code></pre>"},{"location":"Developers_and_Administrators/How-to-deploy-Codabench-on-your-server/#missing-static-resources-cssjs","title":"Missing static resources (css/js)","text":"<p>Solution: Change the value of the <code>DEBUG</code> parameter to <code>True</code></p> <ul> <li><code>nano competitions-v2/src/settings/base.py</code></li> <li><code>DEBUG = os.environ.get(\"DEBUG\", True)</code></li> </ul> <p></p> <ul> <li>Also comment out the following code in <code>base.py</code></li> </ul> <p></p>"},{"location":"Developers_and_Administrators/How-to-deploy-Codabench-on-your-server/#cors-error-could-not-upload-bundle","title":"CORS Error (could not upload bundle)","text":"<p>Exception detail (by checkout google develop tools)</p> <pre><code>botocore.exceptions.EndpointConnectionError: Could not connect to the endpoint URL: \"[http://docker.for.mac.localhost:9000/private/dataset/2021-02-18-1613624215/24533cfc523e/competition.zip](http://docker.for.mac.localhost:9000/private/dataset/2021-02-18-1613624215/24533cfc523e/competition.zip)\"\n</code></pre> <p>Solution: Set AWS_S3_ENDPOINT_URL to an address that is accessible to the external network</p> <ul> <li><code>nano codabench/.env</code></li> </ul> <p></p> <p>Make sure the IP address and port number is accessible by external network, You can check this by :</p> <ul> <li><code>telnet {ip-address-filling-in AWS_S3_ENDPOINT_URL} {port-filling-in AWS_S3_ENDPOINT_URL}</code></li> <li>Make sure the firewall is closed on port 9000</li> </ul> <p>This problem may also be caused by a bug in MinIO, in which case you will need to follow these steps</p> <ul> <li>Upgrade the minio docker image to the latest version</li> <li>Delete the previous minio directory folder in your codabench folder under <code>/var/minio</code> directory</li> <li>Stop the current minio container</li> <li>Delete the current minio container and the corresponding image</li> <li>Re-execute <code>docker compose up -d</code></li> </ul>"},{"location":"Developers_and_Administrators/How-to-deploy-Codabench-on-your-server/#display-logos-error-logos-dont-upload-from-minio","title":"Display logos error: logos don't upload from minio:","text":"<p>Check bucket policy of public minio bucket: read/write access should be allowed.</p> <p>This can easily be done via the minio web console (local URL: minio:9000)</p> <p></p>"},{"location":"Developers_and_Administrators/How-to-deploy-Codabench-on-your-server/#compute-worker-execution-with-insufficient-privileges","title":"Compute worker execution with insufficient privileges","text":"<p>This issue may be encountered when starting a docker container in a compute worker, the problem is caused by the installation of snap docker (if you are using Ubuntu).</p> <p>Solution</p> <ul> <li>Uninstall snap docker</li> <li>Install the official version of docker</li> </ul>"},{"location":"Developers_and_Administrators/How-to-deploy-Codabench-on-your-server/#securing-codabench-and-minio","title":"Securing Codabench and Minio","text":"<p>Codabench uses Caddy to manage HTTPS and to secure Codabench. What you need is a valid DNS pointed towards the IP address of your instance.</p>"},{"location":"Developers_and_Administrators/How-to-deploy-Codabench-on-your-server/#secure-minio-with-a-reverse-proxy","title":"Secure Minio with a reverse proxy","text":"<p>To secure MinIO, you should install a reverse-proxy, e.g: Nginx, and have a valid SSL certificate. Here is a tutorial sample:</p> <p>Secure MinIO with Certbot and Letsencrypt</p> <p>Don't forget to update your AWS_S3_ENDPOINT_URL parameter</p> <p>Update it to <code>AWS_S3_ENDPOINT_URL=https://&lt;your minio&gt;</code></p>"},{"location":"Developers_and_Administrators/How-to-deploy-Codabench-on-your-server/#secure-minio-on-the-same-server-as-codabench-simpler","title":"Secure Minio on the same server as codabench (simpler)","text":"<p>Summary:</p> <ul> <li>Use same SSL certs from letsencrypt (certbot) but change fullchain.pem -&gt; public.crt and privkey.pem -&gt; private.key. I copied from ./certs/caddy (for django/caddy) to ./certs/minio/certs.</li> <li>You need to change the command for minio to \"server /export --certs-dir /root/.minio/certs\" and not just \"server /export\"</li> <li>Mount in certs:</li> <li>Add \"- ./certs/minio:/root/.minio\" under the minio service's \"volumes\" section</li> <li>Certs must be in /${HOME}/.minio and for dockers ends up being /root/.minio</li> <li>Edit the .env with minio cert location: <pre><code>MINIO_CERT_FILE=/root/.minio/certs/public.crt\nMINIO_KEY_FILE=/root/.minio/certs/private.key\n# MINIO_CERTS_DIR=/certs/caddy # was told .pem files could work but for now separating\nMINIO_CERTS_DIR=/root/.minio/certs # either this or the CERT\\KEY above is redundant...but it works for now.\n# NOTE! if you change this port, change it in AWS_S3_ENDPOINT_URL as well\nMINIO_PORT=9000\n</code></pre></li> <li>Here is an example docker-compose.yml change for this: <pre><code>  #-----------------------------------------------\n  # Minio local storage helper\n  #-----------------------------------------------\n  minio:\n    image: minio/minio:RELEASE.2020-10-03T02-19-42Z\n    command: server /export --certs-dir /root/.minio/certs\n    volumes:\n      - ./var/minio:/export\n      - ./certs/minio:/root/.minio\n    restart: unless-stopped\n    ports:\n      - $MINIO_PORT:9000\n    env_file: .env\n    healthcheck:\n      test: [\"CMD\", \"nc\", \"-z\", \"minio\", \"9000\"]\n      interval: 5s\n      retries: 5\n  createbuckets:\n    image: minio/mc\n    depends_on:\n      minio:\n        condition: service_healthy\n    env_file: .env\n    # volumes:\n    #   This volume is shared with `minio`, so `z` to share it\n    #   - ./var/minio:/export\n    entrypoint: &gt;\n      /bin/sh -c \"\n      set -x;\n      if [ -n \\\"$MINIO_ACCESS_KEY\\\" ] &amp;&amp; [ -n \\\"$MINIO_SECRET_KEY\\\" ] &amp;&amp; [ -n \\\"$MINIO_PORT\\\" ]; then\n        until /usr/bin/mc config host add minio_docker https://minio:$MINIO_PORT $MINIO_ACCESS_KEY $MINIO_SECRET_KEY &amp;&amp; break; do\n          echo '...waiting...' &amp;&amp; sleep 5;\n        done;\n        /usr/bin/mc mb minio_docker/$AWS_STORAGE_BUCKET_NAME || echo 'Bucket $AWS_STORAGE_BUCKET_NAME already exists.';\n        /usr/bin/mc mb minio_docker/$AWS_STORAGE_PRIVATE_BUCKET_NAME || echo 'Bucket $AWS_STORAGE_PRIVATE_BUCKET_NAME already exists.';\n        /usr/bin/mc anonymous set download minio_docker/$AWS_STORAGE_BUCKET_NAME;\n      else\n        echo 'MINIO_ACCESS_KEY, MINIO_SECRET_KEY, or MINIO_PORT are not defined. Skipping buckets creation.';\n      fi;\n      exit 0;\n      \"\n</code></pre> </li> </ul> <p>Note</p> <p>Don't forget to change the entrypoint to run https and not http.</p>"},{"location":"Developers_and_Administrators/How-to-deploy-Codabench-on-your-server/#workaround-minio-and-django-on-the-same-machine-with-only-the-port-443-opened-to-the-external-network","title":"Workaround: MinIO and Django on the same machine with only the port 443 opened to the external network.","text":"<p>The S3 API signature calculation algorithm does not support proxy schemes where you host the MinIO Server API such as example.net/s3/.</p> <p>However, we can set the same URL for minio and django site and configure a proxy for each bucket in the Caddyfile : </p><pre><code>DOMAIN_NAME=https://mysite.com\nAWS_S3_ENDPOINT_URL=https://mysite.com\n</code></pre><p></p> <p>Caddyfile :  </p><pre><code>    @dynamic {\n        not path /static/* /media/* /{$AWS_STORAGE_BUCKET_NAME}* /{$AWS_STORAGE_PRIVATE_BUCKET_NAME}* /minio*\n    }\n    reverse_proxy @dynamic django:8000 \n\n    @min_bucket {\n      path /{$AWS_STORAGE_BUCKET_NAME}* /{$AWS_STORAGE_PRIVATE_BUCKET_NAME}*\n    }\n    reverse_proxy @min_bucket minio:{$MINIO_PORT}\n</code></pre><p></p>"},{"location":"Developers_and_Administrators/Manual-validation/","title":"Manual Validation","text":"<p>This is a checklist to follow in order to perform a manual validation of the platform's proper functioning. This is especially useful when validating a \"bump\" pull request made by the dependabot, a fresh installation or any change in the code base.</p> <ol> <li>Create a user account and login</li> <li>Create a competition</li> <li>Create a queue</li> <li>Upload a submission</li> <li>Check that the submission was processed (results, visualization tab, leaderboard)</li> <li>Change/recover password</li> <li>Delete user</li> <li>Delete submission</li> <li>Delete queue</li> <li>Delete competition</li> <li>Admin page</li> <li>Look at the logs for any problematic messages (<code>docker compose logs -f</code>)</li> </ol>"},{"location":"Developers_and_Administrators/Robot-submissions/","title":"Robot Submissions","text":"<p>This script is designed to test the Robot Submissions feature. Robot users should be able to submit to bot-enabled competitions without being admitted as a participant.</p> <p>This article will explain how to make a robot submission on your local computer, and how to present the results on the Leaderboard.</p>"},{"location":"Developers_and_Administrators/Robot-submissions/#pre-requisite","title":"Pre-requisite","text":"<ul> <li>Python 3</li> <li>Demo bundle: autowsl</li> <li>Github download URL</li> </ul> <ul> <li>Robot submission sample script here: link</li> </ul> <p>Brief description for demo bundle:</p> <ul> <li><code>code_submission</code>: It contains the sample bundle for the code submission and the code solution for the submission.</li> <li><code>auto_wsl_code_submission.zip</code>:This bundle is used for making submission.</li> <li><code>new_v18_code_mul_mul.zip</code>: The bundle is multiple phases, each phase has multiple tasks, and between these tasks, they share the same scoring program, that is, there is no need to copy multiple scoring program for hardcode.</li> <li><code>new_v18_code_mul_mul_sep_scoring.zip</code>: This bundle is multiple phases, multiple tasks under each phase share a scoring program that is exclusive to their particular phase.</li> <li> <p><code>new_v18_code_sin_mul.zip</code>: This is the sample bundle of a single phase multi-task that shares the same scoring program. </p> </li> <li> <p><code>dataset_submission</code>: It contains the sample bundle for data submission and the corresponding dataset solution for submission.</p> </li> <li><code>AutoWSL_dataset_submission.zip</code>: This is the bundle used for dataset submissions.</li> <li><code>new_v18_dataset_mul_mul.zip</code>:This is a multi-phase, each phase has multiple tasks below the sample bundle, multiple tasks, using the same scoring program, do not need to copy multiple scoring program for hardcode</li> <li><code>new_v18_dataset_mul_mul_sep_scoring.zip</code>: This is a multi-phase, each phase has multiple tasks below the sample bundle, the task between the different phases, using a different scoring program, that is, each phase has its own independent scoring program.</li> <li><code>new_v18_dataset_sin_mul.zip</code>: This is a sample bundle of single-phase multi-task commit datasets. </li> </ul>"},{"location":"Developers_and_Administrators/Robot-submissions/#getting-started","title":"Getting started","text":""},{"location":"Developers_and_Administrators/Robot-submissions/#upload-a-bundle","title":"Upload a bundle","text":"<p>Use the sample bundle provided above to upload the bundle and create a competition </p>"},{"location":"Developers_and_Administrators/Robot-submissions/#set-the-competition-to-allow-robot-submissions","title":"Set the competition to allow robot submissions","text":"<p>On the created competition page, click the EDIT button</p> <p></p> <p>Then click on the Participation tab, then scroll down to the bottom and click on Allow robot submission and click SAVE button.</p> <p> </p> <p>After the above steps are done, the Competition is allowed for making robot submission.</p>"},{"location":"Developers_and_Administrators/Robot-submissions/#set-yourself-to-is-bot","title":"Set yourself to Is bot","text":"<p>Go to the backend administration page, PROFILES tab bar below the user</p> <p> </p> <p>Check the <code>is bot</code> box, click save. You can now proceed with your robot submissions.</p>"},{"location":"Developers_and_Administrators/Robot-submissions/#change-codalab_url-address","title":"Change CODALAB_URL address","text":"<p>Change CODALAB_URL address in following scripts:<code>get_competition_details.py</code>, <code>example_submission.py</code>, <code>get_submission_details.py</code> CODALAB_URL = 'https://www.codabench.org/'</p> <p>Find scripts at <code>docs/example_scripts</code></p>"},{"location":"Developers_and_Administrators/Robot-submissions/#choose-the-competition","title":"Choose the competition","text":"<p>Run the following command on the command line: <code>python3 get_competition_details.py</code> What you're about to see is something like this </p> <p>Choose the ID of the competition you are interested in, for example 127. </p> <p>Run the script again with the competition ID as a parameter <code>python3 get_competition_details.py 127</code> Then you will see the following </p> <p>You can select the phase ID you're interested in, then use it as the second argument and run the script again, this time you'll get the task information associated with that phase. <code>python3 get_competition_details.py 127 215</code> </p>"},{"location":"Developers_and_Administrators/Robot-submissions/#making-submission","title":"Making submission","text":"<p>Inside the <code>example_submission.py</code> script, configure these options: </p> <ul> <li><code>CODALAB_URL</code>\u00a0can be changed if not testing locally.</li> <li><code>USERNAME</code>\u00a0and\u00a0<code>PASSWORD</code>\u00a0should correspond with the user being tested.</li> <li><code>PHASE_ID</code>\u00a0should correspond with the phase being tested on.</li> <li><code>TASK_LIST</code>\u00a0can be used to submit to specific tasks on a phase. If left blank, the submission will run on all tasks.</li> <li><code>SUBMISSION_ZIP_PATH</code>\u00a0You can fill in the absolute path of the submission directly.</li> </ul> <p>The idea here is that I'm going to test all the tasks below the competition with phase ID <code>215</code>. Then run the script. <code>python3 example_submission.py</code> </p> <p>You can see that you have successfully submitted the submission bundle.</p>"},{"location":"Developers_and_Administrators/Robot-submissions/#view-submission-details","title":"View submission details","text":"<p>Configure the<code>get_submission_details.py</code> options before running. </p> <ul> <li><code>CODALAB_URL</code>\u00a0can be changed if not testing locally.</li> <li><code>USERNAME</code>\u00a0and\u00a0<code>PASSWORD</code>\u00a0should correspond with the user being tested. Run the <code>get_submission_details</code>.py script with the ID of the phase containing the desired submission as the first argument.</li> </ul> <p>Since we chose <code>215</code> for our phase ID above, we'll choose <code>215</code> here.</p> <p>Then run the script. <code>python3 get_submission_details.py 215</code> </p> <p>Find the ID of the desired submission. For example, <code>542</code>.</p> <p>Then run the script. <code>python3 get_submission_details.py 215 542</code> </p>"},{"location":"Developers_and_Administrators/Robot-submissions/#finally","title":"Finally","text":"<p>Finally, you can go to the competition page, add your submission, and add it to the Leaderboard! </p> <p>On the Leaderboard, you can see the score details of each of your tasks. </p>"},{"location":"Developers_and_Administrators/Robot-submissions/#using-the-scripts","title":"Using the Scripts:","text":""},{"location":"Developers_and_Administrators/Robot-submissions/#setup","title":"Setup:","text":"<ul> <li>Create a competition with robot submissions enabled</li> </ul> <p>Example competition bundle</p> <p></p> <ul> <li>Create a user and enable the bot user flag on the Django admin page.</li> </ul> <p></p>"},{"location":"Developers_and_Administrators/Robot-submissions/#get_competition_detailspy","title":"get_competition_details.py:","text":"<ul> <li>Inside the <code>get_competition_details.py</code> script, configure these options:</li> </ul> <ul> <li> <p><code>CODALAB_URL</code> can be changed if not testing locally.</p> </li> <li> <p>Run the <code>get_competition_details</code> script with no arguments.</p> </li> <li> <p>Find the competition you want to test on.</p> </li> <li> <p>Run the <code>get_competition_details</code> script again with the competition ID as the only argument.</p> </li> <li> <p>Find the phase you want to test on.</p> </li> <li> <p>If you want to use the task selection feature, run the script again with the competition ID as the first argument and the phase ID as the second argument.</p> </li> <li> <p>Select the task you would like to run your submission on.</p> </li> <li> <p>Use the phase ID and task IDs to configure <code>example_submission.py</code>.</p> </li> </ul>"},{"location":"Developers_and_Administrators/Robot-submissions/#example_submissionpy","title":"example_submission.py:","text":"<ul> <li>Inside the <code>example_submission.py</code> script, configure these options:</li> </ul> <ul> <li> <p><code>CODALAB_URL</code> can be changed if not testing locally.</p> </li> <li> <p><code>USERNAME</code> and <code>PASSWORD</code> should correspond with the user being tested.</p> </li> <li> <p><code>PHASE_ID</code> should correspond with the phase being tested on.</p> </li> <li> <p><code>TASK_LIST</code> can be used to submit to specific tasks on a phase. If left blank, the submission will run on all tasks.</p> </li> <li> <p><code>SUBMISSION_ZIP_PATH</code> should be changed if testing on anything but the default \"Classify Wheat Seeds\" competition. An example submission can be found here.</p> </li> <li> <p>Run this script in a python3 environment with <code>requests</code> library installed.</p> </li> </ul>"},{"location":"Developers_and_Administrators/Robot-submissions/#get_submission_detailspy","title":"get_submission_details.py","text":"<ul> <li>Configure the <code>get_submission_details.py</code> options before running.</li> </ul> <ul> <li> <p><code>CODALAB_URL</code> can be changed if not testing locally.</p> </li> <li> <p><code>USERNAME</code> and <code>PASSWORD</code> should correspond with the user being tested.</p> </li> <li> <p>Run the <code>get_submission_details.py --phase &lt;id&gt;</code> script with the ID of the phase containing the desired submission.</p> </li> <li> <p>Find the ID of the desired submission.</p> </li> <li> <p>Run the <code>get_submission_details.py --submission &lt;id&gt;</code> script with the desired submission ID.</p> </li> <li> <p>The output of the script should be a submission object and a submission <code>get_details</code> object. This data can be used view scores, get prediction results, ect.</p> </li> <li> <p>Run the <code>get_submission_details.py --submission &lt;id&gt; -v</code> to save a zip containing previous info plus the original submission and logs.</p> </li> <li><code>--output &lt;PATH&gt;</code> can be used to choose where to save the zip file. Otherwise, it will be saved in the current directory.</li> </ul>"},{"location":"Developers_and_Administrators/Robot-submissions/#rerun_submissionpy","title":"rerun_submission.py","text":"<p>Robot users have the unique permission to rerun anyone's submission on a specific task. This enables clinicians to test pre-made solutions on private datasets that exist on tasks that have no competition.</p> <ul> <li>Configure the <code>rerun_submission.py</code> options before running.</li> </ul> <p></p> <ul> <li> <p><code>CODALAB_URL</code> can be changed if not testing locally.</p> </li> <li> <p><code>USERNAME</code> and <code>PASSWORD</code> should correspond with the user being tested.</p> </li> </ul>"},{"location":"Developers_and_Administrators/Robot-submissions/#running-the-script","title":"Running the script","text":"<ol> <li> <p>Create a competition that allows robots, and create a user marked as a robot    user. Use that username and password below.</p> </li> <li> <p>Get into a python3 environment with requests installed</p> </li> <li> <p>Review this script and edit the applicable variables, like...</p> <pre><code>CODALAB_URL\nUSERNAME\nPASSWORD\n...\n</code></pre> </li> <li> <p>Execute the contents of this script with no additional command line arguments with    the command shown below:</p> <p><code>./rerun_submission.py</code></p> </li> </ol> <p>The script is built to assist the user in the selection of the submission that will be re-run.</p> <ol> <li> <p>After selecting a submission ID from the list shown in the previous step, add that ID to    the command as a positional argument as shown below.</p> <p><code>./rerun_submission.py 42</code></p> </li> </ol> <p>The script will assist the user in the selection of a task ID.</p> <ol> <li>After selecting both a submission ID and a task ID, run the command again with both arguments to    see a demonstration of a robot user re-running a submission on a specific task.</li> </ol> <p>e.g.</p> <p><code>./rerun_submission.py 42 a217a322-6ddf-400c-ac7d-336a42863724</code></p>"},{"location":"Developers_and_Administrators/Running-tests/","title":"Running Tests","text":"<pre><code># Without \"end to end\" tests\n$ docker compose exec django py.test -m \"not e2e\"\n\n# \"End to end tests\" (a shell script to launch a selenium docker container)\n$ ./run_selenium_tests.sh\n\n# If you are on Mac OSX it is easy to watch these tests, no need to install\n# anything just do:\n$ open vnc://0.0.0.0:5900\n\n# And login with password \"secret\"\n</code></pre>"},{"location":"Developers_and_Administrators/Running-tests/#circleci","title":"CircleCI","text":"<p>To simulate the tests run by CircleCI locally, run the following command:</p> <pre><code>docker compose -f docker-compose.yml -f docker-compose.selenium.yml exec django py.test src/ -m \"not e2e\"\n</code></pre>"},{"location":"Developers_and_Administrators/Running-tests/#example-competitions","title":"Example competitions","text":"<p>The repo comes with a couple examples that are used during tests:</p>"},{"location":"Developers_and_Administrators/Running-tests/#v2-test-data","title":"v2 test data","text":"<pre><code>src/tests/functional/test_files/submission.zip\nsrc/tests/functional/test_files/competition.zip\n</code></pre>"},{"location":"Developers_and_Administrators/Running-tests/#v15-legacy-test-data","title":"v1.5 legacy test data","text":"<pre><code>src/tests/functional/test_files/submission15.zip\nsrc/tests/functional/test_files/competition15.zip\n</code></pre>"},{"location":"Developers_and_Administrators/Running-tests/#other-codalab-competition-examples","title":"Other Codalab Competition examples","text":"<p>https://github.com/codalab/competition-examples/tree/master/v2/</p>"},{"location":"Developers_and_Administrators/Submission-Docker-Container-Layout/","title":"Submission Docker Container Layout","text":"<p>When you make a submission to Codabench, the information and file are saved to the database. Afterwards, a Celery task gets sent to the compute worker (Default queue or a compute worker attached to a custom queue). From there the compute worker spins up another docker container with either the default docker image for submissions, or a custom one supplied by the organizer.</p>"},{"location":"Developers_and_Administrators/Submission-Docker-Container-Layout/#site-worker","title":"Site Worker","text":"<p>The site worker can be thought of exactly how it sounds. It's a local celery worker for the site to handle different Celery tasks and other miscellaneous functions. This is what is responsible for creating competition dumps, unpacking competitions, firing off the tasks for re-running submissions, etc.</p>"},{"location":"Developers_and_Administrators/Submission-Docker-Container-Layout/#compute-worker","title":"Compute Worker","text":"<p>Is a remote and/or local celery worker that listens to the main RabbitMQ server for tasks. A compute worker can be given a special queue to listen to, or listen to the default queue. For more information on setting up a custom queue and compute worker, see:</p> <ul> <li>Queue Management</li> <li>Compute Worker Management and Setup</li> </ul>"},{"location":"Developers_and_Administrators/Submission-Docker-Container-Layout/#submission-container","title":"Submission Container","text":"<p>The submission container is the container that participant's submissions get ran in. The image used to create this container is either the Codabench default if the organizer's haven't specified an image, or the custom image they specified in the competition. The default docker image is <code>codalab/codalab-legacy:py37</code>. Organizers can specify their own image using either the YAML file or the editor.</p> <p>This container is also created with some specific directories, some of which are only available at specific steps of the run. For example, generally for a submission there are 2 steps. The prediction step (If the submission needs to make predictions) and the scoring step, where the predictions are scored against the truth reference data.</p> <p>Here are the following directories:</p> <ul> <li><code>/app/input_data</code> Where input data will be (Only exists if input data is supplied for the task)</li> <li><code>/app/output</code> Where all submission output should go (Should always exist)</li> <li><code>/app/ingestion_program</code> Where the ingestion program files should exist (Only available in ingestion)</li> <li><code>/app/program</code> Where the scoring program and/or the ingestion program should be located (Should always exist)</li> <li><code>/app/input</code> Where any input for this step should be. (I.E: Previous predictions from ingestion for scoring. Only exists on scoring step)</li> <li><code>/app/input/ref</code> Where the reference data should live (Not available to submissions. Only available on scoring step.)</li> <li><code>/app/input/res</code> Where predictions/output from the prediction step should live. (Only available on scoring step)</li> <li><code>/app/shared</code> Where any data that needs to be shared between the ingestion program and submission should exist. (Only available on scoring steps.)</li> <li><code>/app/ingested_program</code> Where submission code should live if it is a code submission. (Only available in ingestion)</li> </ul>"},{"location":"Developers_and_Administrators/Submission-Process-Overview/","title":"Submission Process Overview","text":"<p>Source: </p><pre><code>graph TD\nA((User Submission))\nB{{Codalab Front End}}\nC(Clientside Javascript)\nD[Codalab API]\nE[Codalab Back End]\nF{{Compute Worker}}\n\nA--&gt;B\nB--&gt;C\nC--&gt;B\nC--&gt;D\nD--&gt;C\nD--&gt;E\nE--&gt;D\nE--&gt;|RabbitMQ|F\nF--&gt;|Web Sockets &amp; Results|E</code></pre><p></p>"},{"location":"Developers_and_Administrators/Submission-Process-Overview/#overview","title":"Overview:","text":"<ul> <li>When making a submission to Codabench, the website uses a client-side javascript to send the submission file along with proper details to an API point for submissions. </li> <li>Once the API receives the new submission, Codabench's back-end fires off a Celery task through RabbitMQ. </li> <li>This task is picked up by a Codabench Compute Worker listening in on the associated RabbitMQ. The Compute Worker starts processing the data. </li> <li>The compute worker begins execution by creating the container the submission will run in using the organizer's specified docker image for the competition.</li> <li>Once the container is created, the scoring program and other necessary organizer and user supplied binaries are executed to produce results. While the submission is processing, communication back to the Codabench instance is possible through web-sockets. </li> <li>Once the submission is done processing, it moves the output to the proper folders, and the compute worker posts these scores (if found) to the Codabench API. </li> <li>From here, the submission is done processing and is marked as failed/finished. If at any part of the processing step (Scoring/Ingestion step of the submission) an exception is raised, the compute worker will halt and send all current output.</li> </ul>"},{"location":"Developers_and_Administrators/Validation-and-deployment-of-pull-requests/","title":"Validation and deplyement of pull requests","text":""},{"location":"Developers_and_Administrators/Validation-and-deployment-of-pull-requests/#1-local-testing-and-validation-of-the-changes","title":"1. Local testing and validation of the changes","text":""},{"location":"Developers_and_Administrators/Validation-and-deployment-of-pull-requests/#setup","title":"Setup","text":"<p>Required: - \"Maintain\" role on the repository - A working local installation</p> <p>Pull the changes, checkout the branch you are testing and deploy your local instance:</p> <pre><code>cd codabench\ngit pull\ngit checkout branch\ndocker compose up -d\n</code></pre> <p>If necessary, migrate and collect static files (see this page).</p> <p>Note</p> <p>If the branch with the changes is from an external repository, you can create a branch in Codabench's repository and make a first merging into this new branch. Then, you'll be able to merge the new branch into master. This way, the automatic tests will be triggered.</p> <p>EDIT: It may be possible to trigger the tests even if the branch is external. To be confirmed.</p>"},{"location":"Developers_and_Administrators/Validation-and-deployment-of-pull-requests/#testing","title":"Testing","text":"<p>Here is the usual checklist in order to validate the pull request:</p> <p></p> <p>The contributor may have provided guidelines for testing that you should follow. In addition to this:</p> <ul> <li>Testing must be thorough, really trying to break the new changes. Try as many use cases as possible. Do not trust the contributor.</li> <li>In addition to the checklist of the PR, you can follow this checklist to check that the basic functionalities of the platform are still working: manual validation</li> </ul>"},{"location":"Developers_and_Administrators/Validation-and-deployment-of-pull-requests/#merging","title":"Merging","text":"<p>Once everything is validated, merge the pull request. If there are many minor commits, use \"squash and merge\" to merge them into one.</p> <p></p> <p>You can then safely click on <code>Delete branch</code>. It is a good practice to keep the project clean.</p>"},{"location":"Developers_and_Administrators/Validation-and-deployment-of-pull-requests/#update-the-test-server","title":"Update the test server","text":"<p>Here are the necessary steps to update the Codabench server to reflect the last changes. We prodive here general guidelines that work for both the test server and the production server.</p>"},{"location":"Developers_and_Administrators/Validation-and-deployment-of-pull-requests/#log-into-the-server","title":"Log into the server","text":"<pre><code>ssh codabench-server\ncd /home/codalab/codabench\n</code></pre> <p>Replace <code>codabench-server</code> by your own SSH host setting, or the IP address of the server.</p> <p>Note</p> <p>Make sure to log in as the user that deployed the containers.</p>"},{"location":"Developers_and_Administrators/Validation-and-deployment-of-pull-requests/#pull-the-last-change","title":"Pull the last change","text":"<p>Tip</p> <ul> <li>If you are deploying on a test server, you can use the <code>develop</code> branch.</li> <li>If you are deploying on a Production server, we strongly adivse on using the <code>master</code> branch.</li> </ul> <pre><code>docker compose down\ngit status\ngit pull\ndocker compose up -d\n</code></pre>"},{"location":"Developers_and_Administrators/Validation-and-deployment-of-pull-requests/#restart-django","title":"Restart Django","text":"<pre><code>docker compose stop django\ndocker compose rm django\ndocker compose create django\ndocker compose start django\n</code></pre> <p>If <code>docker compose</code> does not exist, use <code>docker-compose</code>.</p>"},{"location":"Developers_and_Administrators/Validation-and-deployment-of-pull-requests/#database-migration","title":"Database migration","text":"<pre><code>docker compose exec django ./manage.py migrate\n</code></pre> <p>Do not use <code>makemigrations</code></p> <p>Remark: we need to solve the migration files configuration. In the meantime, <code>makemigrations --merge</code> may be needed.</p>"},{"location":"Developers_and_Administrators/Validation-and-deployment-of-pull-requests/#collect-static-files","title":"Collect static files","text":"<pre><code>docker compose exec django ./manage.py collectstatic --noinput\n</code></pre>"},{"location":"Developers_and_Administrators/Validation-and-deployment-of-pull-requests/#final-testing","title":"Final testing","text":"<ul> <li>Access the platform from your browser</li> <li>You may need a hard refresh (Maj + R) so the changes take effect.</li> <li>Check that everything is working fine, the new changes, and the basic functionalities of the platform</li> </ul>"},{"location":"Developers_and_Administrators/Validation-and-deployment-of-pull-requests/#merge-develop-into-master","title":"Merge develop into master","text":"<p>Once some pull requests (~3 - 10) were merged into <code>develop</code>, we can prepare a merge into <code>master</code>. Simply create a new pull request from Github interface, selecting <code>master</code> as the base branch:</p> <p></p> <p>In the text of the PR, link all relevant PR made to develop, and indicate the URL of the test server.  Example: https://github.com/codalab/codabench/pull/1166</p>"},{"location":"Developers_and_Administrators/Validation-and-deployment-of-pull-requests/#update-the-production-server","title":"Update the production server","text":"<p>Same procedure as Update the test server, but on the production server.</p> <p>Tip</p> <p>Do not forget to access the platform and perform a final round of live testing after the deployment.</p>"},{"location":"Developers_and_Administrators/Validation-and-deployment-of-pull-requests/#creating-a-release","title":"Creating a Release","text":"<p>Once the develop branch has been merged into master, it is possible to use the Github interface to tag the commit of the merge and create a release containing all the changes as well as manual interventions if needed.</p> <p>For this, you will need to go to the release page and click on <code>Draft a new release</code></p> <p></p> <p>Afterwards, you click on <code>Choose a tag</code> and enter the tag you want to create (in this exemple, 1.17.0 which doesn't exist yet)</p> <p></p> <p>You can then choose the targeted branch to create the tag on (<code>master</code> in our case) and then click on <code>Generate release notes</code> Github will automatically generate releases based on the new commits compared to the last tag.</p> <p></p> <p>You can then change the text format however you like, as well as add things like Manual Intervention if there are any.</p> <p>When you are done, you publish the release.</p> <p></p>"},{"location":"Developers_and_Administrators/Validation-and-deployment-of-pull-requests/#todo","title":"TODO","text":"<p>Add a note about:</p> <ul> <li>Merging the github action PR to update the release tag</li> </ul>"},{"location":"Developers_and_Administrators/Upgrading_Codabench/","title":"Index","text":""},{"location":"Developers_and_Administrators/Upgrading_Codabench/#upgrade-codabench","title":"Upgrade Codabench","text":"<pre><code>cd codabench\ngit pull\ndocker compose build &amp;&amp; docker compose up -d\ndocker compose exec django ./manage.py migrate\ndocker compose exec django ./manage.py collectstatic --noinput\n</code></pre>"},{"location":"Developers_and_Administrators/Upgrading_Codabench/#manual-interventions","title":"Manual interventions","text":"<p>You can find here various manual intervention needed depending on which version you are upgrading from:</p> <ul> <li>Upgrade RabbitMQ (version &lt; 1.0.0)</li> <li>Create new logos for each competition (version &lt; 1.4.1)</li> <li>Worker Docker Image manual update (version &lt; 1.3.1)</li> <li>Add line into <code>.env</code> for default queue worker duration (version &lt; 1.7.0)</li> <li>Uncomment a line in your <code>.env</code> file (version &lt; 1.8.0)</li> <li>Rebuilding all docker images (version &lt; 1.9.2)</li> <li>Move the last storage_inconsistency files from logs folder to var logs folder (version &lt; 1.12.0)</li> <li>Submissions and Participants Counts (version &lt; 1.14.0)</li> <li>Homepage counters (version &lt; 1.15.0)</li> <li>User removal (version &lt; 1.17.0)</li> <li>Database size Fix (version &lt; 1.18.0)</li> </ul>"},{"location":"Developers_and_Administrators/Upgrading_Codabench/Add-line-into-.env-for-default-queue-worker-duration/","title":"Add line in .env file for default worker queue duration (version < 1.7.0)","text":"<p>This intervention is needed when upgrading from a version equal or lower than v1.7.0</p> <p>You will need to add the following line into your <code>.env</code> file </p>.env<pre><code>MAX_EXECUTION_TIME_LIMIT=600 # time limit for the default queue (in seconds)\n</code></pre><p></p> <p>This will change the maximum time a job can run on the default queue of your instance, as noted here</p>"},{"location":"Developers_and_Administrators/Upgrading_Codabench/Create-new-logos-for-each-competition/","title":"Create new logos for each competitions (version < 1.4.1)","text":"<p>This intervention is needed when upgrading from a version equal or lower than v1.4.1</p> <p>In order to create a \"logo icon\" for each existing competition</p> <ol> <li> <p>Shell into django </p><pre><code>docker compose exec django bash\npython manage.py shell_plus --plain\n</code></pre><p></p> </li> <li> <p>Get competitions that don't have logo icons </p><pre><code>import io, os\nfrom PIL import Image\nfrom django.core.files.base import ContentFile\ncomps_no_icon_logo = Competition.objects.filter(logo_icon__isnull=True)\nall = Competition.objects.all()\nlen(Competition.objects.all())\nlen(comps_no_icon_logo)\n</code></pre><p></p> </li> <li> <p>Then run this script </p><pre><code>for comp in comps_no_icon_logo:\n    try:\n        comp.make_logo_icon()\n        comp.save()\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        print(comp)\n</code></pre><p></p> </li> </ol>"},{"location":"Developers_and_Administrators/Upgrading_Codabench/Database-size-fixes/","title":"Database size fix (version < 1.18.0)","text":"<p>Warning</p> <p>You need to stop the database from being changed while running these commands. They might take time to complete depending on the size of your database.</p> <p>Start maintenance mode: </p><pre><code>touch maintenance/maintenance.on\n</code></pre><p></p>"},{"location":"Developers_and_Administrators/Upgrading_Codabench/Database-size-fixes/#1-django-migration-1774-1752","title":"1. Django migration (1774, 1752)","text":"<pre><code>docker compose exec django ./manage.py migrate\n</code></pre>"},{"location":"Developers_and_Administrators/Upgrading_Codabench/Database-size-fixes/#2-reset-user-quota-from-bytes-to-gb-1749","title":"2. Reset User Quota from Bytes to GB (1749)","text":"<pre><code>docker compose exec django ./manage.py shell_plus\n</code></pre> <pre><code>from profiles.quota import reset_all_users_quota_to_gb\nreset_all_users_quota_to_gb()\n</code></pre> <pre><code># Convert all 16 GB quota into 15 GB\nfrom profiles.models import User\nusers = User.objects.all()\nfor user in users:\n    # Reset quota to 15 if quota is between 15 and 20\n    # Do not reset quota for special users like adrien\n    if user.quota &gt; 15 and user.quota &lt; 20:\n        user.quota = 15\n        user.save()\n</code></pre>"},{"location":"Developers_and_Administrators/Upgrading_Codabench/Database-size-fixes/#3-important-for-file-sizes-cleanup-1752","title":"3. Important for file sizes cleanup (1752)","text":"<p>We have some critical changes here so before deployment we should run the following 3 blocks of code to get the last ids of <code>Data</code>, <code>Submission</code> and <code>SubmissionDetail</code></p> <p>Then, in the shell_plus:</p> <pre><code># Get the maximum ID for Data\nfrom datasets.models import Data\nlatest_id_data = Data.objects.latest('id').id\nprint(\"Data Last ID: \", latest_id_data)\n</code></pre> <pre><code># Get the maximum ID for Submission\nfrom competitions.models import Submission\nlatest_id_submission = Submission.objects.latest('id').id\nprint(\"Submission Last ID: \", latest_id_submission)\n</code></pre> <pre><code># Get the maximum ID for Submission Detail\nfrom competitions.models import SubmissionDetails\nlatest_id_submission_detail = SubmissionDetails.objects.latest('id').id\nprint(\"SubmissionDetail Last ID: \", latest_id_submission_detail)\n</code></pre> <p>After we have the latest ids, we should deploy and run the 3 blocks of code below to fix the sizes i.e. to convert all kib to bytes to make everything consistent. For new files uploaded after the deployment, the sizes will be saved in bytes automatically that is why we need to run the following code for older files only.</p> <pre><code># Run the conversion only for records with id &lt;= latest_id\nfrom datasets.models import Data\nfor data in Data.objects.filter(id__lte=latest_id_data):\n    if data.file_size:\n        data.file_size = data.file_size * 1024  # Convert from KiB to bytes\n        data.save()\n</code></pre> <pre><code># Run the conversion only for records with id &lt;= latest_id\nfrom competitions.models import Submission\nfor sub in Submission.objects.filter(id__lte=latest_id_submission):\n    updated = False  # Track if any field is updated\n\n    if sub.prediction_result_file_size:\n        sub.prediction_result_file_size = sub.prediction_result_file_size * 1024  # Convert from KiB to bytes\n        updated = True\n\n    if sub.scoring_result_file_size:\n        sub.scoring_result_file_size = sub.scoring_result_file_size * 1024  # Convert from KiB to bytes\n        updated = True\n\n    if sub.detailed_result_file_size:\n        sub.detailed_result_file_size = sub.detailed_result_file_size * 1024  # Convert from KiB to bytes\n        updated = True\n\n    if updated:\n        sub.save()\n</code></pre> <pre><code># Run the conversion only for records with id &lt;= latest_id\nfrom competitions.models import SubmissionDetails\nfor sub_det in SubmissionDetails.objects.filter(id__lte=latest_id_submission_detail):\n    if sub_det.file_size:\n        sub_det.file_size = sub_det.file_size * 1024  # Convert from KiB to bytes\n        sub_det.save()\n</code></pre> <p>Then, do not forget to stop maintenance mode:</p> <pre><code>rm maintenance/maintenance.on\n</code></pre>"},{"location":"Developers_and_Administrators/Upgrading_Codabench/Homepage-counters/","title":"Homepage Counters (version < 1.15.0)","text":"<p>After upgrading from Codabench &lt;1.15, you will need to follow these steps to compute the homepage counters. See this for more information</p> <ol> <li>Re-build containers</li> </ol> <pre><code>docker compose build &amp;&amp; docker compose up -d\n</code></pre> <ol> <li>Update the homepage counters (to avoid waiting 1 day)</li> </ol> <pre><code>docker compose exec django ./manage.py shell_plus\n</code></pre> <pre><code>from analytics.tasks import update_home_page_counters\neager_results = update_home_page_counters.apply_async()\n</code></pre>"},{"location":"Developers_and_Administrators/Upgrading_Codabench/Move-the-last-storage_inconsistency-files-from--logs-folder-to--var-logs--folder/","title":"Move the latest storage_inconsistency files from the logs folder to var/logs (version < 1.12.0)","text":"<p>This intervention is needed when upgrading from a version equal or lower than v1.11.0</p> <p>You will need to move the last storage_inconsistency files from /logs folder to /var/logs/ folder.</p> <pre><code>cd codabench\ncp -r logs/* var/logs\n</code></pre> <p>You will also need to rebuild the celery image because of a version change that's needed. </p><pre><code>docker compose down \ndocker images # Take the ID of the celery image\ndocker rmi *celery_image_id*\ndocker compose up -d\n</code></pre> More information here<p></p>"},{"location":"Developers_and_Administrators/Upgrading_Codabench/Rebuilding-all-docker-images/","title":"Rebuilding all docker images (version < 1.9.2)","text":"<p>This intervention is needed when upgrading from a version equal or lower than v1.9.2</p> <p>Since we are now using Poetry, we need to rebuild all our docker images to include it.</p> <p>You can achieve this by running the following commands : </p> <p>Warning</p> <p>If your machine has other images or containers that you want to keep, do not run <code>docker system prune -af</code>. Instead, manually delete all the images related to codabench</p> <pre><code>docker compose build &amp;&amp; docker compose up -d\ndocker system prune -a\n</code></pre> <p>More information (and alternative commands) here</p>"},{"location":"Developers_and_Administrators/Upgrading_Codabench/Submissions-and-Participants-Counts/","title":"Submissions and Participants count (version < 1.14.0)","text":"<p>After upgrading from Codabench &lt;1.14, you will need to follow these steps to compute the submissions and participants counts on the competition pages. See this for more information</p>"},{"location":"Developers_and_Administrators/Upgrading_Codabench/Submissions-and-Participants-Counts/#1-re-build-containers","title":"1. Re-build containers","text":"<pre><code>docker compose build &amp;&amp; docker compose up -d\n</code></pre>"},{"location":"Developers_and_Administrators/Upgrading_Codabench/Submissions-and-Participants-Counts/#2-migration","title":"2. Migration","text":"<pre><code>docker compose exec django ./manage.py migrate\n</code></pre>"},{"location":"Developers_and_Administrators/Upgrading_Codabench/Submissions-and-Participants-Counts/#3-update-counts-for-all-competitions","title":"3. Update counts for all competitions","text":"<p>Bash into django console </p><pre><code>docker compose exec django ./manage.py shell_plus\n</code></pre><p></p> <p>Import and call the function </p><pre><code>from competitions.submission_participant_counts import compute_submissions_participants_counts\ncompute_submissions_participants_counts()\n</code></pre><p></p>"},{"location":"Developers_and_Administrators/Upgrading_Codabench/Submissions-and-Participants-Counts/#4-feature-some-competitions-in-home-page","title":"4. Feature some competitions in home page","text":"<p>There are two ways to do it: 1. Use Django admin -&gt; click the competition -&gt; scroll down to is featured filed -&gt; Check/Uncheck it 2. Use competition ID in the django bash to feature / unfeature a competition </p><pre><code>docker compose exec django ./manage.py shell_plus\n</code></pre> <pre><code>comp = Competition.objects.get(id=&lt;ID&gt;)  # replace &lt;ID&gt; with competition id\ncomp.is_featured = True  # set to False if you want to unfeature a competition\ncomp.save()\n</code></pre><p></p>"},{"location":"Developers_and_Administrators/Upgrading_Codabench/Uncomment-a-line-in-your-.env-file/","title":"Uncomment a line in your .env file (version < 1.8.0)","text":"<p>This intervention is needed when upgrading from a version equal or lower than v1.8.0</p> <p>After the Caddy upgrade, you will need to uncomment a line in your <code>.env</code> file: </p>.env<pre><code>TLS_EMAIL = \"your@email.com\"\n</code></pre> More information here<p></p>"},{"location":"Developers_and_Administrators/Upgrading_Codabench/Upgrade-RabbitMQ/","title":"Upgrade RabbitMQ (version < 1.0.0)","text":"<p>This intervention is needed when upgrading from a version equal or lower than v1.0.0</p>"},{"location":"Developers_and_Administrators/Upgrading_Codabench/Upgrade-RabbitMQ/#backup-rabbitmq-settings","title":"Backup RabbitMQ settings","text":"<p>Go to <code>http://&lt;instance_ip&gt;:&lt;rabbitmq_admin_port&gt;/api/definitions</code>   and save the response (enter login and password as configured in <code>.env</code>)</p> <p>For example:</p> <p></p> <p>Do not submit any submission and wait until all submissions are processed</p>"},{"location":"Developers_and_Administrators/Upgrading_Codabench/Upgrade-RabbitMQ/#stop-and-remove-rabbitmqs-container-and-data","title":"Stop and remove RabbitMQ's container and data","text":"<pre><code>docker compose stop rabbit &amp;&amp; docker compose rm rabbit\nsudo rm -rvf var/rabbit/*\n</code></pre>"},{"location":"Developers_and_Administrators/Upgrading_Codabench/Upgrade-RabbitMQ/#switch-to-the-latest-rabbitmq-version","title":"Switch to the latest RabbitMQ version","text":"<p>Add <code>WORKER_CONNECTION_TIMEOUT=&lt;your timeout value&gt;</code> into your <code>.env</code> file with your custom value. Then execute:</p> <pre><code>git pull\ndocker compose build rabbit\ndocker compose up -d\n</code></pre>"},{"location":"Developers_and_Administrators/Upgrading_Codabench/Upgrade-RabbitMQ/#restore-the-backup-settings","title":"Restore the backup settings","text":"<p>Connect to the instance by ssh and upload your json file at 1st step, execute :</p> <pre><code>curl -u &lt;login&gt;:&lt;password&gt; -H \"Content-Type: application/json\" -X POST -T &lt;your definitions file&gt;.json http://localhost:&lt;rabbit_admin_port&gt;/api/definitions\n</code></pre> <p>You can check if it succeeded by doing the 1st step.</p>"},{"location":"Developers_and_Administrators/Upgrading_Codabench/Upgrade-RabbitMQ/#verify-that-your-submission-can-be-processed","title":"Verify that your submission can be processed.","text":""},{"location":"Developers_and_Administrators/Upgrading_Codabench/User-removal/","title":"User Removal (version < 1.17.0)","text":"<p>After upgrading from Codabench &lt;1.17, you will need to perform a Django migration (#1715, #1741)</p> <pre><code>docker compose exec django ./manage.py migrate\n</code></pre>"},{"location":"Developers_and_Administrators/Upgrading_Codabench/Worker-Docker-Image-manual-update/","title":"Worker docker image manual update (version < 1.3.1)","text":"<p>This intervention is needed when upgrading from a version equal or lower than v1.3.1</p> <p>To update your worker docker image, you can launch the following code in the terminal on the machine where your worker is located.</p> <pre><code>docker stop compute_worker\ndocker rm compute_worker\ndocker pull codalab/competitions-v2-compute-worker:latest\ndocker run \\\n    -v /codabench:/codabench \\\n    -v /var/run/docker.sock:/var/run/docker.sock \\\n    -d \\\n    --env-file .env \\\n    --name compute_worker \\\n    --restart unless-stopped \\\n    --log-opt max-size=50m \\\n    --log-opt max-file=3 \\\n    codalab/competitions-v2-compute-worker:latest \n</code></pre>"},{"location":"Newsletters_Archive/CodaLab-in-2024/","title":"2024","text":""},{"location":"Newsletters_Archive/CodaLab-in-2024/#codalab-in-2024","title":"CodaLab in 2024","text":""},{"location":"Newsletters_Archive/CodaLab-in-2024/#a-year-of-breakthroughs-and-new-horizons-with-codabench","title":"A Year of Breakthroughs and New Horizons with Codabench","text":"<p>Welcome to the first edition of CodaLab\u2019s newsletter! This year has been full of novelty, success, and scientific progress. The platform is breaking records of participation and number of organized competitions, and Codabench, the new version of CodaLab, had a very promising launch. Let\u2019s dive into more details.</p> <p></p>"},{"location":"Newsletters_Archive/CodaLab-in-2024/#unprecedented-engagement","title":"Unprecedented engagement","text":"<p>In October, Codabench has registered its 10,000th user! From about 100 daily submissions in January, it is now more than 500 daily submissions that are handled on the servers. In 2024, more than 20,000 new users have registered on CodaLab, whereas more than 12,000 on Codabench.</p> <p>249 public competitions have been created on CodaLab, whereas 193 on Codabench: +15% of total competitions of both platforms compared to 2023. CodaLab continues to handle a large number of submissions, as many past competitions remain active even after they have officially ended: 240,000 submissions on CodaLab whereas 83,000 on Codabench.</p> <p>Contributors community is very active with 143 pull requests this year. Since the platform is still relatively new, the primary focus has been on bug fixes, security and performance enhancements, and administrative features, accounting for approximately two-thirds of the pull requests. Nevertheless, we are keen on improving the experience for both participants and organizers. We have set a versioning and a release-notes follow-up to give more visibility to the platform evolution and maturity.</p> <p></p>"},{"location":"Newsletters_Archive/CodaLab-in-2024/#introducing-codabench","title":"Introducing Codabench","text":"<p>Codabench, the modernized version of CodaLab, was released in summer 2023, and presented at JCAD days in November 2024! Codabench platform software is now concentrating all development effort of the community. In addition to CodaLab features, it offers improved performance, live logs, more transparency, data-centric benchmarks and more!</p> <p>We warmly encourage you to use codabench.org for all your new competitions and benchmarks. Note that CodaLab bundles are compatible with Codabench, easing the transition, as explained in the following Wiki page: How to transition from CodaLab to Codabench</p> <p>CodaLab and Codabench are hosted on servers located at Paris-Saclay university, maintained by LISN lab.</p>"},{"location":"Newsletters_Archive/CodaLab-in-2024/#spotlight-on-competitions","title":"Spotlight on competitions","text":"<p>The most popular competition this year, featuring 520 participants, was the SemEval task Bridging the Gap in Text-Based Emotion Detection. The task of this competition focused on identifying the emotion that most people would associate with a speaker based on a given sentence or short text snippet.</p> <p>The competition with the highest reward, a prize pool of $100,000, was the Global Artificial Intelligence Championships, track Maths 2024, a pioneering contest that aimed to advance the development of artificial intelligence tools for solving advanced mathematical problems across multiple levels of difficulty (full report here).</p> <p>Codabench featured interesting NeurIPS 2024 challenges:</p> <ul> <li>The LLM Privacy Challenge, where participants were divided into two teams: Red Team and Blue Team, each one aiming at developing attack and defense approaches for data privacy in LLMs.</li> <li>Fair Universe - Higgs Uncertainy Challenge, exploring uncertainty-aware AI techniques for High Energy Physics (HEP).</li> <li>The Concordia Challenge, focusing on improving the cooperative intelligence of AI systems: promise-keeping, negotiation, reciprocity, reputation, partner choice, compromise, and sanctioning.</li> <li>The Erasing the Invisible Challenge, where the task is to remove invisible watermarks from images, while preserving the overall image quality.</li> <li>Codabench also hosted the NeurIPS 2024 Checklist Assistant, a self-service tool for the NeurIPS 2024 checklist. This verification assistant helps authors confirm their papers\u2019 compliance and submit improved versions based on the Assistant\u2019s feedback. For more details check out the blog post and the experiment results.</li> </ul> <p>Last but not least, Codabench featured several competitions in the fields of medicine and biology, such as the Dental CBCT Scans, Panoramic X-ray Images, Butterfly Hybrid Detection, and Monitoring Age-related Macular Degeneration Progression In Optical Coherence Tomography.</p> <p>We\u2019d like to thank the whole community for these exceptional scientific contributions. You can explore more challenges in the public listing.</p>"},{"location":"Newsletters_Archive/CodaLab-in-2024/#what-about-the-future","title":"What about the future?","text":"<p>We always develop new features to serve the state-of-the art of Machine Learning science. We plan to invest effort in handling medical data, improving federated learning use-cases, and allowing human-in-the loop feedback for better AI experience.</p> <p>The platform interface will also be improved for better visibility and facilitated re-use of datasets, tasks or solutions generated through it.</p> <p>Keep in touch and give us feedback on your experience on Codabench !</p>"},{"location":"Newsletters_Archive/CodaLab-in-2024/#community","title":"Community","text":"<p>Reminder on our communication tools:</p> <ul> <li>Join our google forum to emphasize your competitions and events</li> <li>Contact us for any question: info@codalab.org</li> <li>Write an issue on github about interesting suggestions</li> </ul> <p>Please cite one of these papers when working with our platforms:</p> <pre><code>@article{codabench,\n   title = {Codabench: Flexible, easy-to-use, and reproducible meta-benchmark platform},\n   author = {Zhen Xu and Sergio Escalera and Adrien Pav\u00e3o and Magali Richard and\n               Wei-Wei Tu and Quanming Yao and Huan Zhao and Isabelle Guyon},\n   journal = {Patterns},\n   volume = {3},\n   number = {7},\n   pages = {100543},\n   year = {2022},\n   issn = {2666-3899},\n   doi = {https://doi.org/10.1016/j.patter.2022.100543},\n   url = {https://www.sciencedirect.com/science/article/pii/S2666389922001465}\n}\n</code></pre> <pre><code>@article{codalab_competitions_JMLR,\n author  = {Adrien Pavao and Isabelle Guyon and Anne-Catherine Letournel and Dinh-Tuan Tran and Xavier Baro and Hugo Jair Escalante and Sergio Escalera and Tyler Thomas and Zhen Xu},\n title   = {CodaLab Competitions: An Open Source Platform to Organize Scientific Challenges},\n journal = {Journal of Machine Learning Research},\n year    = {2023},\n volume  = {24},\n number  = {198},\n pages   = {1--6},\n url     = {http://jmlr.org/papers/v24/21-1436.html}\n}\n</code></pre>"},{"location":"Newsletters_Archive/CodaLab-in-2024/#last-words","title":"Last words","text":"<p>Thank you for reading the first edition of our newsletter. We look forward to sharing more exciting updates, competitions, and breakthroughs with you soon. Until then, keep exploring, keep competing, and stay inspired!</p> <p></p>"},{"location":"Organizers/Benchmark_Creation/Advanced-Tutorial/","title":"Advanced Tutorial","text":"<p>Here is an advanced tutorial. If you are new to CodaBench, please refer to get started tutorial first. In this article, you'll learn how to use more advanced features and how to create benchmarks using either the editor or bundles. Before proceeding to our tutorial, make sure you have registered for an account on the Codabench website.</p> <p>The image below is an overview of the benchmark creation process </p>"},{"location":"Organizers/Benchmark_Creation/Advanced-Tutorial/#creating-a-benchmark-by-editor","title":"Creating a Benchmark by Editor","text":"<p>In this chapter, I'll take you step by step through the Editor's approach to creating benchmark, including algorithm type and dataset type.</p>"},{"location":"Organizers/Benchmark_Creation/Advanced-Tutorial/#step-1-click-on-management-in-the-top-right-corner-of-codabenchs-home-page-under-competitions","title":"Step 1: Click on Management in the top right corner of Codabench's home page under Competitions.","text":"<p> When you click on it, you will see the screen as shown in the screenshot below. </p>"},{"location":"Organizers/Benchmark_Creation/Advanced-Tutorial/#step-2-click-on-the-create-button-in-the-top-right-corner-of-competition-management","title":"Step 2: Click on the Create button in the top right corner of Competition Management.","text":""},{"location":"Organizers/Benchmark_Creation/Advanced-Tutorial/#step-3-fill-in-the-details-tab-content","title":"Step 3: Fill in the Details tab content.","text":""},{"location":"Organizers/Benchmark_Creation/Advanced-Tutorial/#step-4-fill-in-the-participant-tab","title":"Step 4: Fill in the Participant Tab.","text":""},{"location":"Organizers/Benchmark_Creation/Advanced-Tutorial/#step-5-fill-in-the-pages-tab","title":"Step 5: Fill in the Pages Tab.","text":""},{"location":"Organizers/Benchmark_Creation/Advanced-Tutorial/#step-6-fill-in-the-phases-tab","title":"Step 6: Fill in the Phases Tab.","text":"<p>When you click on the Manage Tasks/Datasets button, you will see the screenshot shown below</p> <p>Click the Add Dataset button in the diagram to upload the resource files needed to create the competition.</p> <p></p> <p>Creating a phase will require a bundle of the following types (.zip format), I'll give you more details on how to write these bundle later.</p> <p>Now you just need to have this concept in your mind</p> <p></p> <p>Here are the screenshots of the 5 types of bundles after they were uploaded</p> <p></p> <p></p> <p></p> <p></p>"},{"location":"Organizers/Benchmark_Creation/Advanced-Tutorial/#step-7-fill-in-the-leaderboard-tab","title":"Step 7: Fill in the Leaderboard Tab.","text":""},{"location":"Organizers/Benchmark_Creation/Advanced-Tutorial/#step-8-save-and-publish-the-benchmark","title":"Step 8: Save and Publish the Benchmark","text":""},{"location":"Organizers/Benchmark_Creation/Advanced-Tutorial/#creating-a-benchmark-by-bundle","title":"Creating a Benchmark by Bundle","text":"<p>Creating a benchmark through bundles is a much more efficient way than using editors.</p>"},{"location":"Organizers/Benchmark_Creation/Advanced-Tutorial/#simple-version-example-classify-wheat-seeds","title":"Simple Version Example: CLASSIFY WHEAT SEEDS","text":""},{"location":"Organizers/Benchmark_Creation/Advanced-Tutorial/#step-1-download-bundle","title":"Step 1: Download bundle","text":"<p>https://github.com/codalab/competition-examples/blob/master/codabench/wheat_seeds/code_submission_bundle.zip</p> <p>Click on the link above to download the bundle in the screenshot.</p> <p></p>"},{"location":"Organizers/Benchmark_Creation/Advanced-Tutorial/#step-2-go-to-the-benchmark-upload-page","title":"Step 2: Go to the benchmark upload page","text":""},{"location":"Organizers/Benchmark_Creation/Advanced-Tutorial/#step-3-upload-the-bundle","title":"Step 3: Upload the bundle","text":"<p>After the bundle has been uploaded, you will see the screenshot shown below.</p> <p></p>"},{"location":"Organizers/Benchmark_Creation/Advanced-Tutorial/#step-4-view-your-new-benchmark","title":"Step 4: View your new benchmark","text":""},{"location":"Organizers/Benchmark_Creation/Advanced-Tutorial/#benchmark-examples","title":"Benchmark Examples","text":"<p>Example bundles for code &amp; dataset competition can be found here:</p> <p>https://github.com/codalab/competition-examples/tree/master/codabench</p>"},{"location":"Organizers/Benchmark_Creation/Advanced-Tutorial/#iris","title":"Iris","text":"<p>Iris Codabench Bundle is a simple benchmark involving two phases, code submission and results submission.</p>"},{"location":"Organizers/Benchmark_Creation/Advanced-Tutorial/#autowsl","title":"AutoWSL","text":"<p>Two versions of the Automated Weakly Supervised Learning Benchmark: - Code submission benchmark - Dataset submission benchmark</p>"},{"location":"Organizers/Benchmark_Creation/Advanced-Tutorial/#mini-automl","title":"Mini-AutoML","text":"<p>Mini-AutoML Bundle is a benchmark template for Codabench, featuring code submission to multiple datasets (tasks).</p>"},{"location":"Organizers/Benchmark_Creation/Advanced-Tutorial/#how-do-i-set-up-submission-comments-for-multiple-submissions","title":"How do I set up submission comments for multiple submissions?","text":""},{"location":"Organizers/Benchmark_Creation/Advanced-Tutorial/#steps","title":"Steps","text":""},{"location":"Organizers/Benchmark_Creation/Advanced-Tutorial/#step-1-click-the-edit-button","title":"Step 1: Click the edit button","text":""},{"location":"Organizers/Benchmark_Creation/Advanced-Tutorial/#step-2-enable-multiple-submissions-on-leaderboard","title":"Step 2: Enable multiple submissions on leaderboard","text":""},{"location":"Organizers/Benchmark_Creation/Advanced-Tutorial/#step-3-set-up-submission-comment","title":"Step 3: Set up submission comment","text":""},{"location":"Organizers/Benchmark_Creation/Advanced-Tutorial/#step-4-save-all-changes","title":"Step 4: Save all changes","text":""},{"location":"Organizers/Benchmark_Creation/Advanced-Tutorial/#step-5-leave-a-comment-before-making-submission","title":"Step 5: Leave a comment before making submission","text":""},{"location":"Organizers/Benchmark_Creation/Advanced-Tutorial/#step-6-check-out-the-leaderboard","title":"Step 6: Check out the leaderboard","text":""},{"location":"Organizers/Benchmark_Creation/Cancer-Benchmarks/","title":"Example Cancer Benchmarks","text":"<p>This is our use case of cancer benchmarks. This document focuses on how to run the following three bundles in Codabench</p> <ul> <li><code>CODABENCH CANCER HETEROGENEITY DT#1 TRANSCRIPTOME PANCREAS</code></li> <li><code>CODABENCH CANCER HETEROGENEITY DT#2 METHYLOME PANCREAS</code></li> <li><code>CODABENCH CANCER HETEROGENEITY DT#3 IMMUNE CELL TYPES</code></li> </ul>"},{"location":"Organizers/Benchmark_Creation/Cancer-Benchmarks/#steps","title":"Steps","text":""},{"location":"Organizers/Benchmark_Creation/Cancer-Benchmarks/#1-decompressing-the-original-bundle","title":"1. Decompressing the original bundle","text":"<p>Unzip the bundle from its original zip file format into a folder.</p>"},{"location":"Organizers/Benchmark_Creation/Cancer-Benchmarks/#2-decompressing-ingestion_program_1zip","title":"2. Decompressing ingestion_program_1.zip","text":""},{"location":"Organizers/Benchmark_Creation/Cancer-Benchmarks/#3-modify-the-sub_ingestionr-file-in-the-ingestion_program_1-folder","title":"3. Modify the sub_ingestion.R file in the ingestion_program_1 folder.","text":"<p>Add lines 19 and 20 of code, and replace the underlined variable in line 25 with submission_program_dir</p> <p>Two new lines of code have been added to allow the v2 compute worker to find the user-submitted program (program.R). (Because the v2 compute worker does not support searching for user-submitted code in subfolders.)</p> <p></p><pre><code>child_dir &lt;- list.files(path=submission_program)\nsubmission_program_dir &lt;- paste0(submission_program, .Platform$file.sep, tail(child_dir, n=1))\n</code></pre> <pre><code>// read code submitted by the participants :\n.tempEnv &lt;- new.env( )\nsource(\n    file  = paste0(submission_program_dir, .Platform$file.sep, \"program.R\")\n  , local = .tempEnv\n)\n</code></pre> <p></p>"},{"location":"Organizers/Benchmark_Creation/Cancer-Benchmarks/#4save-the-changes-and-re-zip-the-ingestion_program_1-folder","title":"4.Save the changes and re-zip the ingestion_program_1 folder.","text":"<p>Open the command line and go to the ingestion_program_1 folder.</p> <p></p> <p>Use the following command to package the modified folder as a zip file <code>zip -r ingestion_program_1.zip *</code></p> <p></p> <p>Replace the latest compressed ingestion_program_1.zip file with the previous ingestion_program_1.zip file, and delete the ingestion_program_1 folder.</p> <p></p>"},{"location":"Organizers/Benchmark_Creation/Cancer-Benchmarks/#5-recompress-the-modified-original-bundle","title":"5. Recompress the modified original bundle.","text":"<p>Go to the directory at the same level as <code>competition.yaml</code> and execute the following command to compress the file <code>zip -r Codabench_cancer_heterogeneity_DT#2.zip *</code></p> <p></p>"},{"location":"Organizers/Benchmark_Creation/Cancer-Benchmarks/#6-creating-competition-with-compressed-bundles","title":"6. Creating competition with compressed bundles","text":""},{"location":"Organizers/Benchmark_Creation/Cancer-Benchmarks/#7-modify-the-default-execution-time","title":"7. Modify the default execution time","text":"<p>The default execution time is 10 minutes, but since these three bundles are time-consuming to execute, you have to turn it up.</p> <p> </p> <p>We recommend that you adjust the time to the maximum value of <code>2147483647</code>, so that the task will not time out and be forced to terminate by the compute worker.</p>"},{"location":"Organizers/Benchmark_Creation/Cancer-Benchmarks/#summary","title":"Summary","text":"<p>This paragraph summarizes the results of the execution of three bundles in codalab v2.</p>"},{"location":"Organizers/Benchmark_Creation/Cancer-Benchmarks/#codabench-cancer-heterogeneity-dt1-transcriptome-pancreas","title":"CODABENCH CANCER HETEROGENEITY DT#1 TRANSCRIPTOME PANCREAS","text":"<p>https://www.codabench.org/competitions/147/</p> <p>All three submissions were successful.</p> <p></p>"},{"location":"Organizers/Benchmark_Creation/Cancer-Benchmarks/#codabench-cancer-heterogeneity-dt2-methylome-pancreas","title":"CODABENCH CANCER HETEROGENEITY DT#2 METHYLOME PANCREAS","text":"<p>https://www.codabench.org/competitions/174/</p> <p>Two Submissions were successfully run, while the third failed due to insufficient execution time (We have now adjusted from the original 10,000 minute execution time limit to a maximum of 2,147483647.)</p> <p></p>"},{"location":"Organizers/Benchmark_Creation/Cancer-Benchmarks/#codabench-cancer-heterogeneity-dt3-immune-cell-types","title":"CODABENCH CANCER HETEROGENEITY DT#3 IMMUNE CELL TYPES","text":"<p>https://www.codabench.org/competitions/148/</p> <p>2 Submissions run successfully, 1 execution fails (screenshot below)</p> <p></p> <p>Failed execution screenshot:</p> <p></p>"},{"location":"Organizers/Benchmark_Creation/Competition-Bundle-Structure/","title":"Competition YAML Structure","text":"<p>A competition bundle is simply a zip file containing the <code>competition.yaml</code> which defines different aspects and attributes of your competition such as the logo, the html/markdown pages documenting your competition, and the data associated with your competition.</p>"},{"location":"Organizers/Benchmark_Creation/Competition-Bundle-Structure/#what-is-a-competition","title":"What is a Competition?","text":"<p>A competition is composed of a phase or many phases defining the active times of the competition, along with some other settings such as execution time limit. Each phase can have one or more tasks. </p> <p>A task is the problem the submission should be solving, therefore submissions that solve a task can be thought of as a solution. A task consists of reference data, input data, scoring program, and an ingestion program. </p> <p>For starting kits in v2, they should be solutions included with the competition bundle. See the example <code>competiton.yaml</code> or the section Competition YAML below for a link with more info. For more information on the different types of data, see the lower section of this page.</p>"},{"location":"Organizers/Benchmark_Creation/Competition-Bundle-Structure/#example-competition-bundle-layout","title":"Example Competition Bundle Layout:","text":"<p>Some competition bundle examples!</p> <p>Note</p> <p>Files can be under a directory, they just have to be referenced by their full path in the YAML. See the Competition YAML section below.</p> <pre><code>--\\ example_competition.zip\n  |\n  |- competition.yaml\n  |- logo.png\n  |- example_reference_data.zip\n  |- example_scoring_program.zip\n  |- example_solution.zip\n  |- overview.md\n  |- evaluation.md\n  |- terms_and_conditions.md\n  |- data.md\n</code></pre>"},{"location":"Organizers/Benchmark_Creation/Competition-Bundle-Structure/#example-competitionyaml","title":"Example competition.yaml:","text":"competition.yaml<pre><code>title: Example Competition Submit Scores\ndescription: An example competition where submissions should output the score they want\nimage: logo.jpg\nterms: terms.md\npages:\n    - title: overview\n      file: overview.md\n    - title: evaluation\n      file: evaluation.md\n    - title: terms\n      file: terms_and_conditions.md\n    - title: data\n      file: data.md\nphases:\n    - index: 0\n      name: First phase\n      description: An example phase\n      start: 2018-03-01\n      end: 2027-03-01\n      tasks:\n        - 0\ntasks:\n    - index: 0\n      name: First Phase Task\n      description: Task for the first phase\n      scoring_program: example_scoring_program.zip\n      reference_data: example_reference_data.zip\nsolutions:\n    - index: 0\n      path: example_solution.zip\n      tasks:\n        - 0\nleaderboard:\n  - title: Results\n    key: main\n    columns:\n      - title: score\n        key: score\n        index: 0\n        sorting: desc\n</code></pre>"},{"location":"Organizers/Benchmark_Creation/Competition-Bundle-Structure/#competition-yaml","title":"Competition YAML","text":"<p>The <code>competition.yaml</code> file is the most important file in the bundle. It's what Codabench looks for to figure out the structure and layout of your competition, along with additional details. For more information on setting up a <code>competition.yaml</code> see the wiki page here: Competition YAML</p>"},{"location":"Organizers/Benchmark_Creation/Competition-Bundle-Structure/#data-types-and-their-role","title":"Data Types And Their Role:","text":""},{"location":"Organizers/Benchmark_Creation/Competition-Bundle-Structure/#reference-data","title":"Reference Data:","text":"<p>Reference data is typically the truth data that your participant's predictions are compared against.</p>"},{"location":"Organizers/Benchmark_Creation/Competition-Bundle-Structure/#scoring-program","title":"Scoring Program","text":"<p>The scoring program is the file that gets ran to determine the scores of the submission, typically either based on the submission's prediction outputs, or the results from the submission itself when compared with the reference data. Usually this should be a script like a python file, but it can generally be anything.</p> <p>This is also paired with a <code>metadata.yaml</code> file with a key <code>command</code> that correlates to the command used to run your scoring program. There are also special directories available for use. </p> <p>Example: Here's what a <code>metadata.yaml</code> might look like: </p>metadata.yaml<pre><code>command: python3 /app/program/scoring.py /app/input/ /app/output/\n</code></pre><p></p> <p>This specifies that python3 is going to run the scoring program (which is going to be located in /app/program/scoring.py). It also gives, as <code>args</code>: - The input folder, which contains either the user's own results or predictions from ingestion - The output folder where the score is placed. </p> <p>The arguments are optional, but passing them may be more convenient.</p> <p>The scoring program outputs a <code>scores.json</code> file containing the results for each column of the leaderboard. After computing a submission, this file should look like something like this:</p> scores.json<pre><code>{\"accuracy\": 0.886, \"duration\": 42.4}\n</code></pre> <p>The keys should match the leaderboard columns keys defined in the <code>competition.yaml</code> file.</p> <p>The scoring program can also output detailed results as an HTML file for each submission. Click here for more information.</p>"},{"location":"Organizers/Benchmark_Creation/Competition-Bundle-Structure/#ingestion-program","title":"Ingestion Program","text":"<p>The ingestion program is a file that gets ran to generate the predictions from the submissions if necessary. This is usually a python script or a script in another language, but it can generally be anything.</p> <p>The ingestion program is also paired with a <code>metadata.yaml</code> that specifies how to run it. It should have a key <code>command</code> that is the command used to run your ingestion program. The same special directories should be available to your ingestion program. </p> <p>Example: Here's what an ingestion <code>metdata.yaml</code> might look like this: </p>metadata.yaml<pre><code>command: python3 /app/program/ingestion.py /app/input_data/ /app/output/ /app/program /app/ingested_program\n</code></pre><p></p> <p>Just like the example above, this specifies we're using python to run our ingestion program. Please note that it is not necessary to pass these directories as arguments to the programs, but it can be convenient. More information about the folder layout here.</p>"},{"location":"Organizers/Benchmark_Creation/Competition-Bundle-Structure/#input-data","title":"Input Data","text":"<p>This is usually the test data used to generate predictions from a user's code submission when paired with an ingestion program.</p>"},{"location":"Organizers/Benchmark_Creation/Competition-Creation/","title":"Competition Creation","text":"<p>Competition creation can be done two ways. Through the online form on Codalab, or by uploading a competition bundle to Codalab.</p>"},{"location":"Organizers/Benchmark_Creation/Competition-Creation/#bundle-upload","title":"Bundle Upload","text":"<p>For more information on Bundle Upload see here:</p> <p>Competition Creation: Bundle</p> <p>For more information on Competition Bundle Structure, see here:</p> <p>Competition Bundle Structure</p>"},{"location":"Organizers/Benchmark_Creation/Competition-Creation/#gui-creation","title":"GUI creation","text":"<p>For more information on GUI creation see here:</p> <p>Competition Creation: Form</p>"},{"location":"Organizers/Benchmark_Creation/Competition-Creation%3A-Bundle/","title":"Competition Creation Bundle","text":"<p>This page is relatively simple. It's where you submit a completed competition bundle to Codabench, in order for it to be processed into a competition instance. For more information on competition bundles, see this link here: Competition Bundle Structure.</p> <p></p> <p>To begin, just click the paper clip icon, or the bar next to it. It should open a file select dialogue. From here, you select your competition bundle, and click upload. Once Codabench is done processing and unpacking your competition, you should be greeted with a success message and a link to your new competition.</p> <p></p>"},{"location":"Organizers/Benchmark_Creation/Competition-Creation%3A-Bundle/#backward-compatibility","title":"Backward compatibility","text":"<p>If you previously used CodaLab Competitions, note that Codabench is compatible with CodaLab bundles.</p>"},{"location":"Organizers/Benchmark_Creation/Competition-Creation%3A-Form/","title":"Competition Creation Form","text":"<p>Competitions can now be created through a wizard/form. This page will cover each different tab of the competition form correlating to a section, and the fields for each section.</p>"},{"location":"Organizers/Benchmark_Creation/Competition-Creation%3A-Form/#details","title":"Details","text":"<p>The details tab covers all basic competition info, such as title, logo, description, and the queue used.</p> <p></p> <ul> <li>Title: The title of your competition.</li> <li>Logo: The logo corresponding to your competition</li> <li>Description: The description of your competition</li> <li>Queue: If you've previously created a queue through queue management, you can assign it to your competition here.</li> </ul>"},{"location":"Organizers/Benchmark_Creation/Competition-Creation%3A-Form/#participation","title":"Participation","text":"<p>The participation tab covers your terms and conditions for the competition, and settings for auto-approval of participants.</p> <p></p> <ul> <li>Terms: Your terms and conditions for the competition.</li> <li>Auto Approve Registration: If checked, the organizer is not required to approve new participants.</li> </ul>"},{"location":"Organizers/Benchmark_Creation/Competition-Creation%3A-Form/#pages","title":"Pages","text":"<p>In the pages section, you can add any additional content you would like to display to competition participants as <code>pages</code>. (Tabs on the competition detail page).</p> <p></p> <p>Clicking <code>Add page</code> should present you a modal with the following layout: </p> <ul> <li>Title: The title of the page you are adding</li> <li>Content: The content of your page formatted as Markdown.</li> </ul>"},{"location":"Organizers/Benchmark_Creation/Competition-Creation%3A-Form/#phases","title":"Phases","text":"<p>The phases section allows you to define your phases and their attached tasks.</p> <p></p> <p>By clicking <code>Add phase</code>, you should be presented with a modal for phase creation: </p> <ul> <li>Name: The name of your phase</li> <li>Start: The start day of your phase</li> <li>End: The end day of your phase</li> <li>Tasks: Here you can assign one or multiple task objects to your phase. Tasks are problems that the submission should be solving. For more information, see the explanation on competition structure here: If you don't have any tasks created yet, click the green button at the bottom of the new phase modal titled <code>Manage Tasks/Datasets</code></li> <li>Description: The description of your phase</li> </ul> <p>Advanced</p> <ul> <li>Execution Time Limit: The time limit for submission execution time measured in seconds (Currently the label says this is measured in MS, but this seems to be false)</li> <li>Max submissions per day: The max submissions allowed for the phase. The time period is from midnight UTC to midnight the next day UTC.</li> <li>Max submissions per person: The absolute max amount of submissions a participant can make on this phase.</li> </ul>"},{"location":"Organizers/Benchmark_Creation/Competition-Creation%3A-Form/#leaderboards","title":"Leaderboards","text":"<p>The leaderboards section allows you to define leaderboards which determine how submissions are scored.</p> <p></p> <p>Clicking add leaderboard should bring up this modal:</p> <p></p> <ul> <li>Title: The title of your leaderboard</li> <li>Key: A unique name to refer to this leaderboard by (Preferably lowercase)</li> </ul> <p>Adding a column will add some fields for the values of the column:</p> <p></p> <ul> <li>Title (Unlabeled top input): The title of your column</li> <li>Primary Column: Whether this is the main column in the leaderboard (I.E: Is the sum/average of other columns)</li> <li>Computation (None/Average): If average is selected, this column should not have a score submitted to it, and will be a computation of the average of all the other columns.</li> <li>Sorting: Determine which way scores are sorted for this column.</li> <li>Column Key: A unique key to refer to this column by.</li> </ul>"},{"location":"Organizers/Benchmark_Creation/Competition-Creation%3A-Form/#collaborators","title":"Collaborators","text":"<p>Here you can add other users to your competition as administrators.</p> <p></p> <p>As per the other pages, clicking <code>Add collaborator</code> should bring up a modal. The search text can be the user's email if you know it, or their username.</p> <p></p>"},{"location":"Organizers/Benchmark_Creation/Competition-docker-image/","title":"Competition Docker Image","text":"<p>The competition docker image defines the docker environment in which the submissions of the competitions or benchmarks are run. Each competition can have a different docker environment, referred by its DockerHub name and tag.</p>"},{"location":"Organizers/Benchmark_Creation/Competition-docker-image/#default-competition-docker-image","title":"Default competition docker image","text":"<p>The default competition docker image is <code>codalab/codalab-legacy:py37</code>.  More information here: https://github.com/codalab/codalab-dockers</p>"},{"location":"Organizers/Benchmark_Creation/Competition-docker-image/#set-up-another-image","title":"Set up another image","text":"<p>You can select another docker image:</p> <ul> <li>In the <code>competition.yaml</code> file, using <code>docker_image: username/image:tag</code></li> <li>In the editor field \"Competition Docker image\" as shown in the following screenshot: </li> </ul>"},{"location":"Organizers/Benchmark_Creation/Competition-docker-image/#building-an-image","title":"Building an image","text":"<p>If the default image does not suit your needs (missing libraries, etc.), you can either:</p> <ul> <li>Select an existing image from DockerHub</li> <li>Create your own image from scratch</li> <li>Create a custom image based on the Codalab image. (more information below)</li> </ul> <p>If you wish to create a custom image based on the Codalab image, you can follow the steps below:</p> <p>1) Install Docker 2) Sign up to DockerHub 3) <code>docker run -itd -u root codalab/codalab-legacy:py39 /bin/bash</code> 4) Use <code>docker ps</code> to find running container id 5) Now run <code>docker  exec -it -u root &lt;CONTAINER ID&gt; bash</code> 6) Install anything you want at the docker container shell (<code>apt-get install</code>, <code>pip install</code>, etc.) 7) Exit the shell with <code>exit</code> 8) <code>docker commit &lt;CONTAINER ID&gt; username/image:tag</code> 9) <code>docker login</code> 10) <code>docker push username/image:tag</code> </p>"},{"location":"Organizers/Benchmark_Creation/Dataset-competition-creation-and-participate-instruction/","title":"Dataset Competition Creation and participate instruction","text":"<p>This page focuses on how to create a dataset contest via bundle and make submission for dataset competition</p>"},{"location":"Organizers/Benchmark_Creation/Dataset-competition-creation-and-participate-instruction/#overall-process","title":"Overall process","text":"<p>The brief process can be summarized in the following diagram</p> <p>There are two main parts: - the contest organizer creates the dataset competition by uploading a bundle (For more information on how to create a contest via bundle, and the definition of bundle, you can refer to this link Competition-Creation:-Bundle)</p> <ul> <li>Competition participant submission dataset</li> </ul>"},{"location":"Organizers/Benchmark_Creation/Dataset-competition-creation-and-participate-instruction/#differences-from-the-code-submission-competition","title":"Differences from the code submission competition","text":""},{"location":"Organizers/Benchmark_Creation/Dataset-competition-creation-and-participate-instruction/#for-the-competition-creator","title":"For the competition creator","text":"<p>The main difference is the definition of the bundle, which differs from the code commit bundle in 2 ways</p>"},{"location":"Organizers/Benchmark_Creation/Dataset-competition-creation-and-participate-instruction/#input-data","title":"Input data","text":"<p> - In the code submission, input data folder is filled with the dataset files</p> <ul> <li>In the dataset submission, input data folder is filled with the sample code submission files(NB: the sample code submission file is the algorithm file to be submitted by the participants in the code submission.)</li> </ul>"},{"location":"Organizers/Benchmark_Creation/Dataset-competition-creation-and-participate-instruction/#ingestion-program","title":"Ingestion program","text":"<p>  - Unlike the code submission, we need to switch the position of the variables $input and $submission_program - In a dataset submission competition, the contents of $submission_program is the dataset submitted by the participant, and the contents of $input is the competition creator's built-in sample code submission.</p>"},{"location":"Organizers/Benchmark_Creation/Dataset-competition-creation-and-participate-instruction/#for-the-competition-participant","title":"For the competition participant","text":"<p> - The left-hand side of the image above shows the contents of the documents that competition participants need to prepare for the code submission competition</p> <ul> <li> <p>the right-hand side of the image above shows the contents of the documents that competition participants need to prepare for the dataset submission competition</p> </li> <li> <p>The competition creator needs to define the bundle by specifying the content of the dataset file to be uploaded </p> </li> <li> <p>For example, the right-hand side of the picture shows the contents of the dataset file required for the HADACA competition to run.</p> </li> <li> <p>Therefore, when competition participants upload their dataset submission, the zip file must contain all the files shown on the right side of the picture above.</p> </li> </ul>"},{"location":"Organizers/Benchmark_Creation/Detailed-Results-and-Visualizations/","title":"Detailed Results and Visualization","text":"<p>Detailed results is a means of passing extra information from the scoring program to the frontend.</p> <p>This is done via writing to a <code>detailed_results.html</code> file (OR any <code>.html</code> -- first by alphabetical order -- in the output folder), and setting <code>enable_detailed_results</code> to <code>True</code> in competition settings (via yaml or editor). </p> <p>This file is watched for changes and updated on the frontend every time the file is updated, so users can get a live feed from the compute worker.</p> <p>There is no limitation to the contents of this HTML file, and can thus be used to relay any information desired. Use case ideas:</p> <ul> <li>Plot data using a python plot library like matplotlib or seaborn.</li> <li>plot the learning curve over time of a reinforcement learning challenge</li> <li>plot the slope of a linear regression model</li> <li>plot the location of clusters in a classification challenge</li> <li>plot anything you can conceive of</li> <li>Run a profiler that outputs a network of method calls. </li> <li>Display any additional data about the submission file that can not be distilled down in to a score of some kind</li> </ul>"},{"location":"Organizers/Benchmark_Creation/Detailed-Results-and-Visualizations/#how-to-include-figures","title":"How to include figures","text":"<p>Figures can be included directly inside the HTML code, by converting them in bytes format. An example is given in the scoring program of the Mini-AutoML bundle.</p> scoring.py<pre><code>[...]\n\n# Path\ninput_dir = '/app/input'    # Input from ingestion program\noutput_dir = '/app/output/' # To write the scores\nreference_dir = os.path.join(input_dir, 'ref')  # Ground truth data\nprediction_dir = os.path.join(input_dir, 'res') # Prediction made by the model\nscore_file = os.path.join(output_dir, 'scores.json')          # Scores\nhtml_file = os.path.join(output_dir, 'detailed_results.html') # Detailed feedback\n\ndef write_file(file, content):\n    \"\"\" Write content in file.\n    \"\"\"\n    with open(file, 'a', encoding=\"utf-8\") as f:\n        f.write(content)\n\ndef make_figure(scores):\n    x = get_dataset_names()\n    y = [scores[dataset] for dataset in x]\n    fig, ax = plt.subplots()\n    ax.plot(x, y, 'bo')\n    ax.set_ylabel('accuracy')\n    ax.set_title('Submission results')\n    return fig\n\ndef fig_to_b64(fig):\n    buf = io.BytesIO()\n    fig.savefig(buf, format='png')\n    buf.seek(0)\n    fig_b64 = base64.b64encode(buf.getvalue()).decode('ascii')\n    return fig_b64\n\ndef main():\n    # Initialized detailed results\n    write_file(html_file, '&lt;h1&gt;Detailed results&lt;/h1&gt;') # Create the file to give real-time feedback\n\n    [...] # compute the scores\n\n    # Create a figure for detailed results\n    figure = fig_to_b64(make_figure(scores))\n    write_file(html_file, f'&lt;img src=\"data:image/png;base64,{figure}\"&gt;')\n</code></pre>"},{"location":"Organizers/Benchmark_Creation/Detailed-Results-and-Visualizations/#example","title":"Example","text":"<p>When the visualization is enabled, a link to the detailed results can be found on the leaderboard for each submission: </p> <p></p> <p>The <code>detailed_results.html</code>, generated by the scoring program, is then shown:</p> <p></p>"},{"location":"Organizers/Benchmark_Creation/Getting-started-with-Codabench/","title":"Getting Started Tutorial","text":"<p>Codabench is an upgraded version of the CodaLab Competitions platform, allowing you to create either competitions or benchmarks. A benchmark is essentially an ever-lasting competition with multiple tasks, for which a participant can make multiple entries in the result table.</p> <p>This getting started tutorial shows a simple example of how to create a competition. Advanced users should check fancier examples and the full documentation. If you simply wish to participate in a benchmark or competition, go to Participating in a benchmark.</p>"},{"location":"Organizers/Benchmark_Creation/Getting-started-with-Codabench/#getting-ready","title":"Getting ready","text":"<ul> <li>Create a Codabench account (if not done yet)</li> <li>Download the sample competition bundle and the sample submission.</li> <li>Do not unzip them.</li> </ul>"},{"location":"Organizers/Benchmark_Creation/Getting-started-with-Codabench/#create-a-competition","title":"Create a competition","text":"<ul> <li>From the front page of Codabench, in the top menu, go to the Benchmark &gt; Management </li> <li>Click the green Upload button at the top right.</li> <li>Upload the (zipped) sample competition bundle =&gt; this will create your competition.</li> </ul>"},{"location":"Organizers/Benchmark_Creation/Getting-started-with-Codabench/#make-changes","title":"Make changes","text":"<ul> <li>Click on the Edit gray button at the top to enter the editor.</li> <li>Make small changes<ul> <li>Change the logo in the Details tab to another png of jpg file.</li> <li>Change the end date in the Phases tab: if the competition is terminated, you will not be able to make submissions.</li> </ul> </li> <li>Save your changes and verify that they have become effective.</li> </ul>"},{"location":"Organizers/Benchmark_Creation/Getting-started-with-Codabench/#make-a-submission","title":"Make a submission","text":"<ul> <li>In your competition page, go to the tab My submissions</li> <li>Submit the (zipped) sample submission bundle you downloaded.</li> <li>When your submission finishes, go to the Result tab to check if it shows up on the leaderboard.</li> </ul> <p>In some competitions, submissions successfully processed do not automatically get pushed to the leaderboard</p>"},{"location":"Organizers/Benchmark_Creation/Getting-started-with-Codabench/#publish-your-competition","title":"Publish your competition","text":"<ul> <li>If you want to make your competition visible to all, you must publish it by checking the Publish box at the very bottom of the editor Details page.</li> <li>After you publish your competition, it should now be visible when you are not logged in, at the URL of your competition page.</li> </ul> <p>You are done with this simple tutorial. Next, check the more advanced tutorial.</p> <p>You can also check out this blog post: How to create your first benchmark on Codabench.</p>"},{"location":"Organizers/Benchmark_Creation/How-to-transition-from-CodaLab-to-Codabench/","title":"How to Transition from Codalab to Codabench?","text":"<p>Codabench serves as a modern, faster, and more reliable upgrade to CodaLab, designed to supersede it. This quick guide is meant for CodaLab users wondering how to successfully adopt this new challenge platform.</p> <p></p>"},{"location":"Organizers/Benchmark_Creation/How-to-transition-from-CodaLab-to-Codabench/#whats-new-in-codabench","title":"What\u2019s new in Codabench?","text":"<p>Codabench includes all features from CodaLab Competitions, and proposes a faster and more intuitive interface. It also has new features such as:</p> <ul> <li>Live logs during submission processes</li> <li>Storage quotas </li> <li>Computation servers management for all users. </li> </ul> <p>It also emphasizes on benchmarking, allowing dataset submissions and multiple leaderboard rows per user. Finally, future project development and maintenance will be focused on Codabench.</p>"},{"location":"Organizers/Benchmark_Creation/How-to-transition-from-CodaLab-to-Codabench/#do-i-need-to-create-a-new-account","title":"Do I need to create a new account?","text":"<p>Yes, even if you previously had a CodaLab account, you need to create a new account on Codabench. Sign up is quick and free. From there, as a competition participant, you are all set. </p> <p>The next questions concern competition organizers.</p>"},{"location":"Organizers/Benchmark_Creation/How-to-transition-from-CodaLab-to-Codabench/#can-i-upload-my-old-competition-bundles-to-codabench","title":"Can I upload my old competition bundles to Codabench?","text":"<p>Yes! That is the good news: competition bundles are back-compatible. This means that you can upload your CodaLab competition bundles into Codabench without any modifications and have them working just fine.</p> <p>Simply go to \u201cBenchmarks &gt; Management\u201d, then click on \u201cUpload\u201d and select your competition bundle.</p> <p></p> <p>Go to \u201cBenchmarks &gt; Management\u201d.</p> <p></p> <p>Click on \u201cUpload\u201d and select your competition bundle.</p> <p>That\u2019s it! Your competition is ready to receive submissions.</p>"},{"location":"Organizers/Benchmark_Creation/How-to-transition-from-CodaLab-to-Codabench/#how-to-move-a-competition-from-codalab-to-codabench","title":"How to move a competition from CodaLab to Codabench?","text":"<p>If you competitition is already live on CodaLab, that is fine too. You simply need to create a dump, download it and re-upload it on Codabench.</p> <p></p> <p>Go to \u201cDumps\u201d organizer interface.</p> <p></p> <p>Click on \u201cCreate Dump\u201d then \u201cDownload\u201d. You\u2019ll obtain a competition bundle that you\u2019ll be able to re-upload on Codabench, following the instructions of the previous section.</p> <p>Leaderboard results won\u2019t be transferred. For that, you\u2019ll need to re-submit the submissions.</p>"},{"location":"Organizers/Benchmark_Creation/How-to-transition-from-CodaLab-to-Codabench/#how-to-create-a-competition-from-scratch-on-codabench","title":"How to create a competition from scratch on Codabench?","text":"<p>If you don\u2019t have any previous competition, and want to learn how to create one from scratch, please refer to the Getting started guide.</p>"},{"location":"Organizers/Benchmark_Creation/How-to-transition-from-CodaLab-to-Codabench/#concluding-remarks","title":"Concluding remarks","text":"<p>Codabench, the new version of the competition and benchmark platform CodaLab, was launched on August 2023 and is already receiving great attention. For users accustomed to CodaLab, the transition to Codabench is quick and easy. Indeed, competition bundles are back-compatible, and all that is required is to create an account on Codabench. To go further, you can refer to Codabench\u2019s Wiki.</p>"},{"location":"Organizers/Benchmark_Creation/Leaderboard-Functionality/","title":"Leaderboard Features","text":"<p>For specific information on leaderboard and column fields, see the explanations in the YAML structure.</p>"},{"location":"Organizers/Benchmark_Creation/Leaderboard-Functionality/#writing-scores","title":"Writing scores","text":"<p>A leaderboard and column are written to via their keys. A leaderboard declaration like so </p><pre><code>leaderboard:\n  - title: Results\n    key: main\n    submission_rule: \"Force_Last\"\n    columns:\n      - title: Accuracy Score 1\n        key: accuracy_1\n        index: 0\n        sorting: desc\n      - title: Accuracy Score 2\n        key: accuracy_2\n        index: 1\n        sorting: desc\n      - title: Max Accuracy\n        key: max_accuracy\n        index: 2\n        sorting: desc\n        computation: max\n        computation_indexes:\n          - 0\n          - 1\n      - title: Duration\n        key: duration\n        index: 3\n        sorting: asc\n</code></pre> would require, via the scoring program, the following <code>scores.json</code> file scores.json<pre><code>{\"accuracy_1\": 0.5, \"accuracy_2\": 0.75, \"duration\": 123.45}\n</code></pre> This is the end result shown on the leaderboard: <pre><code>| Accuracy Score 1 | Accuracy Score 2 | Max Accuracy | Duration |\n|:----------------:|:----------------:|:------------:|:--------:|\n|              0.5 |             0.75 |         0.75 |   123.45 |\n</code></pre><p></p>"},{"location":"Organizers/Benchmark_Creation/Leaderboard-Functionality/#computation","title":"Computation","text":"<p>Scores should not be written to computation columns, instead they will be calculated by the platform at the time scores are read from <code>scores.json</code>.</p> <p>Computation options are:    - sum   - avg   - min   - max</p> <p>These are applied across the columns specified as <code>computation_indexes</code>.</p> <p>So in the example above, the computation option specified is <code>max</code> and the indexes are 0 and 1, meaning we will take the max score of columns at index 0 and 1 (i.e: .5 and .75) so .75 is returned in the computation.</p>"},{"location":"Organizers/Benchmark_Creation/Leaderboard-Functionality/#primary-columns","title":"Primary columns","text":"<p>Ranking is determined first by the primary column of the leaderboard. In the <code>competition.yaml</code>, this is the column at index 0. This option can be changed in the competition editor. After sorting scores by the primary column (asc or desc as specified on the column) sorting then continues from left to right. Final sorting is done by the <code>submitted_at</code> timestamp, so that if submissions have identical scores (as in the case of baselines), the earlier submissions will be ranked higher.</p> <p>Example (with Max Accuracy set as the primary column): </p><pre><code>| Rank | Accuracy Score 1 | Accuracy Score 2 | Max Accuracy | Duration |\n|------|:----------------:|:----------------:|:------------:|:--------:|\n|   1  |              0.5 |             0.75 |         0.75 |   123.45 |\n|   2  |             0.43 |             0.75 |         0.75 |   123.45 |\n|   3  |              0.6 |              0.6 |          0.6 |      100 |  # submitted at Jan 1, 2020\n|   4  |              0.6 |              0.6 |          0.6 |      100 |  # submitted at Jan 2, 2020\n</code></pre> So we sort the submissions by the primary column, (Max Accuracy) and then by columns from left to right, so accuracy 1, then accuracy 2, then duration, then by submission_at.<p></p>"},{"location":"Organizers/Benchmark_Creation/Leaderboard-Functionality/#submission-rules","title":"Submission rules","text":"<p>The submission rule set the behavior of the leaderboard regarding new submissions. Submissions can be forced to the leaderboard or manually selected, can be unique or multiple on the leaderboard, etc.</p> <ul> <li>Add: Only allow adding one submission</li> <li>Add And Delete: Allow users to add a single submission and remove that submission</li> <li>Add And Delete Multiple: Allow users to add multiple submissions and remove those submissions</li> <li>Force Last: Force only the last submission</li> <li>Force Latest Multiple: Force latest submission to be added to leaderboard (multiple)</li> <li>Force Best: Force only the best submission to the leaderboard</li> </ul> <p>Here are the corresponding values for the YAML field <code>submission_rule</code>: \"Add\", \"Add_And_Delete\", \"Add_And_Delete_Multiple\", \"Force_Last\", \"Force_Latest_Multiple\" or \"Force_Best\".</p>"},{"location":"Organizers/Benchmark_Creation/Leaderboard-Functionality/#hidden-leaderboard","title":"Hidden Leaderboard","text":"<p>If a leaderboard is marked as hidden, it will not be visible to participants in the competition. It will only be visible to platform administrators, competition administrators, and competition collaborators.</p>"},{"location":"Organizers/Benchmark_Creation/Leaderboard-Functionality/#downloading-leaderboard-data","title":"Downloading Leaderboard Data","text":"<p>If an administrator, competition administrator, and competition collaborator would like to download the current leaderboard data, they will have access to a button labeled \"CSV\" on the leaderboard page. This creates a downloadable ZIP file. Each CSV file inside will be titled with the name of the leaderboard. The first row of the CSV is the title for each column, followed by all the submissions on the leaderboard. This can be access directly through the API by sending a GET request to <code>[HOSTNAME]/api/competitions/'ID'/get_csv</code> where 'ID' is the competition ID.</p>"},{"location":"Organizers/Benchmark_Creation/Public-Tasks-and-Tasks-Sharing/","title":"Public Tasks and Tasks Sharing","text":"<p>Codabench tasks are a combination of datasets and programs:</p> <ul> <li>Scoring program </li> <li>Ingestion program</li> <li>Input data</li> <li>Reference data</li> </ul> <p>A scoring program is required while others are optional in a task.</p> <p>In the Codabench Resources Interface you can upload datasets and programs in the <code>Datasets &amp; Programs</code> tab and then create a task in the <code>Tasks</code> tab.</p> <p>Example of uploaded datasets and programs: </p> <p>Example of a task created using the above datasets and programs: </p>"},{"location":"Organizers/Benchmark_Creation/Public-Tasks-and-Tasks-Sharing/#make-a-task-public","title":"Make a Task Public","text":"<p>You can make a task public that you have created by clicking on the task name to show task details and then click the button <code>Make Public</code></p>"},{"location":"Organizers/Benchmark_Creation/Public-Tasks-and-Tasks-Sharing/#example-of-task-details","title":"Example of task details:","text":""},{"location":"Organizers/Benchmark_Creation/Public-Tasks-and-Tasks-Sharing/#search-public-tasks","title":"Search Public Tasks","text":"<p>To search public tasks, you can check the <code>Show Public Tasks</code> to view public tasks from other users </p>"},{"location":"Organizers/Benchmark_Creation/Public-Tasks-and-Tasks-Sharing/#use-public-tasks-in-competitions","title":"Use Public Tasks in Competitions","text":"<p>You can use public tasks created by other people in your competitions, to do this follow the steps below:</p> <ol> <li> <p>Open your competition and click <code>Edit</code> button </p> </li> <li> <p>Click the <code>Phases</code> tab and click the edit button in front of the phase where you want to use a public task </p> </li> <li> <p>Start writing the task name in the Tasks field and the matching task will show up. Click the task in the list to select it </p> </li> </ol>"},{"location":"Organizers/Benchmark_Creation/Yaml-Structure/","title":"YAML Structure","text":"<p>This page describes all the attributes in the Codabench competition definition language, using YAML. This is used to create configuration files in Codabench competition bundles.</p> <p>Iris competition YAML file example!</p>"},{"location":"Organizers/Benchmark_Creation/Yaml-Structure/#versioning","title":"Versioning","text":"<p>A version for the YAML is required as this platform can support multiple versions of a <code>competition.yaml</code> file. For examples of v1.5 bundles, look here.</p> <p>For all v2 style competition bundles, be sure to add <code>version: 2</code> to the top of the competition.yml file.</p> <p>Note: Not all features of v1.5 competitions are currently supported in v2.</p>"},{"location":"Organizers/Benchmark_Creation/Yaml-Structure/#competition-properties","title":"Competition Properties","text":""},{"location":"Organizers/Benchmark_Creation/Yaml-Structure/#required","title":"Required","text":"<ul> <li>title: Title of the competition</li> <li>image: File path of competition logo, relative to <code>competition.yaml</code></li> <li>terms: File path to a markdown or HTML page containing the terms of participation participants must agree to before joining a competition</li> </ul>"},{"location":"Organizers/Benchmark_Creation/Yaml-Structure/#optional","title":"Optional","text":"<ul> <li>description: A brief description of the competition.</li> <li>registration_auto_approve: True/False. If True, participation requests will not require manual approval by competition administrators. Defaults to False</li> <li>docker_image: Can specify a specific docker image for the competition to use. Defaults to <code>codalab/codalab-legacy:py3</code>. More information here.</li> <li>make_programs_available: Can specify whether to share the ingestion and scoring program with participants or not. Always available to competition organizer.</li> <li>make_input_data_available: Can specify whether to share the input data with participants or not. Always available to competition organizer.</li> <li>queue: Queue submissions are sent to. Can be used to specify competition specific compute workers. Defaults to the standard queue shared by all competitions. The queue should be referenced by its Vhost, not by its name. You can find the Vhost in <code>Queue Management</code> by clicking the eye button <code>View Queue Detail</code>.</li> <li>enable_detailed_results: True/False. If True, competition will watch for a <code>detailed_results.html</code> file and send its contents to storage. More information here.</li> <li>show_detailed_results_in_submission_panel: a boolean (default: <code>True</code>) If set to <code>True</code>, participants can see detailed results in the submission panel</li> <li>show_detailed_results_in_leaderboard: a boolean (default: <code>True</code>) If set to <code>True</code>, participants can see detailed results in the leaderboard</li> <li>contact_email:  a valid contact email to reach the organizers.</li> <li>reward: a string to show the reward of the competition e.g. \"$1000\" for competition.</li> <li>auto_run_submissions: a boolean (default: <code>True</code>) if set to <code>False</code>, organizers have to manually run the submissions of each participant</li> <li>can_participants_make_submissions_public: a boolean (default: <code>True</code>) if set to <code>False</code>, participants cannot make their submissions public from submissions panel.</li> <li>forum_enabled: a boolean (default: <code>True</code>) if set to <code>False</code>, organizers and participants cannot see or interact with competition forum.</li> </ul> <pre><code># Required\nversion: 2\ntitle: Compute Pi\nimage: images/pi.png\nterms: pages/terms.md\n\n# Optional\ndescription: Calculate pi to as many digits as possible, as quick as you can.\nregistration_auto_approve: True\ndocker_image: codalab/codalab-legacy:py37 # default docker image\nmake_programs_available: True\nmake_input_data_available: False\nenable_detailed_results: True\nshow_detailed_results_in_submission_panel: True\nshow_detailed_results_in_leaderboard: True\ncontact_email: organizer_email@example.com\nreward: $1000 prize pool\nauto_run_submissions: True\ncan_participants_make_submissions_public: False\nforum_enabled: True\n</code></pre>"},{"location":"Organizers/Benchmark_Creation/Yaml-Structure/#pages","title":"Pages","text":""},{"location":"Organizers/Benchmark_Creation/Yaml-Structure/#required_1","title":"Required","text":"<ul> <li>title: String that will be displayed in the competition detail page as the title of the page</li> <li>file: File path to a markdown or HTML page relative to competition.yaml containing the desired content of the page.</li> </ul> <pre><code>pages:\n  - title: Welcome\n    file: welcome.md\n  - title: Getting started\n    file: pages/getting_started.html\n</code></pre>"},{"location":"Organizers/Benchmark_Creation/Yaml-Structure/#phases","title":"Phases","text":""},{"location":"Organizers/Benchmark_Creation/Yaml-Structure/#required_2","title":"Required","text":"<ul> <li>name: Name of the phase</li> <li>start: Datetime string for the start of the competition. ISO format strings are recommended. Use <code>YYYY-MM-DD HH:MM:SS</code> date-time format. (Example date-time: 2024-12-31 14:30:00)</li> <li>end: Datetime string for the end of the phase (optional for last phase only. If not supplied for the final phase, that phase continues indefinitely). Use <code>YYYY-MM-DD HH:MM:SS</code> date-time format. (Example date-time: 2024-12-31 14:30:00)</li> <li>tasks: An array of numbers pointing to the index of any defined tasks relevant to this phase (see tasks for more information)</li> </ul>"},{"location":"Organizers/Benchmark_Creation/Yaml-Structure/#optional_1","title":"Optional","text":"<ul> <li>index: Integer for noting the order of phases, Phases must be sequential, without any overlap. If indexes are not supplied, ordering will be assumed by declaration order.</li> <li>max_submissions: Total submissions allowed per participant for the entire phase</li> <li>max_submissions_per_day: Submission limit for each participant for a given day</li> <li>auto_migrate_to_this_phase: Cannot be set on the first phase of the competition. This will re-submit all successful submissions from the previous phase to this phase at the time the phase starts.</li> <li>execution_time_limit: Execution time limit for submissions, given in seconds. Default is 600.</li> <li>hide_output: True/False. If True, stdout/stderr for all submissions to this phase will be hidden from users who are not competition administrators or collaborators.</li> <li>hide_prediction_output: True/False. If True, participants won't be able to download the \"Output from prediction step\".</li> <li>hide_score_output: True/False. If True, participants won't be able to download the \"Output from scoring step\" containing the <code>scores.txt</code> file.</li> <li>starting_kit: path to the starting kit, a folder that participants will be able to download. Put there any useful files to help participants (example submissions, notebooks, documentation).</li> <li>public_data: path to public data, that participants will be able to download.</li> <li>accepts_only_result_submissions(default=False): When set to True, the phase is expected to accept only result submissions.</li> </ul> <pre><code>phases:\n  - index: 0\n    name: Development Phase\n    description: Tune your models\n    start: 2019-12-12 13:30:00  # Time in UTC+0 and 24-hour format\n    end: 2020-02-01 00:00:00  # Time in UTC+0 and 24-hour format\n    execution_time_limit: 1200\n    starting_kit: starting_kit\n    public_data: public_data\n    accepts_only_result_submissions: True\n    tasks:\n      - 0\n  - index: 1\n    name: Final Phase\n    description: Final testing of your models\n    start: 2020-02-02 00:00:00 # Time in UTC+0 and 24-hour format\n    auto_migrate_to_this_phase: True\n    accepts_only_result_submissions: False\n    tasks:\n      - 1\n</code></pre>"},{"location":"Organizers/Benchmark_Creation/Yaml-Structure/#tasks","title":"Tasks","text":""},{"location":"Organizers/Benchmark_Creation/Yaml-Structure/#required_3","title":"Required","text":"<ul> <li>index: Number used for internal reference of the task, pointed to by solutions (below) and phases (above)</li> <li>name: Name of the Task</li> <li>scoring_program: File path relative to <code>competition.yaml</code> pointing to a <code>.zip</code> file or an unzipped directory, containing the scoring program</li> </ul> <p>OR</p> <ul> <li>key: UUID of a task already in the database. If key is provided, all fields other than index will be ignored</li> </ul>"},{"location":"Organizers/Benchmark_Creation/Yaml-Structure/#optional_2","title":"Optional","text":"<ul> <li>description: Brief description of the task</li> <li>input_data: File path to the data to be provided during the prediction step</li> <li>reference_data: File path to the data to be provided to the scoring program</li> <li>ingestion_program: File path to the ingestion program files</li> <li>ingestion_only_during_scoring: True/False. If true, the ingestion program will be run in parallel with the scoring program, and can communicate w/ the scoring program via a shared directory</li> </ul> <pre><code>tasks:\n  - index: 0\n    name: Compute Pi Developement Task\n    description: Compute Pi, focusing on accuracy\n    input_data: dev_phase/input_data/\n    reference_data: dev_phase/reference_data/\n    ingestion_program: ingestion_program.zip\n    scoring_program: scoring_program.zip\n  - index: 1\n    name: Compute Pi Final Task\n    description: Compute Pi, speed and accuracy matter\n    input_data: final_phase/input_data/\n    reference_data: final_phase/reference_data/\n    ingestion_program: ingestion_program.zip\n    scoring_program: scoring_program.zip\n</code></pre>"},{"location":"Organizers/Benchmark_Creation/Yaml-Structure/#solutions","title":"Solutions","text":""},{"location":"Organizers/Benchmark_Creation/Yaml-Structure/#required_4","title":"Required","text":"<ul> <li>index: Index number of solution</li> <li>tasks: Array of the tasks (referenced internally) for which this solution applies.</li> <li>path: File path to <code>.zip</code> or directory containing the solution data.</li> </ul> <pre><code>solutions:\n  - index: 0\n    path: solutions/solution1.zip\n    tasks:\n    - 0\n    - 1\n  - index: 1\n    path: solutions/solution2/\n    tasks:\n    - 0\n</code></pre>"},{"location":"Organizers/Benchmark_Creation/Yaml-Structure/#fact-sheet","title":"Fact Sheet","text":""},{"location":"Organizers/Benchmark_Creation/Yaml-Structure/#optional_3","title":"Optional","text":"<p>JSON for asking metadata questions about each submission when they are submitted - KEY: Programmatic name for a response. Should not contain any whitespace. - QUESTION TYPE:    - \"checkbox\": Prompts the user with a checkbox for a yes/no or true/false type question       - Required SELECTION: [true, false]     - \"text\":: Prompts the user with a text box to write a response.       - Required SELECTION: \"\"       - <code>\"is_required\": \"false\"</code> will allow the user not to submit a response. Otherwise, the user will have to type something.     - \"select\": Gives the user a dropdown to select a value from.       - SELECTION: Give an array of comma separated values that the user can select from: [\"Value1\",\"Value2\",\"Value3\",...,\"ValueN\"]       - TIP: If you want this selection to be optional you can add \"\" as an option. ex. [\"\", \"Value1\", ...] and set <code>\"is_required\": \"false\"</code> - <code>is_on_leaderboard:</code> setting this to \"true\" will show this response on the leaderboard along with their submission.</p>"},{"location":"Organizers/Benchmark_Creation/Yaml-Structure/#structure","title":"Structure","text":"<pre><code>fact_sheet: {\n    \"[KEY]\": {\n        \"key\": \"[KEY]\",\n        \"type\": \"[QUESTION TYPE]\",\n        \"title\": \"[DISPLAY NAME]\",\n        \"selection\": [SELECTION],\n        \"is_required\": [\"true\" OR \"false\"],\n        \"is_on_leaderboard\": [\"true\" OR \"false\"]\n}\n</code></pre> <pre><code>fact_sheet: {\n    \"bool_question\": {\n        \"key\": \"bool_question\",\n        \"type\": \"checkbox\",\n        \"title\": \"boolean\",\n        \"selection\": [True, False],\n        \"is_required\": \"false\",\n        \"is_on_leaderboard\": \"false\"\n    },\n    \"text_question\": {\n        \"key\": \"text_question\",\n        \"type\": \"text\",\n        \"title\": \"text\",\n        \"selection\": \"\",\n        \"is_required\": \"false\",\n        \"is_on_leaderboard\": \"false\"\n    },\n    \"text_required\": {\n        \"key\": \"text_required\",\n        \"type\": \"text\",\n        \"title\": \"text\",\n        \"selection\": \"\",\n        \"is_required\": \"true\",\n        \"is_on_leaderboard\": \"false\"\n    },\n    \"selection\": {\n        \"key\": \"selection\",\n        \"type\": \"select\",\n        \"title\": \"selection\",\n        \"selection\": [\"\", \"v1\", \"v2\", \"v3\"],\n        \"is_required\": \"false\",\n        \"is_on_leaderboard\": \"true\"\n    }\n}\n</code></pre>"},{"location":"Organizers/Benchmark_Creation/Yaml-Structure/#leaderboards","title":"Leaderboards","text":""},{"location":"Organizers/Benchmark_Creation/Yaml-Structure/#leaderboard-details","title":"Leaderboard Details","text":""},{"location":"Organizers/Benchmark_Creation/Yaml-Structure/#required_5","title":"Required","text":"<ul> <li>title: Title of leaderboard</li> <li>key: Key for scoring program to write to</li> <li>columns: An array of columns (see column layout below)</li> </ul>"},{"location":"Organizers/Benchmark_Creation/Yaml-Structure/#optional_4","title":"Optional","text":"<ul> <li>submission_rule: \"Add\", \"Add_And_Delete\", \"Add_And_Delete_Multiple\", \"Force_Last\", \"Force_Latest_Multiple\" or \"Force_Best\". It sets the behavior of the leaderboard regarding new submissions. See Leaderboard Functionality for more details.</li> <li>hidden: True/False. If True, the contents of this leaderboard will be hidden to all users who are not competition administrators or collaborators.</li> </ul>"},{"location":"Organizers/Benchmark_Creation/Yaml-Structure/#column-details","title":"Column Details","text":""},{"location":"Organizers/Benchmark_Creation/Yaml-Structure/#required_6","title":"Required","text":"<ul> <li>title: Title of the column</li> <li>key: Key for the scoring program to write to. The keys must match the keys of the <code>scores.json</code> file returned by the scoring program, as explained with more details here.</li> <li>index: Number specifying the order the column should show up on the leaderboard</li> </ul>"},{"location":"Organizers/Benchmark_Creation/Yaml-Structure/#optional_5","title":"Optional","text":"<ul> <li>sorting: sorting order for the column: Descending (desc) or Ascending (asc)</li> <li>Ascending: smaller scores are better</li> <li>Descending: larger scores are better</li> <li>computation: computation to be applied must be accompanied by computation indexes</li> <li>computation options: sum, avg, min, max</li> <li>computation_indexes: an array of indexes of the columns the computation should be applied to</li> <li>precision: (integer, default=2) to round the score to precision number of digits</li> <li>hidden: (boolean, default=False) to hide/unhide a column on leaderboard</li> </ul> <pre><code>leaderboards:\n  - title: Results\n    key: main\n    submission_rule: \"Force_Last\"\n    columns:\n      - title: Accuracy Score 1\n        key: accuracy_1\n        index: 0\n        sorting: desc\n        precision: 2\n        hidden: False\n      - title: Accuracy Score 2\n        key: accuracy_2\n        index: 1\n        sorting: desc\n        precision: 3\n        hidden: False\n      - title: Max Accuracy\n        key: max_accuracy\n        index: 2\n        sorting: desc\n        computation: max\n        precision: 3\n        hidden: False\n        computation_indexes:\n          - 0\n          - 1\n      - title: Duration\n        key: duration\n        index: 3\n        sorting: asc\n        precision: 2\n        hidden: False\n</code></pre>"},{"location":"Organizers/Running_a_benchmark/Competition-Detail-Page/","title":"Benchmark Detail Page","text":"<p>The competition detail page is the main way to interact with competitions. This is where your participants (Or if you are a participant) will read the pages you uploaded, register, make submissions, and check results.</p> <p></p> <p>1) Editor (organizer feature) 2) Copy competition secret URL (Document icon, secret URL covered) 3) Competition Detail Tab Navigation  </p> <ul> <li>Get Started</li> <li>Phases</li> <li>My Submissions</li> <li>Results</li> <li>Forum (if enabled)</li> </ul> <p>4) Competition Detail Tab Pane Navigation  </p>"},{"location":"Organizers/Running_a_benchmark/Competition-Detail-Page/#competition-organizer-features","title":"Competition organizer features","text":"<p>These features are only for competition organizers.</p> <p></p> <ul> <li>Editor</li> <li>Manage participants</li> <li>Manage submissions</li> <li>Manage dumps (save the current state of the competition as a bundle)</li> <li>Migration</li> </ul>"},{"location":"Organizers/Running_a_benchmark/Competition-Detail-Page/#submissions","title":"Submissions","text":"<p>From here, you can:  - Delete submissions  - Re-run submissions  - Set a submissions score  - Force a submission to leaderboard</p> <p>You can also view the logs, and all output associated with a submission.</p> <p></p> <ul> <li>Download CSV: Download a CSV file with all submission info</li> <li>Re-run all submissions per phase: Re-runs all submissions in a phase.</li> <li>Search: Used to search for a submission by file name</li> <li>Phase: Used to filter submissions by phase</li> <li>Status: Used to filter submissions by status</li> <li>Action Buttons:</li> <li>Blue Circular Arrow: Re-runs the submission</li> <li>Yellow Cross: Cancels the current submission if it is running</li> <li>Red Trash Can: Deletes the submission</li> <li>Green arrow: Puts this submission on the leaderboard</li> </ul>"},{"location":"Organizers/Running_a_benchmark/Competition-Detail-Page/#participants","title":"Participants","text":"<p>From here, you should be able to:  - Email all participants  - Approve/Deny participants  - Revoke participants</p> <p></p> <ul> <li>Search: Search for a participant by username or email address.</li> <li>Status: Filter participants by status</li> <li>Email Participants: Opens a modal to email all participants:</li> <li>Subject: The email subject</li> <li>Content: The email content</li> </ul> <p></p>"},{"location":"Organizers/Running_a_benchmark/Competition-Detail-Page/#copy-competition-secret-url","title":"Copy competition secret URL","text":"<p>Clicking the document icon copies the competition secret URL to your clipboard.</p>"},{"location":"Organizers/Running_a_benchmark/Competition-Detail-Page/#competition-detail-tab-navigation","title":"Competition Detail Tab Navigation","text":"<p>Used to navigate between the different sections of the competition detail page.</p>"},{"location":"Organizers/Running_a_benchmark/Competition-Detail-Page/#get-started","title":"Get Started","text":"<p>Contains all the organizer made pages, and some defaults.</p> <p></p>"},{"location":"Organizers/Running_a_benchmark/Competition-Detail-Page/#phases","title":"Phases","text":"<p>Contains a diagram list with details on each phase in the order in which they're active.</p> <p></p>"},{"location":"Organizers/Running_a_benchmark/Competition-Detail-Page/#my-submissions","title":"My Submissions","text":"<p>This view contains a table with all your submissions, and allows you to upload new ones.</p> <p></p>"},{"location":"Organizers/Running_a_benchmark/Competition-Detail-Page/#results","title":"Results","text":"<p>This view contains the leaderboard results.</p> <p></p>"},{"location":"Organizers/Running_a_benchmark/Competition-Management-%26-List/","title":"Benchmark Management & List Page","text":"<p>This page will show you how to create, manage, edit and delete your competitions.</p> <p>It will also show you how to track the competitions you are currently in.</p> <p></p>"},{"location":"Organizers/Running_a_benchmark/Competition-Management-%26-List/#competition-create-button-form","title":"Competition create button (Form)","text":"<p>This button will take you to the wizard/form for creating competitions. This will allow you to walk through each step of creating a competition using our creation/edit form. For more information on this form/wizard, please see the following link: Competition Creation: Form</p>"},{"location":"Organizers/Running_a_benchmark/Competition-Management-%26-List/#competition-create-button-upload","title":"Competition create button (Upload)","text":"<p>This button will take you to the upload page for competition bundles. Here you will be able to upload a competition bundle, and if it is validated and processed successfully, you should see a link to your new competition. For more information on this page, please see the following link: Competition Creation: Bundle</p>"},{"location":"Organizers/Running_a_benchmark/Competition-Management-%26-List/#competitions-im-running-tab","title":"Competitions I'm running tab","text":"<p>This should be the default selection for the tab navigation at the top. Having this selected will show you all the competitions you currently run/manage, and the available actions for them.</p>"},{"location":"Organizers/Running_a_benchmark/Competition-Management-%26-List/#competitions-im-in-tab","title":"Competitions I'm in tab","text":"<p>Clicking on this tab will change the main view of the page. You should now see a list of competitions you're competing in (Without any competition administrator options). Clicking any of these titles should bring you to the competition detail page of that competition.</p>"},{"location":"Organizers/Running_a_benchmark/Competition-Management-%26-List/#publish-competition-button","title":"Publish competition button","text":"<p>This button will publish your competition in order to make it publicly available. If your competition is already published, this button will appear green and be used to remove your competition from public availability (It will not be deleted). By default, if your competition is un-published, it appears grey.</p>"},{"location":"Organizers/Running_a_benchmark/Competition-Management-%26-List/#edit-competition-button","title":"Edit competition button","text":"<p>This button will take you to the wizard/form for editing competitions. For more information on the competition edit form, please see the link here</p>"},{"location":"Organizers/Running_a_benchmark/Competition-Management-%26-List/#delete-competition-button","title":"Delete competition button","text":"<p>Deletes your competition. There will be a confirmation dialogue before deletion. We cannot recover deleted competitions.</p>"},{"location":"Organizers/Running_a_benchmark/Competition-Management-%26-List/#competition-link","title":"Competition link","text":"<p>A link to the competition's detail page where users can register, make submissions, view the leaderboard and terms, etc. For more information about the competition detail page, see the link here</p>"},{"location":"Organizers/Running_a_benchmark/Compute-Worker-Management---Setup/","title":"Compute Worker Management & Setup","text":"<p>Compute workers are simply machines that are able to accept/send celery messages on the port used by the broker URL you wish to connect to that have a compute worker image, or other software to receive submissions. This means that you can add computing power to your competitions or benchmarks if needed! Any computer, from your own physical machines to virtual machines on cloud computing services can be used for this purpose. You can add multiple workers to a queue to process several submissions simultaneously.</p> <p>To use Podman, go to the Podman documentation.</p> <p>To use Docker, follow these instructions below:</p> <p>Steps:</p> <ul> <li>Have a machine (either physical or virtual, 100 GB storage recommended)</li> <li>Install Docker</li> <li>Pull Compute Worker Image</li> <li>Run the compute worker via Docker</li> </ul>"},{"location":"Organizers/Running_a_benchmark/Compute-Worker-Management---Setup/#install-docker","title":"Install Docker","text":"<p>Either:</p> <p>a) Install docker via the installation script: https://docs.docker.com/install/linux/docker-ce/ubuntu/#install-using-the-convenience-script</p> <pre><code>curl https://get.docker.com | sudo sh\nsudo usermod -aG docker $USER\n</code></pre> <p>b) Install manually, following the steps at: https://docs.docker.com/install/</p>"},{"location":"Organizers/Running_a_benchmark/Compute-Worker-Management---Setup/#pull-compute-worker-image","title":"Pull Compute Worker Image","text":"<p>On the compute worker machine, run the following command in a shell: </p><pre><code>docker pull codalab/competitions-v2-compute-worker\n</code></pre><p></p> <p>That will pull the latest image for the v2 worker. For specific versions, see the docker hub page at: https://hub.docker.com/r/codalab/competitions-v2-compute-worker/tags</p>"},{"location":"Organizers/Running_a_benchmark/Compute-Worker-Management---Setup/#start-cpu-worker","title":"Start CPU worker","text":"<p>Make a file <code>.env</code> and put this in it: </p>.env<pre><code># Queue URL\nBROKER_URL=&lt;desired broker URL&gt;\n\n# Location to store submissions/cache -- absolute path!\nHOST_DIRECTORY=/codabench\n\n# If SSL isn't enabled, then comment or remove the following line\nBROKER_USE_SSL=True\n</code></pre><p></p> <p>Note</p> <ul> <li> <p>The broker URL is a unique identifier of the job queue that the worker should listen to. To create a queue or obtain the broker URL of an existing queue, you can refer to Queue Management wiki page.</p> </li> <li> <p><code>/codabench</code> -- this path needs to be volumed into <code>/codabench</code> on the worker, as you can see below. You can select another location if convenient.</p> </li> </ul> <p>Create a <code>docker-compose.yml</code> file and paste the following content in it: </p>docker-compose.yml<pre><code># Codabench Worker\nservices:\n    worker:\n        image: codalab/competitions-v2-compute-worker:latest\n        container_name: compute_worker\n        volumes:\n            - /codabench:/codabench\n            - /var/run/docker.sock:/var/run/docker.sock\n        env_file:\n            - .env\n        restart: unless-stopped\n        #hostname: ${HOSTNAME}\n        logging:\n            options:\n                max-size: 50m\n                max-file: 3\n</code></pre><p></p> <p>Note</p> <p><code>hostname: ${HOSTNAME}</code> allows you to set the hostname of the compute worker container, which will then be shown in the server status page on Codabench. This can be set to anything you want, by setting the <code>HOSTNAME</code> environment variable on the machine hosting the Compute Worker, then uncommenting the line the <code>docker-compose.yml</code> before launching the compute worker.</p> <p>You can then launch the worker by running this command in the terminal where the <code>docker-compose.yml</code> file is located: </p><pre><code>docker compose up -d\n</code></pre><p></p>"},{"location":"Organizers/Running_a_benchmark/Compute-Worker-Management---Setup/#deprecated-method-one-liner","title":"Deprecated method (one liner)","text":"<p>Alternately, you can use the docker run below: </p><pre><code>docker run \\\n    -v /codabench:/codabench \\\n    -v /var/run/docker.sock:/var/run/docker.sock \\\n    -d \\\n    --env-file .env \\\n    --name compute_worker \\\n    --restart unless-stopped \\\n    --log-opt max-size=50m \\\n    --log-opt max-file=3 \\\n    codalab/competitions-v2-compute-worker:latest\n</code></pre><p></p>"},{"location":"Organizers/Running_a_benchmark/Compute-Worker-Management---Setup/#start-gpu-worker","title":"Start GPU worker","text":"<p>Make a <code>.env</code> file, as explained in CPU worker instructions.</p> <p>Then, install the NVIDIA toolkit: Nvidia toolkit installation instructions</p> <p>Once you install and configure the NVIDIA container toolkit, you can create a <code>docker-compose.yml</code> file with the following content: </p>docker-compose.yml<pre><code># Codabench GPU worker (NVIDIA)\nservices:\n    worker:\n        image: codalab/competitions-v2-compute-worker:gpu1.3\n        container_name: compute_worker\n        volumes:\n            - /codabench:/codabench\n            - /var/run/docker.sock:/var/run/docker.sock\n        env_file:\n            - .env\n        restart: unless-stopped\n        #hostname: ${HOSTNAME}\n        logging:\n            options:\n                max-size: 50m\n                max-file: 3\n        runtime: nvidia\n        deploy:\n            resources:\n                reservations:\n                    devices:\n                        - driver: nvidia\n                          count: all\n                          capabilities:\n                              - gpu\n</code></pre><p></p> <p>Note</p> <p><code>hostname: ${HOSTNAME}</code> allows you to set the hostname of the compute worker container, which will then be shown in the server status page on Codabench. This can be set to anything you want, by setting the <code>HOSTNAME</code> environment variable on the machine hosting the Compute Worker, then uncommenting the line the <code>docker-compose.yml</code> before launching the compute worker.</p> <p>You can then launch the worker by running this command in the terminal where the <code>docker-compose.yml</code> file is located: </p><pre><code>docker compose up -d\n</code></pre><p></p>"},{"location":"Organizers/Running_a_benchmark/Compute-Worker-Management---Setup/#nvidia-docker-wrapper-deprecated-method","title":"NVIDIA-docker Wrapper (deprecated method)","text":"<p>Nvidia installation instructions </p><pre><code>nvidia-docker run \\\n    -v /codabench:/codabench \\\n    -v /var/run/docker.sock:/var/run/docker.sock \\\n    -v /var/lib/nvidia-docker/nvidia-docker.sock:/var/lib/nvidia-docker/nvidia-docker.sock \\\n    -d \\\n    --env-file .env \\\n    --name compute_worker \\\n    --restart unless-stopped \\\n    --log-opt max-size=50m \\\n    --log-opt max-file=3 \\\n    codalab/competitions-v2-compute-worker:gpu\n</code></pre><p></p> <p>Note that a competition docker image including CUDA and other GPU libraries, such as <code>codalab/codalab-legacy:gpu</code>, is then required.</p>"},{"location":"Organizers/Running_a_benchmark/Compute-Worker-Management---Setup/#check-logs","title":"Check logs","text":"<p>Use the following command to check logs and ensure everything is working fine:</p> <pre><code>docker logs -f compute_worker\n</code></pre>"},{"location":"Organizers/Running_a_benchmark/Compute-Worker-Management---Setup/#cleaning-up-periodically","title":"Cleaning up periodically","text":"<p>It is recommended to clean up docker images and containers regularly to avoid filling up the storage.</p> <ol> <li>Run the following command:</li> </ol> <pre><code>sudo crontab -e\n</code></pre> <ol> <li>Add the following line:</li> </ol> <pre><code>@daily docker system prune -af\n</code></pre>"},{"location":"Organizers/Running_a_benchmark/Compute-Worker-Management---Setup/#keep-track-of-the-worker","title":"Keep track of the worker","text":"<p>It is recommended to store the docker container hostname to identify the worker. This way, it is easier to troubleshoot issues when having multiple workers in one queue. To get the hostname, simply run <code>docker ps</code> and look at the key <code>CONTAINER ID</code> at the beginning of the output:</p> <pre><code>$ docker ps\nCONTAINER ID   IMAGE                                           COMMAND                  CREATED      STATUS      PORTS     NAMES\n1a2b3d4e5f67   codalab/competitions-v2-compute-worker:latest   \"/bin/sh -c 'celery \u2026\"   3 days ago   Up 3 days             compute_worker\n</code></pre> <p>For each submission made to your queue, you can know what worker computed the ingestion and the scoring jobs in the server status page.</p>"},{"location":"Organizers/Running_a_benchmark/Compute-Worker-Management---Setup/#optional-put-data-directly-inside-the-compute-worker","title":"Optional: put data directly inside the compute worker","text":"<p>The folder <code>$HOST_DIRECTORY/data</code>, usually <code>/codabench/data</code>, is shared between the host (the compute worker) and the container running the submission (a new container is created for each submission). It is mounted inside the container as <code>/app/data</code>. This means that you can put data in your worker, in <code>$HOST_DIRECTORY/data</code>, so it can be read-only accessed during the job's process. You'll need to modify the scoring and/or ingestion programs accordingly, to points to <code>/app/data</code>. This is especially useful if you work with confidential data, or with a heavy dataset.</p> <p>If you have several workers in your queue, remember to have the data accessible for each one.</p> <p></p> <p>If you simply wish to set up some compute workers to increase the computing power of your benchmark, you don't need to scroll this page any further.</p>"},{"location":"Organizers/Running_a_benchmark/Compute-Worker-Management---Setup/#building-compute-worker","title":"Building compute worker","text":"<p>This is helpful only if you want to build the compute worker image. It is not needed if you simply want to set up compute workers to run submissions.</p> <p>To build the normal image:</p> <pre><code>docker build -t codalab/competitions-v2-compute-worker:latest -f Dockerfile.compute_worker .\n</code></pre> <p>To build the GPU version: </p><pre><code>docker build -t codalab/competitions-v2-compute-worker:gpu -f Dockerfile.compute_worker_gpu .\n</code></pre><p></p> <p>To update the image (add tag <code>:latest</code>, <code>:gpu</code> or else if needed)</p> <pre><code>docker push codalab/competitions-v2-compute-worker\n</code></pre> <p>If you have running compute workers, you'll need to pull again the image and to restart the workers to take into account the changes.</p>"},{"location":"Organizers/Running_a_benchmark/Compute-Worker-Management---Setup/#worker-management","title":"Worker management","text":"<p>Outside of docker containers install Fabric like so:</p> <pre><code>pip install fab-classic==1.17.0\n</code></pre> <p>Create a <code>server_config.yaml</code> in the root of this repository using: </p><pre><code>cp server_config_sample.yaml server_config.yaml\n</code></pre><p></p> <p>Below is an example <code>server_config.yaml</code> that defines 2 roles <code>comp-gpu</code> and <code>comp-cpu</code>, one with GPU style workers (<code>is_gpu</code> and the GPU <code>docker_image</code>) and one with CPU style workers</p> server_config.yaml<pre><code>comp-gpu:\n  hosts:\n    - ubuntu@12.34.56.78\n    - ubuntu@12.34.56.79\n  broker_url: pyamqp://user:pass@host:port/vhost-gpu\n  is_gpu: true\n  docker_image: codalab/competitions-v2-compute-worker:gpu\n\ncomp-cpu:\n  hosts:\n    - ubuntu@12.34.56.80\n  broker_url: pyamqp://user:pass@host:port/vhost-cpu\n  is_gpu: false\n  docker_image: codalab/competitions-v2-compute-worker:latest\n</code></pre> <p>You can of course create your own <code>docker_image</code> and specify it here.</p> <p>You can execute commands against a role:</p> <pre><code>fab -R comp-gpu status\n..\n[ubuntu@12.34.56.78] out: CONTAINER ID        IMAGE                                           COMMAND                  CREATED             STATUS              PORTS               NAMES\n[ubuntu@12.34.56.78] out: 1d318268bee1        codalab/competitions-v2-compute-worker:gpu   \"/bin/sh -c 'celery \u2026\"   2 hours ago         Up 2 hours                              hardcore_greider\n..\n\nfab -R comp-gpu update\n..\n(updates workers)\n</code></pre> <p>See available commands with <code>fab -l</code></p>"},{"location":"Organizers/Running_a_benchmark/Compute-Worker-Management---Setup/#update-docker-image","title":"Update docker image","text":"<p>If the compute worker docker image was updated, you can reflect the changes using the following commands.</p> <p>Check no job is running:</p> <pre><code>docker ps\n</code></pre> <p>Update the worker:</p> <pre><code>docker stop compute_worker\ndocker rm compute_worker\ndocker pull codalab/competitions-v2-compute-worker:latest    # or other relevant docker image\ndocker run \\                                                 # or docker compose up -d\n    -v /codabench:/codabench \\\n    -v /var/run/docker.sock:/var/run/docker.sock \\\n    -d \\\n    --env-file .env \\\n    --name compute_worker \\\n    --restart unless-stopped \\\n    --log-opt max-size=50m \\\n    --log-opt max-file=3 \\\n    codalab/competitions-v2-compute-worker:latest            # or other relevant docker image\n</code></pre>"},{"location":"Organizers/Running_a_benchmark/Compute-worker-installation-with-Podman/","title":"Compute Worker Management with Podman","text":"<p>Here is the specification for compute worker installation by using Podman. </p>"},{"location":"Organizers/Running_a_benchmark/Compute-worker-installation-with-Podman/#requirements-for-the-host-machine","title":"Requirements for the host machine","text":"<p>We need to install Podman on the VM. We use Debian based OS, like Ubuntu. Ubuntu is recommended, because it has better Nvidia driver support. </p> <p><code>sudo apt install podman</code></p> <p>After installing Podman, you will need to launch the service associated to it with <code>systemctl --user enable --now podman</code></p> <p>Then, configure where Podman will download the images: Podman will use Dockerhub by adding this line into <code>/etc/containers/registries.conf</code>:</p> <p><code>unqualified-search-registries = [\"docker.io\"]</code></p> <p>Create the <code>.env</code> file in order to add the compute worker into a queue (here, the default queue is used. If you use a particular queue, then, fill in your BROKER_URL generated when creating this particular queue) : </p> .env<pre><code>BROKER_URL=pyamqp://&lt;login&gt;:&lt;password&gt;@codabench-test.lri.fr:5672 \nHOST_DIRECTORY=/codabench\n# If SSL isn't enabled, then comment or remove the following line\nBROKER_USE_SSL=True\nCONTAINER_ENGINE_EXECUTABLE=podman\n</code></pre> <p>You will also need to create the <code>codabench</code> folder defined in the <code>.env</code> file, as well as change its permissions to the user that is running the compute worker.</p> In your terminal<pre><code>sudo mkdir /codabench\nsudo mkdir /codabench/data\nsudo chown -R $(id -u):$(id -g) /codabench\n</code></pre> <p>You should also run the following command if you don't want the container to be shutdown when you log out of the user: </p><pre><code>sudo loginctl enable-linger *username*\n</code></pre> Make sure to use the username of the user running the podman container.<p></p>"},{"location":"Organizers/Running_a_benchmark/Compute-worker-installation-with-Podman/#for-gpu-compute-worker-vm","title":"For GPU compute worker VM","text":"<p>You need to install nvidia packages supporting Podman and nvidia drivers:</p> <pre><code>distribution=$(. /etc/os-release;echo $ID$VERSION_ID) \\\n    &amp;&amp; curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add - \\\n    &amp;&amp; curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-container.list\nsudo apt update\nsudo apt install nvidia-container-runtime nvidia-containe-toolkit nvidia-driver-&lt;version&gt;\n</code></pre> <p>Edit the nvidia runtime config</p> <pre><code>sudo sed -i 's/^#no-cgroups = false/no-cgroups = true/;' /etc/nvidia-container-runtime/config.toml\n</code></pre> <p>Check if nvidia driver is working, by executing:</p> <pre><code>nvidia-smi\n\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 520.61.05    Driver Version: 520.61.05    CUDA Version: 11.8     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  NVIDIA GeForce ...  On   | 00000000:01:00.0 Off |                  N/A |\n| 27%   26C    P8    20W / 250W |      1MiB / 11264MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n</code></pre> <p>The result should show gpu card information.</p> <p>We need to configure the OCI hook (entry point to inject code) script for nvidia. Create this file <code>/usr/share/containers/oci/hooks.d/oci-nvidia-hook.json</code> if not exists:</p> oci-nvidia-hook.json<pre><code>{\n    \"version\": \"1.0.0\",\n    \"hook\": {\n        \"path\": \"/usr/bin/nvidia-container-toolkit\",\n        \"args\": [\"nvidia-container-toolkit\", \"prestart\"],\n        \"env\": [\n            \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"\n        ]\n    },\n    \"when\": {\n        \"always\": true,\n        \"commands\": [\".*\"]\n    },\n    \"stages\": [\"prestart\"]\n}\n</code></pre> <p>Validating if all are working by running a test container:</p> <p></p><pre><code>podman run --rm -it \\\n --security-opt=\"label=disable\" \\\n --hooks-dir=/usr/share/containers/oci/hooks.d/ \\\n nvidia/cuda:11.6.2-base-ubuntu20.04 nvidia-smi\n</code></pre> The result should show as same as the command <code>nvidia-smi</code> above.<p></p> <p>You will also need to add this line in your <code>.env</code> file: </p><pre><code>USE_GPU=True\n</code></pre><p></p>"},{"location":"Organizers/Running_a_benchmark/Compute-worker-installation-with-Podman/#compute-worker-installation","title":"Compute worker installation","text":""},{"location":"Organizers/Running_a_benchmark/Compute-worker-installation-with-Podman/#for-cpu-container","title":"For CPU container","text":"<p>Run the compute worker container : </p> <pre><code>podman run -d \\\n --volume /run/user/$(id -u)/podman/podman.sock:/run/user/1000/podman/podman.sock:U \\\n --env-file .env \\\n --name compute_worker \\\n --security-opt=\"label=disable\" \\\n --userns host \\\n --restart unless-stopped \\\n --log-opt max-size=50m \\\n --log-opt max-file=3 \\\n --cap-drop all \\\n --volume /codabench:/codabench:U,z \\\n codalab/codabench_worker_podman:latest \n</code></pre>"},{"location":"Organizers/Running_a_benchmark/Compute-worker-installation-with-Podman/#for-gpu-container","title":"For GPU container","text":"<p>Run the GPU compute worker container</p> <pre><code>podman run -d \\\n    --env-file .env \\\n    --device nvidia.com/gpu=all \\\n    --name gpu_compute_worker \\\n    --device /dev/fuse \\\n    --security-opt=\"label=disable\" \\\n    --restart unless-stopped \\\n    --log-opt max-size=50m \\\n    --log-opt max-file=3 \\\n    --hostname ${HOSTNAME} \\\n    --userns host \\\n    --volume /home/codalab/worker/codabench:/codabench:z,U \\\n    --cap-drop=all \\\n    --volume /run/user/$(id -u)/podman/podman.sock:/run/user/1000/podman/podman.sock:U \\\n    codalab/codabench_worker_podman_gpu:latest\n</code></pre>"},{"location":"Organizers/Running_a_benchmark/Queue-Management/","title":"Queue Management","text":"<p>The queue management page lists all queues you have access to, and optionally all current public queues. </p> <p>For queues you've created, it will also show options for editing, deleting, copying and displaying the broker URL. </p> <p>You can also create new queues from this page. You can use the server status page to have an overview of the submissions made to a queue you own.</p> <p></p> <p>1) Show Public Queues  2) Create Queue  3) Action Buttons </p> <ul> <li>Eye Icon</li> <li>Document Icon</li> <li>Edit Icon</li> <li>Trash Icon</li> </ul>"},{"location":"Organizers/Running_a_benchmark/Queue-Management/#show-public-queues","title":"Show Public Queues","text":"<p>Enabling this checkbox will display public queues as well as queues you organized, or have been given access to. You will not be able to edit them, but you can view queue details and copy the broker URL.</p>"},{"location":"Organizers/Running_a_benchmark/Queue-Management/#create-queue","title":"Create Queue","text":"<p>Clicking this button will bring up a modal with the queue form.</p> <p></p> <p>The following fields are present:</p> <ul> <li>Name: The name of the queue</li> <li>Make Public: If checked, this queue will be available for public use.</li> <li>Collaborators: A multi-select field that you can search for users by username or email. These will be people who have access to your queue.</li> </ul>"},{"location":"Organizers/Running_a_benchmark/Queue-Management/#action-buttons","title":"Action Buttons","text":""},{"location":"Organizers/Running_a_benchmark/Queue-Management/#eye-icon","title":"Eye Icon","text":"<p>Clicking this button will show you details about your queue such as the Broker URL and Vhost name. </p>"},{"location":"Organizers/Running_a_benchmark/Queue-Management/#document-icon","title":"Document Icon","text":"<p>The document icon is used to copy the broker URL to your clipboard with one-click.</p>"},{"location":"Organizers/Running_a_benchmark/Queue-Management/#edit-icon","title":"Edit Icon","text":"<p>The edit icon brings up the queue modal/form for editing the current queue.</p>"},{"location":"Organizers/Running_a_benchmark/Queue-Management/#trash-icon","title":"Trash Icon","text":"<p>The trash icon deletes the current queue. There will be a confirmation dialogue. Once this is done your queue is gone forever so be careful.</p>"},{"location":"Organizers/Running_a_benchmark/Queue-Management/#compute-workers-setup","title":"Compute workers setup","text":"<p>See compute worker management and setup for more information about workers configuration.</p> <p>Internal and external compute workers can be linked to Codabench competitions. The queues dispatch the jobs between the compute workers. Note that a queue can receive jobs (submissions) from several competitions, and can send them to several compute workers. The general architecture of queues and workers can be represented like this:</p> <p></p>"},{"location":"Organizers/Running_a_benchmark/Resource-Management/","title":"Ressource Management Submissions, Datasets/Programs, Tasks and Competition Bundles","text":"<p>This page is where you can manage your resources e.g. Submissions, Datasets, Programs, Tasks, and Competition Bundles. You can also view and manage your quota on this page.</p> <p>You can access this interface by clicking on \"Resources\" in the main menu: </p>"},{"location":"Organizers/Running_a_benchmark/Resource-Management/#submissions","title":"Submissions","text":"<p>In this tab, you can view all your submissions either uploaded in this interface or submitted to a competition. </p> <p>By clicking the <code>Add Submission</code> button you can fill a form and attach a submission file to upload a new submission. This is useful in different cases e.g. when you want to share a sample submission with the participants of a competition you are organizing. </p>"},{"location":"Organizers/Running_a_benchmark/Resource-Management/#datasetsprograms","title":"Datasets/Programs","text":"<p>In this tab,  you can view the datasets and programs that you have uploaded. You can also view auto-created and publicly available datasets/programs by checking the relevant checkboxes. </p> <p></p> <p>By clicking the <code>Add Dataset/Program</code> button you can fill a form and attach a dataset file to upload.</p> <p></p> <p>You can click on a dataset/program and make it public or private. This is useful when you want to share a dataset with participants so that hey can use it to prepare a submission for a competition.</p> <p></p> <p>For a general breakdown of the roles of different types of datasets, see this link: Competition Bundle Structure: Data types and their role.</p>"},{"location":"Organizers/Running_a_benchmark/Resource-Management/#tasks","title":"Tasks","text":"<p>In this tab, you can manage your tasks. You can create a new task, upload a task, edit a task and check task details. </p>"},{"location":"Organizers/Running_a_benchmark/Resource-Management/#create-new-task","title":"Create New Task","text":"<p>To create a new task, you have to fill the form by entering task name and description </p> <p>You also have to select datasets and programs from the already uploaded ones in the Datasets/Programs tab </p>"},{"location":"Organizers/Running_a_benchmark/Resource-Management/#edit-a-task","title":"Edit a Task","text":"<p>You can change the task name and description  </p> <p>You can also change the datasets/programs used in the task </p> <p>Note</p> <p>Organizers should be careful when updating a task because some submissions may have used the task and updating the task will not allow you to rerun those submissions because the task they have used is now changed.</p>"},{"location":"Organizers/Running_a_benchmark/Resource-Management/#upload-a-task","title":"Upload a Task","text":"<p>You can create a new task by uploading a task zip that has the required files in the correct format. </p> <p>Create a zip file that consists of a <code>task.yaml</code> file and zips of datasets/programs if required. You can use already existing datasets/prograsm by using their keys in the yaml, or upload new datasets/programs or use a mix of keys and files e.g. you choose to use already existing input data and reference data but use zip files for ingestion and scoring program. In the last case, codabench will create two programs and then use them in your task and will use existing datasets in the same task.</p> <p>Check the files below for examples of task upload zips.</p> <ul> <li>task_with_keys_only.zip</li> <li>task_with_files_only.zip</li> <li>task_with_mix_of_keys_and_files.zip</li> </ul> <p>For reference, here is the content of the <code>task.yaml</code> file that you can find inside the <code>task_with_mix_of_keys_and_files.zip</code> task: </p>task.yaml<pre><code>name: Iris Task\ndescription: Iris Task for Flower classification\nis_public: false\nscoring_program:\n  zip: iris-scoring-program.zip\ningestion_program:\n  zip: iris-ingestion-program.zip\ninput_data:\n  key: 6c3e6dde-d0fa-4c22-af66-030187dbfd4f\nreference_data:\n  key: c4179c3f-498c-486a-8ac5-1e194036a3ed\n</code></pre><p></p>"},{"location":"Organizers/Running_a_benchmark/Resource-Management/#task-details","title":"Task Details","text":"<p>In the task details, you can view all the task details e.g. title, description, task owner, created date, people with whom this task is shared, competitions where this task is used, the datasets/programs used in this task and option to download them, and option to make the task public/private. </p>"},{"location":"Organizers/Running_a_benchmark/Resource-Management/#competition-bundles","title":"Competition Bundles","text":"<p>In this tab, you can mange your competition bundles. These bundles are stored when you create your competitions using a zip.  </p>"},{"location":"Organizers/Running_a_benchmark/Resource-Management/#quota-and-cleanup","title":"Quota and Cleanup","text":"<p>This section of the resource interface shows you the usage of your quota. A free quota of 15 GB is given to all the users and this can be increased by the platform administrators in special circumstances for selected users. You can also do some quick cleanup from here by deleting unused resources e.g. submissions, datasets and tasks etc. </p>"},{"location":"Organizers/Running_a_benchmark/Server-status-page/","title":"Server Status","text":"<p>The server status page gives information about past and current submissions, and is useful for troubleshooting. Any user can access the interface, and get information about their own submissions and the submissions made to queues they own. Administrators can see all submissions. Here is an overview of the page:</p> <p></p> <p>Note that <code>*</code> refers to the default queue of the platform. \"Hostname\" refers to the docker container ID of the compute worker that computed the job.</p>"},{"location":"Organizers/Running_a_benchmark/Server-status-page/#how-to-access-the-interface","title":"How to access the interface","text":"<p>You can access it either from the top right menu, or from the \"Queue management\" page, as shown in the screenshots below.</p> <p></p> <p></p>"},{"location":"Organizers/Running_a_benchmark/Update-programs-or-data/","title":"Update programs or data","text":"<p>In this page, you'll learn how to update critical elements of your benchmark like the scoring program or the reference data, while your benchmark is already up and running.</p> <p>For a general overview of resources management, click here.</p> <p>If order to update your programs or data, you have two approaches: - A. Edit an existing Task (simpler and straightforward) - B. Create a new Task</p> <p>Let's see both approach in detail.</p>"},{"location":"Organizers/Running_a_benchmark/Update-programs-or-data/#a-edit-an-existing-task","title":"A. Edit an existing Task","text":""},{"location":"Organizers/Running_a_benchmark/Update-programs-or-data/#1-prepare-the-new-dataset-or-program","title":"1. Prepare the new dataset or program","text":"<ul> <li> <p>Make local changes to the elements you want to update: scoring program, ingestion program, input data and/or reference data.</p> </li> <li> <p>Zip the new version of your program or data. Make sure to zip the files at the root of the archive, without zipping the folder structure.</p> </li> </ul>"},{"location":"Organizers/Running_a_benchmark/Update-programs-or-data/#2-upload-the-new-dataset-or-program","title":"2. Upload the new dataset or program","text":"<ul> <li>Go to Resources</li> </ul> <ul> <li>Go to \"Datasets and Programs\" and click on \"Add Dataset/Program\"</li> </ul> <ul> <li>Fill in the form: Name the new program or dataset, select the type (scoring program, input data, etc.), and select your ZIP file</li> </ul>"},{"location":"Organizers/Running_a_benchmark/Update-programs-or-data/#3-update-the-task-used-by-your-benchmark","title":"3. Update the Task used by your benchmark","text":"<p>Still on \"Resources\" page, go to the \"Task\" tab. Find the task you want to edit. In order to recognize it, make sure it is marked as \"In Use\", and click to see more information and make sure it is related to the right benchmark.</p> <p>Then click on the pencil symbol to edit it: </p> <p>Start typing the name of your new program or dataset in the corresponding field and select it, then save.</p> <p></p> <p>Done! Your task is updated and its new version will be triggered by new submissions. You don't need to update the benchmark/competition for the change to take effect.</p>"},{"location":"Organizers/Running_a_benchmark/Update-programs-or-data/#b-create-a-new-task","title":"B. Create a new Task","text":""},{"location":"Organizers/Running_a_benchmark/Update-programs-or-data/#1-prepare-your-new-dataset-program","title":"1. Prepare your new dataset / program","text":"<p>First, upload the new versions of your program and/or dataset. To that end, follow steps 1. and 2. presented above.</p>"},{"location":"Organizers/Running_a_benchmark/Update-programs-or-data/#2-create-task","title":"2. Create Task","text":"<ul> <li>Go to \"Resources\" &gt; \"Task\" &gt; \"Create Task\"</li> </ul> <ul> <li>Fill in all fields: Name, Description, Scoring program, (optionally: Ingestion program, Reference data, Input data)</li> </ul>"},{"location":"Organizers/Running_a_benchmark/Update-programs-or-data/#edit-your-benchmark","title":"Edit your benchmark","text":"<ul> <li>Once your task is created, go to the editor of your challenge</li> </ul> <ul> <li>Go to \"Phases\" and edit the relevant phase</li> </ul> <ul> <li>Select your new task and save</li> </ul> <p>Done! Your benchmark is now ready to run your new task for future submissions.</p>"},{"location":"Participants/User_Participating-in-a-Competition/","title":"Participating in a Competition","text":""},{"location":"Participants/User_Participating-in-a-Competition/#signing-up-and-updating-your-settings","title":"Signing up and updating your settings","text":"<p>When you sign up, you will have to provide your name and a valid email.</p>"},{"location":"Participants/User_Participating-in-a-Competition/#registering-for-a-benchmark","title":"Registering for a Benchmark","text":"<p>To make an entry in a benchmark click on the \"My Submissions\" tab, you will then be prompted to accept the rules to register to that benchmark. When registering, a request may be sent to the benchmark organizer. You will be notified when the benchmark organizer has approved your registration request. Follow the instructions of the organizers.</p> <p></p>"},{"location":"Participants/User_Participating-in-a-Competition/#making-a-submission","title":"Making a Submission","text":"<p>Making a submission to a benchmark involves uploading a bundle (.zip archive) containing files with your answer, in the format that has been specified by the benchmark organizer. There are two types of submissions:  - Code submissions contain a metadata file specifying the command to execute - Results submission contain the solution to the problem (no code executed on the platform)</p> <p>To make a submission</p> <ol> <li>Sign in to Codabench. If you do not have an account, you will need to create one.</li> <li>Select the benchmark you want to work with.</li> <li>Click the My Submissions tab. Here, you can access the data that has been provided by the benchmark organizer.</li> <li>Click on the paper clip logo and select your zip file.</li> </ol> <p></p> <p>On this page, you can make new submissions, and see previous submissions for each phase in the competition.</p> <p>You can also view all your submissions in the Resources Interface.</p>"},{"location":"Participants/User_Participating-in-a-Competition/#viewing-benchmark-results","title":"Viewing Benchmark Results","text":"<p>You can keep up with the progress of benchmarks you are participating in by clicking on the Results tab. This will display the leaderboard.</p> <p></p>"}]}