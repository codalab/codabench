version: '3'
services:
  #-----------------------------------------------
  #   Web Services
  #-----------------------------------------------
  caddy:
    image: caddy:2.8.4
    env_file: .env
    environment:
      - ACME_AGREE=true
    volumes:
      - ./Caddyfile:/etc/caddy/Caddyfile
      - ./src/staticfiles:/var/www/django/static
      - ./caddy_data:/data
      - ./caddy_config:/config
      - ./var/log/caddy:/var/log/
      - ./maintenance_mode/:/srv
    restart: unless-stopped
    ports:
      - 80:80
      - 443:443
    depends_on:
      - django

  django:
    build: .
    # NOTE: We use watchmedo to reload gunicorn nicely, Uvicorn + Gunicorn reloads don't work well
    command: bash -c "python manage.py collectstatic --noinput && cd /app/src &&  watchmedo auto-restart -p '*.py' --recursive -- gunicorn asgi:application -w 2 -k uvicorn.workers.UvicornWorker -b :8000 -b :80 --capture-output --log-level debug"
    environment:
      - DATABASE_URL=postgres://${DB_USERNAME}:${DB_PASSWORD}@${DB_HOST}:${DB_PORT}/${DB_NAME}
    env_file: .env
    volumes:
      - .:/app:delegated
      - /tmp/codalab-v2/django:/codalab_tmp
      - ./backups:/app/backups
      - ./var/logs:/app/logs
    restart: unless-stopped
    ports:
      - 8000:8000
    depends_on:
      - db
      - rabbit
      - minio
    stdin_open: true
    tty: true
    logging:
      options:
        max-size: "20m"
        max-file: "5"


  #-----------------------------------------------
  # Minio local storage helper
  #-----------------------------------------------
  minio:
    image: minio/minio:RELEASE.2020-10-03T02-19-42Z
    command: server /export
    volumes:
      - ./var/minio:/export
    restart: unless-stopped
    ports:
      - $MINIO_PORT:9000
    env_file: .env
    healthcheck:
      test: ["CMD", "nc", "-z", "minio", "9000"]
      interval: 5s
      retries: 5
  createbuckets:
    image: minio/mc
    depends_on:
      minio:
        condition: service_healthy
    env_file: .env
    # volumes:
    #   This volume is shared with `minio`, so `z` to share it
    #   - ./var/minio:/export
    entrypoint: >
      /bin/sh -c "
      set -x;
      if [ -n \"$MINIO_ACCESS_KEY\" ] && [ -n \"$MINIO_SECRET_KEY\" ] && [ -n \"$MINIO_PORT\" ]; then
        until /usr/bin/mc config host add minio_docker http://minio:$MINIO_PORT $MINIO_ACCESS_KEY $MINIO_SECRET_KEY && break; do 
          echo '...waiting...' && sleep 5; 
        done;
        /usr/bin/mc mb minio_docker/$AWS_STORAGE_BUCKET_NAME || echo 'Bucket $AWS_STORAGE_BUCKET_NAME already exists.';
        /usr/bin/mc mb minio_docker/$AWS_STORAGE_PRIVATE_BUCKET_NAME || echo 'Bucket $AWS_STORAGE_PRIVATE_BUCKET_NAME already exists.';
        /usr/bin/mc anonymous set download minio_docker/$AWS_STORAGE_BUCKET_NAME;
      else
        echo 'MINIO_ACCESS_KEY, MINIO_SECRET_KEY, or MINIO_PORT are not defined. Skipping buckets creation.';
      fi;
      exit 0;
      "

  #-----------------------------------------------
  #   Local development helper, rebuilds RiotJS/Stylus on change
  #-----------------------------------------------
  builder:
    build:
      context: .
      dockerfile: Dockerfile.builder
    volumes:
      - .:/app:delegated
    restart: unless-stopped
    logging:
      options:
        max-size: "20m"
        max-file: "5"


  #-----------------------------------------------
  #   Database Service
  #-----------------------------------------------
  db:
    image: postgres:12-alpine
    env_file: .env
    environment:
      - PGDATA=/var/lib/postgresql/data/pgdata
      - POSTGRES_PASSWORD=${DB_PASSWORD}
    command: ["postgres", "-c", "log_statement=all", "-c", "log_destination=stderr"]
    ports:
      - 5432:5432
    volumes:
      - ./var/postgres:/var/lib/postgresql/data:delegated
      - ./backups:/app/backups
    restart: unless-stopped
    logging:
      options:
        max-size: "20m"
        max-file: "5"

  #-----------------------------------------------
  #   Rabbitmq & Flower monitoring tool
  #-----------------------------------------------
  rabbit:
    build:
      context: .
      dockerfile: Dockerfile.rabbitmq
      args:
        - WORKER_CONNECTION_TIMEOUT=${WORKER_CONNECTION_TIMEOUT}
    # setting hostname here makes data persist properly between
    # containers being destroyed..!
    hostname: rabbit
    env_file: .env
    environment:
      - http_proxy=${RABBITMQ_HTTP_PROXY}
      - https_proxy=${RABBITMQ_HTTPS_PROXY}
      - no_proxy=${RABBITMQ_NO_PROXY}
    ports:
      - ${RABBITMQ_MANAGEMENT_PORT:-15672}:15672
      - ${RABBITMQ_PORT}:5672
    volumes:
      - ./var/rabbit:/var/lib/rabbitmq
    restart: unless-stopped
    logging:
      options:
        max-size: "20m"
        max-file: "5"

  flower:
    # image: mher/flower
    build:
      context: .
      dockerfile: Dockerfile.flower
    env_file: .env
    environment:
      - CELERY_BROKER_URL=pyamqp://${RABBITMQ_DEFAULT_USER}:${RABBITMQ_DEFAULT_PASS}@${RABBITMQ_HOST}:${RABBITMQ_PORT}//
    restart: unless-stopped
    ports:
      - ${FLOWER_PUBLIC_PORT:-5555}:5555
    depends_on:
      - rabbit
    logging:
        options:
            max-size: "20m"
            max-file: "5"

  #-----------------------------------------------
  #   Redis
  #-----------------------------------------------
  redis:
    image: redis
    ports:
      - 6379:6379
    restart: unless-stopped
    logging:
      options:
        max-size: "20m"
        max-file: "5"

  #-----------------------------------------------
  #   Celery Service
  #-----------------------------------------------
  site_worker:
    # This auto-reloads
    command: bash -c "watchmedo auto-restart -p '*.py' --recursive -- celery -A celery_config worker -B -Q site-worker -l info -n site-worker@%n --concurrency=2"
    working_dir: /app/src
    build:
      context: .
    depends_on:
      - rabbit
      - db
    env_file: .env
    volumes:
      - .:/app
    restart: unless-stopped
    logging:
      options:
        max-size: "20m"
        max-file: "5"
    deploy:
      resources:
        limits:
          # Limit memory substantially here so we see any problems that may
          # appear on Heroku ahead of time
          memory: 256M

  compute_worker:
    command: bash -c "watchmedo auto-restart -p '*.py' --recursive -- celery -A compute_worker worker -l info -Q compute-worker -n compute-worker@%n"
    working_dir: /app
    build:
      context: .
      dockerfile: Dockerfile.compute_worker
    depends_on:
      - django
      - rabbit
    volumes:
      - ./compute_worker:/app
      - ${HOST_DIRECTORY:-/tmp/codabench}:/codabench
      # Actual connection back to docker parent to run things
      - /var/run/docker.sock:/var/run/docker.sock
    env_file: .env
    environment:
      - BROKER_URL=pyamqp://${RABBITMQ_DEFAULT_USER}:${RABBITMQ_DEFAULT_PASS}@${RABBITMQ_HOST}:${RABBITMQ_PORT}//
      # Make the worker leave behind the submission so we can examine it
      - CODALAB_IGNORE_CLEANUP_STEP=1
    logging:
      options:
        max-size: "20m"
        max-file: "5"
